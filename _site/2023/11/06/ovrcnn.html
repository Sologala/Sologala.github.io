<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Open-Vocabulary Object Detection Using Captions - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Open-Vocabulary Object Detection Using Captions">
    <meta name="keywords"  content="论文阅读">
    <!-- Open Graph -->
    <meta property="og:title" content="Open-Vocabulary Object Detection Using Captions - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2023/11/06/ovrcnn.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB" class="post-tag">论文阅读</a>
          
        
      </div>
      <h1>Open-Vocabulary Object Detection Using Captions</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="23-11-06"><i class="iconfont icon-date"></i>06 Nov 2023</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/655dac84c458853aef02b4b4.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>使用描述进行开集目标检测.</p>
</blockquote>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/2011.10678">Open-Vocabulary Object Detection Using Captions</a></li>
</ul>

<p>现有的开集大型数据如 <strong>Open Images</strong> 和 <strong>MSCOCO</strong> 数据集大约包含 <strong>600</strong> 个数据类别。如果想要识别现实世界中的任何物体，则需要更多的人工数据标注。但人类学习显示视觉世界中的物体很大程度上是基于语言的监督信号，可以使用几个简单的例子来泛化到其他目标上，而不需要预先见过所有的目标实例。</p>

<p>本文作者模仿人类的能力，设计了一个两阶段开集目标检测 <strong>Open-Vocabulary object Detection（OVD）</strong>，首次提出了使用 <strong>image-caption pairs</strong> 来获得无限的词汇，类似于人类从自然语言中学习一样；然后使用部分标注样本来学习目标检测任务。该方法的大致流程如下：</p>
<ol>
  <li>通过较低成本获取大量图像-文本描述样本对，学习视觉-语义空间；</li>
  <li>对一些基础类别（如<strong>COCO</strong>中的$80$类）通过类别标注学习目标检测任务；</li>
  <li>通过探索语义空间检测基础类别之外的新类别。</li>
</ol>

<p><img src="https://pic.imgdb.cn/item/655dae69c458853aef08dc52.jpg" alt="" /></p>

<p>本文提出了一个 <strong>Vision to Language（V2L）</strong>映射层，和 <strong>CNN</strong> 一起在预训练<strong>grounding</strong>任务和一些辅助自监督任务中进行学习。<strong>V2L layer</strong>负责将视觉特征变换到文本空间，好让两个不同模态的特征能在同一空间来衡量相似性。整个模型框架和 <strong>Faster RCNN</strong> 一样，只是将最后的 <strong>cls head</strong> 替换成了 <strong>V2L</strong>，也就是换成了一个将 <strong>visual feature</strong> 投影到 <strong>text embedding space</strong> 的投影矩阵。</p>

<p><img src="https://pic.imgdb.cn/item/655f16d0c458853aef4de228.jpg" alt="" /></p>

<h3 id="-预训练阶段">⚪ 预训练阶段</h3>

<p>为了避免在基础类别上过拟合，作者在大量词汇量$V_C$下进行了预训练，让模型能够学习到更全面的语义信息，而不是只有基础类别的语义信息。即在 <strong>image-caption pairs</strong> 上通过 <strong>grounding</strong>、<strong>masked language modeling (MLM)</strong> 、 <strong>image-text matching (ITM)</strong> 来训练 <strong>ResNet</strong> 和 <strong>V2L</strong>。</p>

<p>模型接收图像输入 <strong>visual backbone</strong>（<strong>ResNet50</strong>），接收文本输入 <strong>language backbone</strong>（<strong>BERT</strong>），分别提取对应的特征。对于<strong>grounding</strong>任务，模型让每个 <strong>caption</strong> 的 <strong>word embedding</strong> 和其对应的图像区域特征嵌入更加接近，设定一个 <strong>global grounding score</strong> 来度量其关系：</p>

\[\langle I,\mathrm{C}\rangle_{G}=\frac{1}{n_{C}}\sum_{j=1}^{n_C}\sum_{i=1}^{n_I}a_{i,j}\,\langle e_i^I,e_j^C\rangle_L \\
a_{i,j}=\frac{\exp\langle e_i^I,e_j^C\rangle_L}{\sum_{i^{\prime}=1}^{n_I}\exp\langle e_{i^{\prime}}^I,e_j^C\rangle_L}\]

<p>成对的 <strong>image-caption</strong> 得分要大，不成对的 <strong>image-caption</strong> 得分要小，采用对比损失实现。使用同一个 <strong>batch</strong> 中的其他图像作为每个 <strong>caption</strong> 的<strong>negative examples</strong>，也使用同一 <strong>batch</strong> 中的其他 <strong>caption</strong> 作为每个 <strong>image</strong> 的 <strong>negative examples</strong>。</p>

\[\mathcal{L}_{G}(I)=-\log\frac{\exp\langle I,C\rangle_{G}}{\sum_{C^{\prime}\in B_{C}}\exp\langle I,C\rangle_{G}} \\
\mathcal{L}_{G}(G)=-\log\frac{\exp\langle I,G^{\prime}\rangle_{G}}{\sum_{I^{\prime}\in B_{I}}\exp\langle I^{\prime},G\rangle_{G}}\]

<p>将两种特征输入多模态特征融合器中，来抽取多模态的 <strong>embedding</strong>。在此基础上执行<strong>MLM</strong>和<strong>ITM</strong>预训练。<strong>MLM</strong>是一种自回归任务，而<strong>ITM</strong>是一种分类任务。</p>

<h3 id="-训练与测试">⚪ 训练与测试</h3>

<p>训练阶段：预训练后使用得到的 <strong>ResNet</strong> 和 <strong>V2L layer</strong> 来初始化 <strong>Faster R-CNN</strong> ，以此来实现开放词汇目标检测，<strong>ResNet 50</strong> 用于 <strong>backbone</strong>，<strong>V2L layer</strong> 是会用于对每个 <strong>proposal</strong> 特征进行变换，变换之后会与类别标签的文本特征计算相似度来进行分类，训练的时候会固定 <strong>V2L layer</strong>，使其学习到的广泛的信息能够泛化到新类。</p>

<p>测试阶段：在完成 <strong>Faster R-CNN</strong> 的训练后采用如下测试流程：</p>
<ol>
  <li>使用经过预训练的 <strong>ResNet50</strong> 的 <strong>stem</strong> 和前 <strong>3</strong> 个 <strong>block</strong> 来抽取图像特征</li>
  <li>使用 <strong>region proposal network</strong> 来预测目标可能出现的位置和 <strong>objectness score</strong>，并且使用 <strong>NMS</strong> 和 <strong>RoI pooling</strong> 来得到每个目标框</li>
  <li>给每个 <strong>proposal</strong> 使用 <strong>ResNet50</strong> 的第 <strong>4</strong> 个 <strong>block</strong> （和一个 <strong>pooling</strong>）来提取每个 <strong>proposal</strong> 的最终特征</li>
  <li>对比每个 <strong>proposal</strong> 被<strong>V2L layer</strong>编码到 <strong>word space</strong> 中的特征和基础类别 <strong>k</strong> 的得分</li>
  <li>选择得分最大的类别作为该目标框的预测类别</li>
</ol>

\[p(i\mathrm{\bf~classified~as~}k)=\frac{\exp\langle e_i^I,e_k^{\mathcal{V}}\rangle}{1+\sum_{k^{\prime}\in\mathcal{V}_B}\exp\langle e_i^I,e_{k^{\prime}}^{\mathcal{V}}\rangle}\]


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2023/11/07/vild.html" class="read-next-link"></a>
        <section>
          <span>Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</span>
          <p>  通过视觉和语言知识蒸馏实现开放词汇目标检测.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/656ed4c2c458853aef278beb.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2023/11/05/mdetr.html" class="read-next-link"></a>
          <section>
            <span>MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding</span>
            <p>  MDETR：用于端到端多模态理解的调制检测.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/655d7579c458853aef493a35.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
