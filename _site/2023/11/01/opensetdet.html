<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>开放集合目标检测(Open-Set Object Detection) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="开放集合目标检测(Open-Set Object Detection)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="开放集合目标检测(Open-Set Object Detection) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2023/11/01/opensetdet.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>开放集合目标检测(Open-Set Object Detection)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="23-11-01"><i class="iconfont icon-date"></i>01 Nov 2023</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/658aa19ac458853aef584c92.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Open-Set Object Detection.</p>
</blockquote>

<p><strong>开集目标检测(Open-Set Object Detection, OSOD)</strong>也被称为<strong>Open-World</strong>目标检测或<strong>Open-Vocabulary</strong>目标检测，是指在可见类（<strong>base class</strong>）的数据上进行训练，然后完成对不可见类（<strong>unseen/target</strong>）数据的定位与识别。</p>

<p>开集目标检测的出发点是制定一种更加通用的目标检测模型来覆盖更多的<strong>object concept</strong>，使得目标检测不再受限于带标注数据的少数类别，从而实现更加泛化的目标检测，识别出更多<strong>novel</strong>的物体类别。</p>

<p>注意：早期的<strong>Open-Set</strong>目标检测与<strong>Open-Vocabulary</strong>目标检测所指代含义是不同的。前者倾向于检测出未知目标即可，不再判别目标的具体类别；后者则需要根据指定的新类别进行检测。本文则不再进行区分。</p>

<p>一些常见的开集目标检测方法包括：</p>
<ul>
  <li>基于无监督学习的开集检测器：通过聚类、弱监督等手段实现开集检测，如<strong>OSODD</strong>, <strong>Detic</strong>, <strong>VLDet</strong></li>
  <li>基于多模态学习的开集检测器：
    <ol>
      <li>基于<strong>Referring</strong>的开集检测器：借助多模态视觉-语言模型实现检测，如<strong>ViLD</strong>, <strong>RegionCLIP</strong>, <strong>VL-PLM</strong>, <strong>Grad-OVD</strong></li>
      <li>基于<strong>Grounding</strong>的开集检测器：把开集检测任务建模为边界框提取+短语定位任务，如<strong>OVR-CNN</strong>, <strong>MDETR</strong>, <strong>GLIP</strong>, <strong>DetCLIP</strong>, <strong>DetCLIPv2</strong>, <strong>Grounding DINO</strong></li>
    </ol>
  </li>
</ul>

<h1 id="1-基于无监督学习的开集检测器">1. 基于无监督学习的开集检测器</h1>

<h3 id="-osodd">⚪ OSODD</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/04/osodd.html"><font color="blue">Towards Open-Set Object Detection and Discovery</font></a></li>
</ul>

<p><strong>OSODD</strong>采用阶段检测方式：<strong>Object Detection and Retrieval (ODR)</strong> 和 <strong>Object Category Discovery (OCD)</strong>。</p>
<ul>
  <li><strong>ODR</strong>对已知物体和未知物体进行检测（通过对类别无感知的<strong>RPN</strong>实现）：已知物体预测位置信息和类别；未知物体只预测其位置信息。</li>
  <li><strong>OCD</strong>对已知物体和未知物体的区域特征进行编码，并用 <strong>constrained k-means</strong>对未知类别进行聚类。</li>
</ul>

<p><img src="https://pic.imgdb.cn/item/655c6f42c458853aef59a09d.jpg" alt="" /></p>

<h3 id="-detic">⚪ Detic</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/09/detic.html"><font color="blue">Detecting Twenty-thousand Classes using Image-level Supervision</font></a></li>
</ul>

<p><strong>Detic</strong>使用图像分类数据集和目标检测数据集一起联合训练。对于分类图像，由<strong>RPN</strong>获取<strong>proposal</strong>，选取最大面积的<strong>proposal</strong>，这个<strong>proposal</strong>对应的<strong>label</strong>就是图像层面的类别。</p>

<p><img src="https://pic.imgdb.cn/item/658933bec458853aef946211.jpg" alt="" /></p>

<h3 id="-vldet">⚪ VLDet</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/13/vldet.html"><font color="blue">Learning Object-Language Alignments for Open-Vocabulary Object Detection</font></a></li>
</ul>

<p><strong>VLDet</strong>直接从<strong>image-text pairs</strong>训练目标检测器，主要出发点是从<strong>image-text pairs</strong>中提取<strong>region-word pairs</strong>可以表述为两个集合的元素匹配问题，该问题可以通过找到区域和单词之间具有最小全局匹配成本的二分匹配来有效解决。</p>

<p><img src="https://pic.imgdb.cn/item/658b9483c458853aef13b6c9.jpg" alt="" /></p>

<h1 id="2-基于多模态学习的开集检测器">2. 基于多模态学习的开集检测器</h1>

<p>当人类捕捉到视觉信息后，会将它们与语言文字联系起来，从而产生丰富的视觉和语义词汇，这些词汇可以用于检测物体，并拓展模型的表达能力。因此通过多模态学习技术借助大量的图像与语义词汇，可以使得目标检测器在未知类别也能进行识别与定位。</p>

<h2 id="1基于referring的开集检测器">（1）基于Referring的开集检测器</h2>

<p>预训练的开放词汇图像分类模型（如<strong>CLIP</strong>）是一种在大量图像和文本对上训练的多模态模型。这类模型可用于查找最能代表图像的文本片段，或查找给定文本查询的最合适图像。</p>

<p>基于<strong>Referring</strong>的开集检测器是指将开放词汇图像分类模型中视觉端的能力迁移至检测模型中，再利用文本编码器完成检测模型的识别工作。</p>

<h3 id="-vild">⚪ ViLD</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/07/vild.html"><font color="blue">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</font></a></li>
</ul>

<p><strong>ViLD</strong>通过<strong>CLIP</strong>的视觉编码器蒸馏训练<strong>R-CNN</strong>中的区域提议网络，再使用<strong>CLIP</strong>的文本编码器对目标区域进行分类。</p>

<p><img src="https://pic.imgdb.cn/item/65701bf4c458853aef0a6a30.jpg" alt="" /></p>

<h3 id="-regionclip">⚪ RegionCLIP</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/12/regionclip.html"><font color="blue">RegionCLIP: Region-based Language-Image Pretraining</font></a></li>
</ul>

<p><strong>CLIP</strong>在<strong>Region</strong>区域上的识别很差，这是由于<strong>CLIP</strong>是在<strong>Image-Language level</strong>上进行的预训练导致的。<strong>RegionCLIP</strong>将<strong>CLIP</strong>在<strong>region</strong>图像和单词层面进行预训练，提高了区域级别的检测能力。</p>

<p><img src="https://pic.imgdb.cn/item/658a7ae7c458853aefdff48d.jpg" alt="" /></p>

<h3 id="-vl-plm">⚪ VL-PLM</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/11/vlplm.html"><font color="blue">Exploiting Unlabeled Data with Vision and Language Models for Object Detection</font></a></li>
</ul>

<p><strong>VL-PLM</strong>利用<strong>CLIP</strong>对无标签数据进行伪标签标注，然后混合伪标签数据和真实标注数据一起训练。注意到<strong>CLIP</strong>对于区域级别的图像识别能力不足，<strong>VL-PLM</strong>反复将<strong>ROI</strong>输入<strong>ROI head</strong>精调，最后<strong>RPN</strong>的分数将和<strong>CLIP</strong>的分类分数作平均。</p>

<p><img src="https://pic.imgdb.cn/item/658a947bc458853aef34c850.jpg" alt="" /></p>

<h3 id="-grad-ovd">⚪ Grad-OVD</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/08/gradovd.html"><font color="blue">Open Vocabulary Object Detection with Pseudo Bounding-Box Labels</font></a></li>
</ul>

<p>给定一个预训练的视觉语言模型和一个图像-描述样本对，<strong>Grad-OVD</strong>在图像中计算<strong>Grad-CAM</strong>激活图，对应于描述中感兴趣的目标。然后将激活映射转换为对应类别的伪标签框。开放词汇检测器可以直接在这些伪标签的监督下进行训练。</p>

<p><img src="https://pic.imgdb.cn/item/6585330dc458853aef21b989.jpg" alt="" /></p>

<h2 id="2基于grounding的开集检测器">（2）基于Grounding的开集检测器</h2>

<p>短语定位（<strong>phrase grounding</strong>）任务是指同时提供一张图像和一段描述图像的文本（<strong>captions</strong>），根据文本的描述信息从图像中找到对应的物体。</p>

<p>基于<strong>Grounding</strong>的开集检测器是指把开集目标检测任务建模为边界框提取+短语定位任务。该过程引入大规模<strong>caption</strong>数据集完成<strong>region-word</strong>级别的视觉模型和语言模型预训练。</p>

<h3 id="-ovr-cnn">⚪ OVR-CNN</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/06/ovrcnn.html"><font color="blue">Open-Vocabulary Object Detection Using Captions</font></a></li>
</ul>

<p><strong>OVR-CNN</strong>使用一个 <strong>Vision to Language（V2L）</strong>映射层将视觉特征变换到文本嵌入空间，使得两个不同模态的特征能在同一空间来衡量相似性。<strong>V2L</strong>通过<strong>grounding</strong>, <strong>masked language modeling (MLM)</strong>和<strong>image-text matching (ITM)</strong>任务预训练后，把预测边界框的特征嵌入后与预先给定类别标签的文本特征计算相似度来进行分类。</p>

<p><img src="https://pic.imgdb.cn/item/655f16d0c458853aef4de228.jpg" alt="" /></p>

<h3 id="-mdetr">⚪ MDETR</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/05/mdetr.html"><font color="blue">MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding</font></a></li>
</ul>

<p><strong>MDETR</strong>采用<strong>CNN</strong>提取视觉特征，采用语言模型提取文本特征，将两者<strong>concat</strong>后通过<strong>DETR</strong>预测目标框。</p>

<p><img src="https://pic.imgdb.cn/item/655d9d3ac458853aefcca84d.jpg" alt="" /></p>

<h3 id="-glip">⚪ GLIP</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/03/glip.html"><font color="blue">Grounded Language-Image Pre-training</font></a></li>
</ul>

<p><strong>GLIP</strong>把图像特征与文本特征分别由单独的编码器编码，然后通过跨模态多头注意力模块（<strong>X-MHA</strong>）进行深度融合。</p>

<p><img src="https://pic.imgdb.cn/item/655c6111c458853aef2d9091.jpg" alt="" /></p>

<h3 id="-detclip">⚪ DetCLIP</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/14/detclip.html"><font color="blue">DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection</font></a></li>
</ul>

<p><strong>GLIP</strong> 在 <strong>text encoder</strong> 中会学习所有类别之间的 <strong>attention</strong>，<strong>DetCLIP</strong>将每个 <strong>concept</strong> 分别送入不同的 <strong>text encoder</strong>。这样能够避免模型受到不相关类别无效关联，并且能给每个 <strong>concept</strong> 都产生一个长描述。</p>

<p><img src="https://pic.imgdb.cn/item/658bcb3cc458853aefce62d3.jpg" alt="" /></p>

<h3 id="-detclipv2">⚪ DetCLIPv2</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/15/detclipv2.html"><font color="blue">DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment</font></a></li>
</ul>

<p><strong>DetCLIPv2</strong>是一个面向开放词汇目标检测的统一端到端预训练框架，通过使用区域和单词之间的最佳匹配集相似性来引导对比目标，以端到端的方式执行检测、定位和图像对数据的联合训练。</p>

<p><img src="https://pic.imgdb.cn/item/658be095c458853aef1b3bf3.jpg" alt="" /></p>

<h3 id="-grounding-dino">⚪ Grounding DINO</h3>
<ul>
  <li>paper：<a href="https://0809zheng.github.io/2023/11/02/groundingdino.html"><font color="blue">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</font></a></li>
</ul>

<p><strong>Grounding DINO</strong>采用双编码器、单解码器结构。对于输入的每个(图像，文本)对，首先分别使用图像主干网络和文本主干网络提取图像特征和文本特征；两个特征被送入特征增强模块，用于跨模态特征融合；获得跨模态文本和图像特征后，使用语言引导的查询选择模块从图像特征中选择跨模态的目标查询；这些跨模态查询被送到跨模态解码器中，用于预测目标框并提取相应的短语。</p>

<p><img src="https://pic.imgdb.cn/item/6555809dc458853aef89eec8.jpg" alt="" /></p>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2023/11/02/groundingdino.html" class="read-next-link"></a>
        <section>
          <span>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</span>
          <p>  Grounding DINO：结合DINO与GLIP用于开集目标检测.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/65548c7fc458853aef25cb7e.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2023/09/27/fsq.html" class="read-next-link"></a>
          <section>
            <span>Finite Scalar Quantization: VQ-VAE Made Simple</span>
            <p>  有限标量量化：简化向量量化的变分自编码器.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/668b8624d9c307b7e98c71cf.png" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
