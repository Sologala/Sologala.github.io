<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DawsonWen的个人网站</title>
    <description>为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平</description>
    <link>http://localhost:4000//</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 18 Dec 2024 00:00:09 +0800</pubDate>
    <lastBuildDate>Wed, 18 Dec 2024 00:00:09 +0800</lastBuildDate>
    <generator>Jekyll v4.3.4</generator>
    
      <item>
        <title>FEAR: Fast Efficient Accurate and Robust Visual Tracker</title>
        <description>&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241216221511256.png&quot; width=&quot;600&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241216221637100.png&quot; width=&quot;400&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;:house: https://github.com/PinataFarms/FEARTracker&lt;/p&gt;

&lt;p&gt;摘要： 在FBNet的基础上，引入了pixel-correlation以及单参数化的dynamic模板更新策，在移动设备上实现效率能耗更优的孪生网络检测器。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提出了三个不同尺寸的模型 &lt;strong&gt;XS、M、L&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当前SOTA的方法都太重了&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SiamRPN++、Ocean参数量都很高&lt;/li&gt;
  &lt;li&gt;Transformer-based在端侧推理的时候内存峰值更高。&lt;/li&gt;
  &lt;li&gt;在线更新问题的更少参数量实现&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;轻量BackBone&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2016 SqueezeNet&lt;/li&gt;
  &lt;li&gt;2018 SqueezeNext&lt;/li&gt;
  &lt;li&gt;2017 MobileNet （dw卷积）&lt;/li&gt;
  &lt;li&gt;2018 ShuffleNet （分组卷积）&lt;/li&gt;
  &lt;li&gt;2019 FBNet (NAS搜索出来的)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241216230725053.png&quot; alt=&quot;image-20241216230725053&quot; /&gt;&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Dec 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/12/15/FEAR_Tracker.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/12/15/FEAR_Tracker.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>Efficient Visual Tracking with Exemplar Transformers</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Efficient Visual Tracking with Exemplar Transformers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;简介&quot;&gt;简介&lt;/h1&gt;

&lt;p&gt;更准确的NNTracker，通常采用更深的网络或者更加高级的block，比如trackformer，但实时性堪忧。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;利用single instance level attention layer来实现transformer based Tracking&lt;/li&gt;
  &lt;li&gt;CPU 47FPS&lt;/li&gt;
  &lt;li&gt;https://github.com/pblatter/ettrack&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transformer始于机器翻译，而后在诸多视觉任务中提升正确率以及鲁棒性。但计算开销巨大。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241215124123740.png&quot; alt=&quot;image-20241215124123740&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;exemplar-attention&quot;&gt;Exemplar Attention&lt;/h2&gt;

</description>
        <pubDate>Sun, 15 Dec 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/12/15/E_T_Track.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/12/15/E_T_Track.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>uaternion-kinimatics</title>
        <description>&lt;h1 id=&quot;旋转的朴素分解&quot;&gt;旋转的朴素分解&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241217225501921.png&quot; alt=&quot;image-20241217225501921&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241217230118436.png&quot; alt=&quot;image-20241217230118436&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241217230036279.png&quot; alt=&quot;image-20241217230036279&quot; style=&quot;zoom:150%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将$x$向量绕着$u$旋转$\phi$的得到向量$x^{‘}$ ，可以通过几何关系表达$x^{‘}$为一个 平行与旋转轴的旋转不变分量以及一个正交平面上的2维旋转。&lt;/p&gt;

&lt;p&gt;记&lt;img src=&quot;../../../.config/Typora/typora-user-images/image-20241217232237530.png&quot; alt=&quot;image-20241217232237530&quot; /&gt;
\(\mathbf{\phi } = \phi u\)&lt;/p&gt;

&lt;h2 id=&quot;欧拉角转四元数&quot;&gt;欧拉角转四元数&lt;/h2&gt;

\[q = Exp(\phi u) = e^{\frac{\phi u}{2}} = cos \frac{\phi}{2} + u sin \frac{\phi}{2} = \begin{bmatrix} cos\frac{\phi}{2} \\ u sin \frac{\phi} {2} \end{bmatrix}\]

&lt;h2 id=&quot;四元数转欧拉角&quot;&gt;四元数转欧拉角&lt;/h2&gt;

\[\phi = 2 arctan(||q_{xyz}||, q_{w}) \\
u = \frac{q_{xyz}}{||q_{xyz}||}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;值得注意的是，当角度很小的时候，$q_{xyz}$虚部的分量会很小，$u$的计算是会变得无穷大。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用如下的方法近似 arctan之后乘上$u$如下：
\(\theta u \approx 2 \frac{q_{xyz}}{q_w} (1 - \frac{||q_{xyz}||^2}{3 q_{w}^2})\)&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Dec 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/12/15/quaternion-kinimatics.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/12/15/quaternion-kinimatics.html</guid>
        
        <category>数学</category>
        
        
      </item>
    
      <item>
        <title>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;DINO-X：开放世界目标检测与理解的统一视觉模型.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;paper：&lt;a href=&quot;https://arxiv.org/abs/2411.14347&quot;&gt;DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1-方法简介&quot;&gt;1. 方法简介&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;是由&lt;strong&gt;IDEA Research&lt;/strong&gt;开发的以目标为中心的统一视觉模型，支持各种开放世界感知和目标级理解任务，包括开放世界目标检测和分割、短语定位、视觉提示目标计数、姿态估计、无提示目标检测和识别、密集区域描述等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/6744254888c538a9b5bb97b9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;采用与&lt;strong&gt;Transformer&lt;/strong&gt;相同的编码器-解码器架构，并将开集检测作为其核心训练任务。为了使长尾目标检测更容易，&lt;strong&gt;DINO-X&lt;/strong&gt;在模型输入阶段采用了更全面的提示设计：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;文本提示：基于用户提供的文本输入来识别感兴趣的目标，这可以覆盖大多数检测场景；&lt;/li&gt;
  &lt;li&gt;视觉提示：&lt;strong&gt;DINO-X&lt;/strong&gt;支持画框或点等视觉提示，进一步覆盖了仅靠文本无法很好描述的检测场景；&lt;/li&gt;
  &lt;li&gt;定制提示符：&lt;strong&gt;DINO-X&lt;/strong&gt;特别引入了定制提示符，它可以作为预定义的或用户调整的提示符嵌入来满足定制需求。通过提示调优，可以为不同的域或特定于功能的提示创建域定制的提示，以满足各种功能需求。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;能够集成多个感知头，同时支持多个目标感知和理解任务，对输入图像提供更详细的目标级理解。除了用于目标检测的边界框头之外，&lt;strong&gt;DINO-X&lt;/strong&gt;还实现了三个额外的头：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;用于预测检测目标的分割掩码的掩码头；&lt;/li&gt;
  &lt;li&gt;用于预测特定类别的关键点的关键点头；&lt;/li&gt;
  &lt;li&gt;用于为每个检测目标生成细粒度描述性标题的语言头。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67441c4c88c538a9b5bb9361.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;包括两个模型：&lt;strong&gt;DINO-X Pro&lt;/strong&gt;模型为各种场景提供增强的感知能力；&lt;strong&gt;DINO-X Edge&lt;/strong&gt;模型优化了更快的推理速度，更适合部署在边缘设备上。&lt;/p&gt;

&lt;p&gt;为了增强模型的预训练能力，作者构建了一个包含超过1亿个高质量样本的大型数据集&lt;strong&gt;Grounding-100M&lt;/strong&gt;，以提高模型的开放词汇检测性能。在这样一个大规模的基础数据集上进行预训练可以得到一个目标的基础级表示。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DINO-X Pro&lt;/strong&gt;模型在&lt;strong&gt;COCO&lt;/strong&gt;、&lt;strong&gt;LVIS-minival&lt;/strong&gt;和&lt;strong&gt;LVIS-val&lt;/strong&gt;的零样本基准测试中分别达到了&lt;strong&gt;56.0 AP&lt;/strong&gt;、&lt;strong&gt;59.8 AP&lt;/strong&gt;和&lt;strong&gt;52.4 AP&lt;/strong&gt;。值得注意的是，它在&lt;strong&gt;LVIS-minival&lt;/strong&gt;和&lt;strong&gt;LVIS-val&lt;/strong&gt;的罕见类别中得分为&lt;strong&gt;63.3&lt;/strong&gt;和&lt;strong&gt;56.5&lt;/strong&gt;，比&lt;strong&gt;Grounding DINO 1.6 Pro&lt;/strong&gt;提高了&lt;strong&gt;5.8 AP&lt;/strong&gt;和&lt;strong&gt;5.0 AP&lt;/strong&gt;，比&lt;strong&gt;Grounding DINO 1.5 Pro&lt;/strong&gt;提高了&lt;strong&gt;7.2 AP&lt;/strong&gt;和&lt;strong&gt;11.9 AP&lt;/strong&gt;，突出了其识别长尾物体的能力显著提高。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67441c0788c538a9b5bb9347.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-模型细节&quot;&gt;2. 模型细节&lt;/h1&gt;

&lt;h2 id=&quot;1dino-x-pro模型&quot;&gt;（1）DINO-X Pro模型&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;利用&lt;strong&gt;ViT&lt;/strong&gt;作为特征提取的骨干，并结合了类似的&lt;strong&gt;Transformer&lt;/strong&gt;编码器-解码器架构。&lt;strong&gt;DINO-X&lt;/strong&gt;支持的提示编码器：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;文本提示编码器：&lt;strong&gt;DINO-X Pro&lt;/strong&gt;使用预训练的&lt;strong&gt;CLIP&lt;/strong&gt;模型作为文本编码器；&lt;/li&gt;
  &lt;li&gt;视觉提示编码器：&lt;strong&gt;DINO-X Pro&lt;/strong&gt;采用&lt;strong&gt;T-Rex2&lt;/strong&gt;的视觉提示编码器，通过使用框和点两种格式的用户自定义视觉提示来增强目标检测：使用正弦余弦层将这些提示转换为位置嵌入，然后投影到统一的特征空间中；使用不同的线性投影分离框和点提示；然后采用多尺度可变形交叉注意力层，以用户提供的视觉提示为条件，从多尺度特征图中提取视觉提示特征。&lt;/li&gt;
  &lt;li&gt;定制提示：&lt;strong&gt;DINO-X Pro&lt;/strong&gt;定义了一系列定制提示，可以通过提示调优技术对其进行微调，以资源效率和成本效益高的方式覆盖更多的长尾、特定于领域或特定于功能的场景。例如作者开发了一个通用的目标提示符来支持无提示的开放世界检测，使检测图像中的任何目标成为可能，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;会在上述提示和从输入图像中提取的视觉特征之间进行深度特征融合，然后对不同的感知任务应用不同的头部。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;边界框头：采用语言引导的查询选择模块，选择与输入提示最相关的特征作为解码器目标查询。然后将每个查询馈送到&lt;strong&gt;Transformer&lt;/strong&gt;解码器并逐层更新；最后应用简单的&lt;strong&gt;MLP&lt;/strong&gt;层预测每个目标查询的相应边界框坐标。使用&lt;strong&gt;L1&lt;/strong&gt;损失和&lt;strong&gt;G-IoU&lt;/strong&gt;损失进行边界框回归，同时使用对比损失将每个目标查询与输入提示对齐。&lt;/li&gt;
  &lt;li&gt;掩码头：通过融合来自&lt;strong&gt;Transformer&lt;/strong&gt;编码器的&lt;strong&gt;1/4&lt;/strong&gt;分辨率骨干特征和上采样的&lt;strong&gt;1/8&lt;/strong&gt;分辨率特征来构建像素嵌入图。然后在&lt;strong&gt;Transformer&lt;/strong&gt;解码器的每个目标查询和像素嵌入映射之间进行点积，以获得查询的掩码输出。为了提高训练效率，只将主干的&lt;strong&gt;1/4&lt;/strong&gt;分辨率特征图用于掩码预测。在最终的掩码损失计算中，只计算采样点的掩码损失。&lt;/li&gt;
  &lt;li&gt;关键点头：&lt;strong&gt;DINO-X&lt;/strong&gt;为人和手实例化了两个关键点头，它们分别有&lt;strong&gt;17&lt;/strong&gt;个和&lt;strong&gt;21&lt;/strong&gt;个预定义的关键点。关键点头从&lt;strong&gt;DINO-X&lt;/strong&gt;获取与关键点相关的检测输出（人和手），每个检测输出都被视为查询，并利用一个单独的解码器解码目标关键点，然后将这些关键点发送到多个可变形的&lt;strong&gt;Transformer&lt;/strong&gt;解码器层，以预测关键点位置及其可见性。&lt;/li&gt;
  &lt;li&gt;语言头：对于任何从&lt;strong&gt;DINO-X&lt;/strong&gt;中检测到的目标，首先使用&lt;strong&gt;RoIAlign&lt;/strong&gt;操作符从&lt;strong&gt;DINO-X&lt;/strong&gt;主干特征中提取其区域特征，并结合其查询嵌入形成目标标记；然后应用一个简单的线性投影来确保它们的尺寸与文本嵌入对齐；轻量级语言解码器将这些区域表示与任务令牌集成在一起，以自动回归的方式生成输出；可学习的任务令牌使语言解码器能够处理各种任务。
&lt;img src=&quot;https://pic.imgdb.cn/item/67441fb988c538a9b5bb94a2.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2dino-x-edge模型&quot;&gt;（2）DINO-X Edge模型&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;DINO-X Edge&lt;/strong&gt;采用&lt;strong&gt;EfficientViT&lt;/strong&gt;作为高效特征提取的骨干。为了进一步提高&lt;strong&gt;DINO-X Edge&lt;/strong&gt;模型的性能和计算效率，作者对模型架构和训练技术进行了以下几个方面的改进：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;更强的文本提示编码器：&lt;strong&gt;DINO-X Edge&lt;/strong&gt;采用与&lt;strong&gt;Pro&lt;/strong&gt;型号相同的&lt;strong&gt;CLIP&lt;/strong&gt;文本编码器。在实践中文本提示嵌入可以预先计算，并且不会影响视觉编码器和解码器的推理速度。使用更强大的文本提示编码器通常会产生更好的结果。&lt;/li&gt;
  &lt;li&gt;知识蒸馏：&lt;strong&gt;DINO-X Edge&lt;/strong&gt;从&lt;strong&gt;Pro&lt;/strong&gt;模型中提取知识以提高性能。具体来说，作者利用了基于特征的蒸馏和基于响应的蒸馏，它们分别在&lt;strong&gt;Edge&lt;/strong&gt;模型和&lt;strong&gt;Pro&lt;/strong&gt;模型之间对齐特征和预测&lt;strong&gt;logits&lt;/strong&gt;值。&lt;/li&gt;
  &lt;li&gt;改进的&lt;strong&gt;FP16&lt;/strong&gt;推理：作者采用一种归一化技术进行浮点乘法，使模型量化到&lt;strong&gt;FP16&lt;/strong&gt;而不影响精度。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3训练数据&quot;&gt;（3）训练数据&lt;/h2&gt;

&lt;p&gt;为了确保核心的开放词汇目标检测能力，作者开发了一个高质量和语义丰富的&lt;strong&gt;Grounding&lt;/strong&gt;数据集，该数据集由从网络上收集的1亿多张图像组成，称为&lt;strong&gt;Grounding-100M&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;作者使用&lt;strong&gt;T-Rex2&lt;/strong&gt;的训练数据和一些额外的工业场景数据进行基于视觉提示的预训练。&lt;/p&gt;

&lt;p&gt;作者使用开源分割模型，如&lt;strong&gt;SAM&lt;/strong&gt;和&lt;strong&gt;SAM2&lt;/strong&gt;为&lt;strong&gt;Grounding-100M&lt;/strong&gt;数据集的一部分生成伪掩码注释作为掩码头的主要训练数据。&lt;/p&gt;

&lt;p&gt;作者从&lt;strong&gt;Grounding-100M&lt;/strong&gt;数据集中采样了一个高质量的数据子集，并利用它们的框注释作为无提示检测训练数据。&lt;/p&gt;

&lt;p&gt;作者还收集了超过&lt;strong&gt;1000&lt;/strong&gt;万个区域理解数据用于语言头训练，涵盖了目标识别、区域描述、&lt;strong&gt;OCR&lt;/strong&gt;和区域级&lt;strong&gt;QA&lt;/strong&gt;场景。&lt;/p&gt;

&lt;h2 id=&quot;4训练策略&quot;&gt;（4）训练策略&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;DINO-X&lt;/strong&gt;采用了两阶段训练策略：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;第一阶段对基于文本提示的检测、基于视觉提示的检测和目标分割进行了联合训练；这种大规模的预训练确保了&lt;strong&gt;DINO-X&lt;/strong&gt;出色的开放词汇检测性能，并产生了基本的目标级表示。&lt;/li&gt;
  &lt;li&gt;第二阶段冻结了&lt;strong&gt;DINO-X&lt;/strong&gt;骨干网络，并添加了两个关键点头（人和手）和一个语言头，每个都被单独训练；通过增加更多的头部，极大地扩展了&lt;strong&gt;DINO-X&lt;/strong&gt;执行更细粒度感知和理解任务的能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;随后作者利用提示调优技术训练了一个通用目标提示符，允许在保留模型的其他功能的同时进行无提示的任何目标检测。&lt;/p&gt;

&lt;p&gt;这种两阶段训练方法确保模型的核心检测能力不受引入新能力的影响，它还验证了大规模开集检测预训练可以作为以目标为中心的模型的强大基础，允许无缝转移到其他开放世界的理解任务。&lt;/p&gt;

</description>
        <pubDate>Mon, 25 Nov 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/25/dinox.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/25/dinox.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>DODA: Diffusion for Object-detection Domain Adaptation in Agriculture</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;DODA：农业中目标检测领域自适应的扩散模型.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;paper：&lt;a href=&quot;https://openreview.net/forum?id=KUpUO7aSSg&quot;&gt;DODA: Diffusion for Object-detection Domain Adaptation in Agriculture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;/h2&gt;

&lt;p&gt;本文提出了一种名为&lt;strong&gt;DODA（Diffusion for Object-detection Domain Adaptation in Agriculture）&lt;/strong&gt;的统一框架，利用扩散模型为多种农业场景生成高质量、特定领域的检测数据。&lt;strong&gt;DODA&lt;/strong&gt;结合了外部域嵌入和改进的布局到图像（&lt;strong&gt;L2I&lt;/strong&gt;）方法，允许它在没有额外训练的情况下为新域生成高质量的检测数据。本文在全球麦穗检测数据集&lt;strong&gt;GWHD&lt;/strong&gt;上展示了&lt;strong&gt;DODA&lt;/strong&gt;的有效性，其中对&lt;strong&gt;DODA&lt;/strong&gt;生成的数据进行微调的检测器在多个领域产生了显著的改进（最大&lt;strong&gt;+15.6 AP&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67404db4d29ded1a8cd5dc46.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-背景介绍&quot;&gt;1. 背景介绍&lt;/h2&gt;

&lt;p&gt;在农业领域，目标检测面临诸多挑战，如环境多样性、作物生长阶段差异、采集设备和时间的不同等。这些因素导致现有检测模型难以在不同农业环境中实现稳定的性能。&lt;/p&gt;

&lt;p&gt;领域自适应（&lt;strong&gt;Domain Adaptation, DA&lt;/strong&gt;）是迁移学习的一个分支，旨在提高模型在未见过的领域中的泛化能力。通过域自适应，模型可以利用源领域的知识来适应目标领域，从而减少对目标领域大量标注数据的依赖。&lt;/p&gt;

&lt;p&gt;现有方法大多依赖于大量标注数据，且在处理农业领域的复杂变化时效果不佳。此外，传统方法通常只关注单一领域的自适应，缺乏跨领域的通用性。这就提出了一个问题：如何利用扩散模型为特定领域生成高质量的检测数据？&lt;/p&gt;

&lt;h2 id=&quot;2-方法介绍&quot;&gt;2. 方法介绍&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;DODA&lt;/strong&gt;的目标是通过将领域信息集成到&lt;strong&gt;L2I&lt;/strong&gt;扩散中来实现领域感知图像的生成。首先将布局表示为图像，然后使用预训练的布局编码器提取特征作为域嵌入，最后通过特征加法融合整合到&lt;strong&gt;L2I&lt;/strong&gt;扩散中，作者将这种方法称为&lt;strong&gt;LI2I （layout-image-to-image）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/674051e4d29ded1a8cda4c4a.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;边界框可能会彼此重叠，为了使布局编码器区分实例，将重叠的实例分配给不同的颜色通道。具体地，将每个图像中边界框的重叠关系表示为邻接矩阵，并使用下列算法对这些框进行排列。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67405277d29ded1a8cdb000e.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;布局编码器具有简单的结构，由一堆时间相关的残差层和下采样层组成。每一残差层的输出为$f_{res}(a, t) + a$，这里$a$是上一层的输出，$t$是时间步长。&lt;/p&gt;

&lt;p&gt;作者观察到，浅层的&lt;strong&gt;U-Net&lt;/strong&gt;层会产生有噪声的局部特征。随着层的依赖，特征变得越来越抽象和整体，逐渐形成图像的整体布局。因此建议将布局嵌入与更深层（&lt;strong&gt;U-Net&lt;/strong&gt;解码器中的层）的特征合并，以更好地传达布局信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/6740530dd29ded1a8cdbaff8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-实验分析&quot;&gt;3. 实验分析&lt;/h2&gt;

&lt;p&gt;作者采用全球小麦检测数据集&lt;strong&gt;GWHD&lt;/strong&gt;进行实验，&lt;strong&gt;GWHD&lt;/strong&gt;数据集是最大的农业检测数据集之一，专门用于近距离麦穗检测。它由47个子域组成，每个子域都有一定的差异，如位置、成像流程、采集时间、小麦发育阶段和小麦品种。&lt;strong&gt;GWHD&lt;/strong&gt;数据集分为训练集、验证集和测试集，分别包含18、11和18个子域。&lt;/p&gt;

&lt;p&gt;用&lt;strong&gt;COCO&lt;/strong&gt;预训练初始化一个&lt;strong&gt;YOLOX-L&lt;/strong&gt;，在&lt;strong&gt;GWHD&lt;/strong&gt;训练集上训练它，并将其作为基线。对于训练集每个域，使用&lt;strong&gt;DODA&lt;/strong&gt;生成一个包含200张图像的数据集对&lt;strong&gt;YOLOX-L&lt;/strong&gt;进行一轮微调。结果表明，利用&lt;strong&gt;DODA&lt;/strong&gt;合成的特定领域数据对检测器进行微调后，这些领域的识别率提高了，表明所提方法有效地帮助检测器适应农业领域的新场景，弥补了受限的人工注释与复杂多变的农业环境之间的差距。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/674057bfd29ded1a8ce01ba1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者进一步在&lt;strong&gt;COCO&lt;/strong&gt;数据集上进行了实验，以证明所提方法的有效性。&lt;strong&gt;COCO&lt;/strong&gt;包含多个类别，因此作者改进了布局编码方法，同一类别的目标以相同的色调描绘，但亮度较弱，并按面积降序绘制每个目标的边界框。&lt;/p&gt;

&lt;p&gt;结果表明，&lt;strong&gt;LI2I&lt;/strong&gt;方法在可控性（&lt;strong&gt;mAP&lt;/strong&gt;）方面明显优于所有以前的&lt;strong&gt;L2I&lt;/strong&gt;方法，同时保持高图像质量（&lt;strong&gt;FID&lt;/strong&gt;）和多样性（&lt;strong&gt;IS&lt;/strong&gt;）。此外与将布局表示为文本的方法（&lt;strong&gt;LayoutDiffusion, Layoutdiffuse, GeoDiffusion&lt;/strong&gt;）相比，布局图像克服了基于文本的布局的限制，允许更精确和详细的控制，包括以前具有挑战性的小目标。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/674058b8d29ded1a8ce0cbeb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/22/doda.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/22/doda.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;CLIBD：融合视觉与基因组学的大规模生物多样性监测.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;paper：&lt;a href=&quot;https://openreview.net/forum?id=d5HUnyByAI&quot;&gt;CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;/h2&gt;

&lt;p&gt;本文提出了一种创新方法，首次融合&lt;strong&gt;DNA&lt;/strong&gt;和图像数据以提高生物多样性监测的准确性。通过对比实验，该方法在零样本学习任务上比先前的单模态方法提高了$8\%$以上的准确率，为生物多样性研究提供了新的思路和方法。&lt;/p&gt;

&lt;h2 id=&quot;1-背景介绍&quot;&gt;1. 背景介绍&lt;/h2&gt;

&lt;p&gt;随着环境变化和栖息地丧失的加速，监测生物多样性对于理解和维护生态系统健康至关重要。大规模的生物分类对于了解区域生物多样性和研究物种相互作用尤为重要。传统上，研究人员依赖图像识别技术来识别图像中的生物体，但这些方法仅基于图像信息，忽略了生物体丰富的进化历史和遗传信息。&lt;/p&gt;

&lt;p&gt;DNA条形码技术（如COI条形码）为生物分类提供了更精确的手段，且比需要人类专家手动检查的分类标签更容易大规模获取。然而，DNA条形码和图像数据通常来自不同的来源，且存在部分物种仅有图像或仅有DNA数据的情况。因此，如何有效地融合这两种模态的数据，以提高生物多样性监测的准确性，成为了一个亟待解决的问题。&lt;/p&gt;

&lt;h2 id=&quot;2-方法介绍&quot;&gt;2. 方法介绍&lt;/h2&gt;

&lt;p&gt;本文提出的&lt;strong&gt;CLIBD&lt;/strong&gt;方法旨在融合图像和DNA数据，以构建一个强大的多模态表示空间。该模型包括一个图像编码器、一个DNA编码器和一个文本编码器。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;图像编码器: 使用&lt;strong&gt;ViT-B1&lt;/strong&gt;，在&lt;strong&gt;ImageNet-21k&lt;/strong&gt;上预训练，在&lt;strong&gt;ImageNet-1k&lt;/strong&gt;上微调；&lt;/li&gt;
  &lt;li&gt;DNA编码器：使用&lt;strong&gt;5-mer tokenization BarcodeBERT&lt;/strong&gt;，在893k个DNA序列上预训练；&lt;/li&gt;
  &lt;li&gt;文本编码器：使用&lt;strong&gt;BERT-Small&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67371781d29ded1a8c856edd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在训练过程中，&lt;strong&gt;CLIBD&lt;/strong&gt;采用对比学习策略，通过最大化相同物种的图像、DNA和文本特征之间的相似性，同时最小化不同物种之间的相似性，来学习一个有效的多模态表示空间。对于图像-DNA对$(X,D)$、DNA-文本对$(D,T)$和图像-文本对$(X,T)$，分别构造对比损失：&lt;/p&gt;

\[L_{XD} = \sum_{i=1}^n \left( -\log \frac{\exp(X_i^TD_i/\tau)}{\sum_{j=1}^n \exp(X_j^TD_j/\tau)}-\log \frac{\exp(D_i^TX_i/\tau)}{\sum_{j=1}^n \exp(D_j^TX_j/\tau)} \right) \\
L_{DT} = \sum_{i=1}^n \left( -\log \frac{\exp(D_i^TT_i/\tau)}{\sum_{j=1}^n \exp(D_j^TT_j/\tau)}-\log \frac{\exp(T_i^TD_i/\tau)}{\sum_{j=1}^n \exp(T_j^TD_j/\tau)} \right) \\
L_{XT} = \sum_{i=1}^n \left( -\log \frac{\exp(X_i^TT_i/\tau)}{\sum_{j=1}^n \exp(X_j^TT_j/\tau)}-\log \frac{\exp(T_i^TX_i/\tau)}{\sum_{j=1}^n \exp(T_j^TX_j/\tau)} \right) \\\]

&lt;p&gt;该方法可以利用实际数据中部分可用的分类标签来构建多模态表示空间，进一步提高了表示的鲁棒性。&lt;/p&gt;

&lt;p&gt;为了使用该模型预测分类标签，计算输入图像（查询）与从可用物种中采样的 DNA 嵌入（键）之间的余弦相似度。使用与最接近的键匹配的分类标签（目、科、属、种）作为预测。这种方法允许在零样本设置下对模型在训练期间未见过的物种进行评估。&lt;/p&gt;

&lt;h2 id=&quot;3-实验分析&quot;&gt;3. 实验分析&lt;/h2&gt;

&lt;p&gt;本文使用了一个包含大量昆虫图像和DNA条形码的数据集&lt;strong&gt;BIOSCAN-1M&lt;/strong&gt;进行训练和评估。该数据集包括多个物种的图像和DNA序列，部分物种仅有图像数据，部分物种仅有DNA数据，还有部分物种同时具有两种数据；只使用有分类标签的数据。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673719e3d29ded1a8c8859de.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;实验结果表明，&lt;strong&gt;CLIBD&lt;/strong&gt;方法在零样本学习任务上比先前的单模态方法提高了8\%以上的准确率。此外，该方法在具有部分分类标签的数据集上也表现出色，进一步证明了其有效性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67371e83d29ded1a8c8e5263.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;除了与单模态方法进行对比外，本文还与&lt;strong&gt;BioCLIP&lt;/strong&gt;等先进的多模态方法进行了对比实验。实验结果表明，&lt;strong&gt;CLIBD&lt;/strong&gt;方法在多个评估指标上均优于&lt;strong&gt;BioCLIP&lt;/strong&gt;等方法，进一步证明了其优越性和实用性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67371a55d29ded1a8c88ef71.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过分析模型的注意力分布和特征表示，发现&lt;strong&gt;CLIBD&lt;/strong&gt;方法能够更准确地捕捉图像和DNA数据之间的关联信息，从而提高了生物分类的准确性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67371abcd29ded1a8c8979ac.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 15 Nov 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/15/clibd.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/15/clibd.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>Adapting the Segment Anything Model for Plant Recognition and Automated Phenotypic Parameter Measurement</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;微调SAM模型实现植物识别和表型参数自动测量.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;paper：&lt;a href=&quot;https://www.mdpi.com/2311-7524/10/4/398&quot;&gt;Adapting the Segment Anything Model for Plant Recognition and Automated Phenotypic Parameter Measurement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;/h2&gt;

&lt;p&gt;传统的表型分析依赖于专家通过视觉检查植物的物理特征，这一费时费力的过程严重阻碍了新品种的高效选育。本研究引入了一种高效的植物识别框架，该框架利用了可解释对比语言图像预训练（&lt;strong&gt;ECLIP&lt;/strong&gt;）指导下的分割一切模型（&lt;strong&gt;SAM&lt;/strong&gt;）。这种方法可以应用于各种植物类型，为了提高植物表型测量的准确性，在植物成分骨架提取过程中加入了&lt;strong&gt;b&lt;/strong&gt;样条曲线。实验结果证明了该方法的有效性，表明该框架对大多数测试样本的平均绝对误差小于0.05。这种性能是在不需要模型训练或标记数据的情况下实现的，突出了框架的实用性和效率。&lt;/p&gt;

&lt;h2 id=&quot;1-背景介绍&quot;&gt;1. 背景介绍&lt;/h2&gt;

&lt;p&gt;植物表型分析是农业研究中日益重要的一个方面，它涉及对植物性状的详细观察和精确测量，包括生长模式、作物产量以及对干旱、病虫害等各种生物和环境胁迫的抗性。这些特征通常是复杂的，并且可能受到遗传因素和环境条件之间相互作用的影响。植物表型在粮食安全和可持续农业中发挥着重要作用。&lt;/p&gt;

&lt;p&gt;以往的研究主要依靠面向图像的测量技术来分析表型性状。这一过程通常需要专家对植物进行物理检查，并记录各种性状的数据，如植物高度、叶面积和颜色。植物表型的最新进展涉及计算机视觉和深度学习的整合，这些方法用深度学习算法对植物图像进行分析，允许准确预测各种植物性状，实现高通量和非破坏性表型测量。&lt;/p&gt;

&lt;p&gt;本研究提出了一种基于深度学习的植物分割和表型性状测量的零样本系统，克服了有限的标记数据和复杂的室外环境的挑战。本文的贡献包括：(1)一个预处理模块，以提高数据集图像质量；(2)基于可解释对比语言-图像预训练（&lt;strong&gt;ECLIP&lt;/strong&gt;）算法的基于&lt;strong&gt;SAM&lt;/strong&gt;的零样本分割方法，消除了人工标注数据的需要；(3)利用&lt;strong&gt;b&lt;/strong&gt;样条曲线作为测量植物长宽的基础，提高了测量精度；(4)与有监督方法相比，该框架实现了更好的分割性能和推理速度。&lt;/p&gt;

&lt;h2 id=&quot;2-植物表型数据集&quot;&gt;2. 植物表型数据集&lt;/h2&gt;

&lt;p&gt;本研究引入了一个样本表型数据库。该数据集包括三种植物品种的图像：萝卜、黄瓜和南瓜。这些照片是用三星&lt;strong&gt;Galaxy S22 ultra&lt;/strong&gt;拍摄的。数据收集于2022年8月至2023年6月在韩国京畿道的一个受控温室设施中进行。为了保持均匀的光照条件并减少所收集图像之间的差异，数据收集在太阳正午（上午11点至下午12点30分）前后的90分钟窗口内进行。&lt;/p&gt;

&lt;p&gt;下图演示了使用智能手机捕获图像所采用的标准化过程。使用三脚架将智能手机相机固定在相对于成像平台的均匀距离和角度上。三脚架放在平台的底部。在整个图像采集过程中，站立杆作为保持正确定位的恒定参考。这种设置确保每张图像都以一致的对齐方式拍摄，从而减少了变化并增加了后续过程的可靠性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee2c9d29ded1a8cca8248.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图显示了&lt;strong&gt;1600&lt;/strong&gt;个带标签的植物图像在训练、验证和测试集中的分布。作者分配了80%的数据（1280张图像）用于训练和验证。其中1024张图像用于训练，其余256张用于验证。剩下的数据集（20%、320张图像）被保留用于测试。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee31dd29ded1a8ccac6de.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-方法介绍&quot;&gt;3. 方法介绍&lt;/h2&gt;

&lt;p&gt;本文提出的零样本植物成分识别和表型性状测量框架如图所示。包含以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;预处理：本研究进行了两种预处理方法，即颜色校准和图像对齐。色彩校准旨在校正由相机设置、照明变化或传感器规格引起的色彩再现不一致；图像对齐解决了由于相机运动，风吹植物或不平坦地形引起的空间不对准问题。最后计算比例因子，将图像空间系统转换为实际空间系统。&lt;/li&gt;
  &lt;li&gt;无标签分割：&lt;strong&gt;ECLIP&lt;/strong&gt;处理植物部位的文本描述，并直接在图像上生成关键点位置；这些点作为&lt;strong&gt;SAM&lt;/strong&gt;的引导信号，识别和分割植物成分；最后通过后处理步骤细化分割掩码，消除错误分割的区域。&lt;/li&gt;
  &lt;li&gt;表型性状测量：利用无标签分割模块创建的分段掩码和计算出的比例因子，可以以现实世界为单位精确测量植物的各种表型性状，如宽度和长度。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee546d29ded1a8ccc6f68.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1预处理&quot;&gt;（1）预处理&lt;/h3&gt;

&lt;p&gt;由于数据集是在现实世界条件下收集的，因此可能会受到各种因素的影响，导致图像不一致。为了保证数据集的质量，预处理步骤首先进行了颜色校准，然后使用了一个附加的几何变换模块。该模块将所有捕获的图像重新排列到一个标准化的角度和距离，以确保在不同图像之间准确地测量表型性状。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee5a5d29ded1a8cccbb35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;a. 颜色校正&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;颜色校准对于确保在不同照明条件下拍摄的图像中一致和准确的颜色至关重要，因为准确的颜色表示会显著影响提取表型性状的准确性。在图像捕获期间，将具有已知颜色值的参考图表附加到成像平台上。然后对捕获的图像进行处理，并在相同的照明条件下调整颜色以匹配检查器的颜色值。这种方法确保图像反映主体的实际颜色，而不管光线或相机设置的变化。&lt;/p&gt;

&lt;p&gt;理想情况下，目标图像（受控条件）中色块对应的&lt;strong&gt;RGB&lt;/strong&gt;值与源图像（室外条件）之间应存在直接的线性关系。然而，目标图像容易受到可变光照条件的影响，这可能导致偏离假定的线性关系。下图比较了源图像和参考图像的颜色检查矩阵。这些矩阵显示了两个图像中每个色块的平均红、绿、蓝值。很明显，在所有颜色通道中，源图像中的一些斑块偏离了预期的线性趋势线。这种偏差突出了颜色校准对精确和可靠结果的关键作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee756d29ded1a8cce0aa8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;b. 图像对齐&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在针孔相机模型的框架内，假设相机运动保持场景几何形状，在从不同视点捕获的同一场景的两幅图像之间可以通过单应性矩阵$H$建立基本联系。$H$采用$8$自由度的$3 \times 3$矩阵的形式，表示一种平面投影变换，能够将源图像中的点映射到目标模板中的对应点（从不同的视点捕获）。&lt;/p&gt;

&lt;p&gt;这项研究使用了一个平面成像平台。为了使输入图像与成像平台模板的透视图对齐，本文应用了单应变换。单应矩阵估计采用&lt;strong&gt;SuperGlue&lt;/strong&gt;，这是一种基于&lt;strong&gt;SuperPoint&lt;/strong&gt;关键点和描述符训练的图神经网络，可以实现鲁棒的特征匹配。&lt;/p&gt;

&lt;p&gt;图像对齐模块首先将包含植物样本的输入图像与成像平台参考模板配对。利用预训练的&lt;strong&gt;SuperGlue&lt;/strong&gt;模型准确识别两幅图像之间的匹配点对。这些匹配的对作为计算单应性矩阵$H$的基础，$H$封装了两个透视图之间的关系。最后以$H$为关键参数进行透视变换，对输入图像进行对齐。&lt;/p&gt;

&lt;h3 id=&quot;2无标签分割&quot;&gt;（2）无标签分割&lt;/h3&gt;

&lt;p&gt;本文使用可解释对比语言-图像预训练（&lt;strong&gt;ECLIP&lt;/strong&gt;）和&lt;strong&gt;SAM&lt;/strong&gt;模型来实现对关键植物成分（如叶子、果实）的精确分割。产生的植物掩码可能包含多个重叠和背景噪声，进一步通过后处理阶段来消除过小或过大的掩码，并根据预定义的阈值参数合并高度重叠的掩码。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee926d29ded1a8ccfc751.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;a. 点提示生成&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ECLIP&lt;/strong&gt;是&lt;strong&gt;CLIP&lt;/strong&gt;模型的增强版本，它允许通过图像-文本相似图（&lt;strong&gt;ITSM&lt;/strong&gt;）对其预测进行可视化解释。&lt;strong&gt;ITSM&lt;/strong&gt;图测量每个图像的特征映射和其相应文本描述的嵌入之间的相似性，可用于识别与文本描述最相关的图像区域。在无标签植物分割的背景下，使用相似性分数超过&lt;strong&gt;0.8&lt;/strong&gt;的&lt;strong&gt;ECLIP&lt;/strong&gt;点作为前景点提示输入来引导&lt;strong&gt;SAM&lt;/strong&gt;；使用相似性分数最低的若干点作为背景点。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;b. 零样本分割&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SAM&lt;/strong&gt;可以仅使用提示来处理以前从未见过的目标，这使得它适用于一系列分割任务。考虑到植物在输入图像中的未知位置，使用&lt;strong&gt;ECLIP&lt;/strong&gt;模型提出的点提示作为提示编码器的输入，将提示编码器的输出与图像编码器的输出连接起来，将这个组合输入到掩码解码器中预测输入图像的分割掩码。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;c. 掩码后处理&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当使用点作为分割输入提示时，所得到的分割掩码通常包含许多高度重叠的掩码和来自背景的噪声。为了解决这个问题，本文实现了一个掩码后处理算法。该算法根据&lt;strong&gt;IoU&lt;/strong&gt;和重叠率两个阈值去除过大或过小的掩码，并对重复或基本重叠的掩码进行合并（超过&lt;strong&gt;IoU&lt;/strong&gt;或重叠率阈值的掩码被合并为单个掩码）。&lt;/p&gt;

&lt;p&gt;考虑到数据集中三种植物类型的大小不同，只保留面积在总图像面积的&lt;strong&gt;5%&lt;/strong&gt;到&lt;strong&gt;50%&lt;/strong&gt;范围内的掩码。在合并重复掩码时，重叠阈值和&lt;strong&gt;IoU&lt;/strong&gt;阈值为0.88。该值的选择与&lt;strong&gt;SAM&lt;/strong&gt;的默认阈值一致。&lt;/p&gt;

&lt;h3 id=&quot;3表型性状测量&quot;&gt;（3）表型性状测量&lt;/h3&gt;

&lt;p&gt;从无标签分割模块中提取的分节植物掩码可用于精确测量植物的宽度和长度等表型性状。长度定义为通过器官的中线最长的一段（不包括茎）。然而确定植物的宽度是具有挑战性的，因为有无数条线可以垂直于植物的中轴线，因此测量的宽度可以根据所选择的测量线而变化。因此在沿中轴的不同点测量宽度特征，以创建一个宽度测量集合，单个宽度值表示为宽度剖面的中位数。&lt;/p&gt;

&lt;p&gt;为了捕获输入掩码的一般结构，首先应用骨架化来获得其内侧轴的粗表示。然而由于某些植物的复杂形状，由此产生的粗糙的内轴线可能有多个分支，并且可能不会与掩码的边界相交。为了解决这些问题并提高测量精度，将b样条曲线拟合到骨架的粗内轴线上。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673eebaad29ded1a8cd1bb4d.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-实验分析&quot;&gt;4. 实验分析&lt;/h2&gt;

&lt;p&gt;下图展示了预处理（包括图像对齐和颜色校准）在三种不同植物物种（南瓜、黄瓜和萝卜）上的关键作用。图像对齐模块有效地将输入图像重新对齐为成像平台的精确鸟瞰图，丢弃不相关的区域，简化下游处理任务；颜色校准利用参考模板来纠正颜色再现中的不一致，由此产生的输出图像显示出显著提高的视觉保真度，与现实生活中的图像更接近。结果强调了预处理在实现准确可靠的分割和表型性状测量结果中的重要性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673eec55d29ded1a8cd23b48.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下表给出了所提出的零样本植物成分识别模型对数据集中三种植物类型（包括萝卜、黄瓜和南瓜）的分割性能。尽管存在现实世界数据的挑战（不同的光照和遮挡），所提植物成分分割框架在所有三种植物类型上都取得了良好的性能，这表明模型可以有效地分割各种植物成分，而无需针对这些类型的任何特定数据训练。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673eecb7d29ded1a8cd29de3.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://pic.imgdb.cn/item/673eecc7d29ded1a8cd2b2f6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图展示了四种新型植物在现实世界环境下的零样本模型的预测掩码。&lt;strong&gt;ECLIP&lt;/strong&gt;的注意力掩码有效地确定了图像中潜在的重要植物成分区域，即使是具有各种形状和背景的植物。因此所提框架能够准确地引导&lt;strong&gt;SAM&lt;/strong&gt;对植物成分进行精确分割。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673eed68d29ded1a8cd38a01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了评估表型性状测量的准确性，从测试数据集中随机选择了300个样本。对于每个样本使用每个图像中的标尺手动记录宽度和长度特征的地面真值。然后使用比例因子将每个测试图像的预测长度和宽度特征测量值从二维图像空间系统转换到现实世界的物体空间系统。&lt;/p&gt;

&lt;p&gt;下图绘制了预测宽度和长度的误差分布，提供了对测量性能的分析。在大多数情况下，&lt;strong&gt;MAE&lt;/strong&gt;保持在&lt;strong&gt;0.05&lt;/strong&gt;以内。然而，一些异常值的长度和宽度的&lt;strong&gt;MAE&lt;/strong&gt;误差都超过&lt;strong&gt;0.1&lt;/strong&gt;。通过对故障案例的调查，发现最常见的误差来源是对茎部分的准确检测。在未成熟样本的情况下，准确的鉴定茎是一个重大挑战，因为茎的宽度与身体部分的宽度具有欺骗性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673eedfdd29ded1a8cd44f04.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Nov 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/03/eclip.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/03/eclip.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>Adapting Vision Foundation Models for Plant Phenotyping</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;微调视觉基础模型用于植物表型分析.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;paper：&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Chen_Adapting_Vision_Foundation_Models_for_Plant_Phenotyping_ICCVW_2023_paper.html&quot;&gt;Adapting Vision Foundation Models for Plant Phenotyping&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;/h2&gt;

&lt;p&gt;视觉基础模型通常是在来自互联网的图像或文本上进行预训练的，因此它们在特定领域（如植物表型）的表现受到质疑。本文研究了视觉基础模型对植物表型任务的适应性。作者对三个植物表型任务（叶片计数、实例分割和疾病分类）进行了实验，对&lt;strong&gt;MAE&lt;/strong&gt;、&lt;strong&gt;DINO&lt;/strong&gt;和&lt;strong&gt;DINOv2&lt;/strong&gt;三个基础模型进行了微调，同时评估两种不同的微调方法：&lt;strong&gt;LoRA&lt;/strong&gt;和解码器微调。实验结果表明，视觉基础模型可以有效地适应多种植物表型任务，其性能与专门为每种任务设计&lt;strong&gt;SoTA&lt;/strong&gt;模型相似。但是在某些情况下，微调的基础模型比特定任务的&lt;strong&gt;SoTA&lt;/strong&gt;模型执行得稍微差一些。&lt;/p&gt;

&lt;h2 id=&quot;1-背景介绍&quot;&gt;1. 背景介绍&lt;/h2&gt;

&lt;p&gt;植物表型分型旨在定量评估植物的结构和功能属性，这些属性在现代农业领域至关重要。然而大多数模型都是为特定任务设计的：虽然它们精通于执行训练任务，但当应用于其他任务时，它们的性能会下降。这种通用性的缺乏使得应用于表型图像的深度学习模型的实现和部署效率低下，因为需要为每个特定任务设计、开发和训练新的模型。&lt;/p&gt;

&lt;p&gt;近年来基础模型的发展为解决这一问题提供了一条有希望的途径。基础模型是具有大量可训练参数的模型，并在广泛的数据上进行预训练，有可能很容易适应各种新的下游任务。这些基础模型有两个主要的限制。首先，基础模型通常是在一般领域数据上训练的，缺乏专门化和特定于领域的知识；其次，基础模型是巨大的和复杂的，对如此大的模型进行微调非常耗时，并且需要硬件基础设施。&lt;/p&gt;

&lt;p&gt;本研究探索了是否有可能通过有效地微调预训练的基础模型来解决不同的植物表型任务，重点研究了基于&lt;strong&gt;ViT&lt;/strong&gt;的三种基础模型：&lt;strong&gt;MAE&lt;/strong&gt;、&lt;strong&gt;DINO&lt;/strong&gt;和&lt;strong&gt;DINOv2&lt;/strong&gt;，使用微调解码器或&lt;strong&gt;LoRA&lt;/strong&gt;来评估这些模型在以下植物表型相关任务上的性能，包括叶片计数、实例分割和疾病分类。&lt;/p&gt;

&lt;h2 id=&quot;2-方法介绍&quot;&gt;2. 方法介绍&lt;/h2&gt;

&lt;p&gt;本文评估的基础模型包括&lt;strong&gt;MAE&lt;/strong&gt;、&lt;strong&gt;DINO&lt;/strong&gt;和&lt;strong&gt;DINOv2&lt;/strong&gt;，因为&lt;strong&gt;MAE&lt;/strong&gt;和&lt;strong&gt;DINO&lt;/strong&gt;是在同一数据集（&lt;strong&gt;ImageNet-1k&lt;/strong&gt;）上使用不同的训练方法进行预训练的，而&lt;strong&gt;DINO&lt;/strong&gt;和&lt;strong&gt;DINOv2&lt;/strong&gt;是在不同的数据集上使用相同的方法进行预训练的（&lt;strong&gt;DINOv2&lt;/strong&gt;是在一个更大的数据集&lt;strong&gt;LVD-142M&lt;/strong&gt;上进行预训练的）。&lt;/p&gt;

&lt;p&gt;本文在三个基本的植物表型任务上进行了广泛的实验：叶片计数，分割和疾病分类。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;叶片计数和分割数据集采用&lt;strong&gt;CVPPP 2017&lt;/strong&gt;，由&lt;strong&gt;A1-A5&lt;/strong&gt;环境下捕获的植物图像组成，&lt;strong&gt;A1&lt;/strong&gt;、&lt;strong&gt;A2&lt;/strong&gt;和&lt;strong&gt;A4&lt;/strong&gt;包含拟南芥植物的不同突变体，&lt;strong&gt;A3&lt;/strong&gt;由烟草植物组成。数据集的测试集还包含&lt;strong&gt;A5&lt;/strong&gt;，其中的图像是从前面四个域中采样的混合图像。&lt;/li&gt;
  &lt;li&gt;叶片疾病分类采用&lt;strong&gt;Cassava&lt;/strong&gt;数据集，是在乌干达拍摄的真实世界野外图像的集合，旨在识别木薯叶片中发现的各种疾病。数据集的训练集包括21,397张分辨率为&lt;strong&gt;800×600&lt;/strong&gt;像素的图像，其中每张图像被分类为代表木薯叶片健康状况的五个类别之一：木薯细菌性枯萎病（&lt;strong&gt;CBB&lt;/strong&gt;），木薯褐条病（&lt;strong&gt;CBSD&lt;/strong&gt;），木薯绿斑病（&lt;strong&gt;CGM&lt;/strong&gt;），木薯花叶病（&lt;strong&gt;CMD&lt;/strong&gt;）和健康(&lt;strong&gt;H&lt;/strong&gt;)。测试集包含大约&lt;strong&gt;15000&lt;/strong&gt;张保存在&lt;strong&gt;Kaggle&lt;/strong&gt;上的图片。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673eced0d29ded1a8cba4654.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本研究中评估的模型由两个主要组成部分组成，一个预训练的主干和一个特定任务的解码器。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;主干是使用&lt;strong&gt;MAE&lt;/strong&gt;， &lt;strong&gt;DINO&lt;/strong&gt;或&lt;strong&gt;DINOv2&lt;/strong&gt;预训练的&lt;strong&gt;ViT b&lt;/strong&gt;特征提取器。&lt;/li&gt;
  &lt;li&gt;叶片计数和疾病分类的解码器是放置在主干上的线性层，用于回归叶片数量或预测疾病类别。&lt;/li&gt;
  &lt;li&gt;叶子分割的解码器采用&lt;strong&gt;ViTDet&lt;/strong&gt;，由特征金字塔网络生成多尺度特征和&lt;strong&gt;Mask RCNN&lt;/strong&gt;解码器预测实例掩码组成。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在实验中为每个模型实现了两种不同的微调配置，即微调解码器和&lt;strong&gt;LoRA&lt;/strong&gt;。在这两种配置下，&lt;strong&gt;ViT b&lt;/strong&gt;网络的预训练权值都被冻结。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在解码器调优期间，只训练解码器引入的参数。这种设置检查了基础模型的直接适应性，这些模型还没有暴露于特定的植物表型数据集，调整工作最少。&lt;/li&gt;
  &lt;li&gt;在&lt;strong&gt;LoRA&lt;/strong&gt;调优中，采用&lt;strong&gt;LoRA&lt;/strong&gt;对主干网络进行微调，使&lt;strong&gt;LoRA&lt;/strong&gt;和解码器添加的参数都是可训练的。该配置研究了预训练主干的有效微调是否可以产生更好的性能。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ece96d29ded1a8cba1a40.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-实验分析&quot;&gt;3. 实验分析&lt;/h2&gt;

&lt;h3 id=&quot;1叶片计数&quot;&gt;（1）叶片计数&lt;/h3&gt;

&lt;p&gt;叶片计数的评估指标为计数差（&lt;strong&gt;DiC&lt;/strong&gt;），计数绝对差（&lt;strong&gt;|DiC|&lt;/strong&gt;），均方误差（&lt;strong&gt;MSE&lt;/strong&gt;）和百分比一致性（&lt;strong&gt;PA&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ecf65d29ded1a8cbab66e.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;结果表明，使用&lt;strong&gt;LoRA&lt;/strong&gt;可以帮助将基础模型推广到叶片计数任务，因为&lt;strong&gt;LoRA&lt;/strong&gt;的性能始终优于&lt;strong&gt;DT&lt;/strong&gt;，可能能够减轻数据稀缺和领域转移的影响。此外由于&lt;strong&gt;DINOv2&lt;/strong&gt;是在一个比&lt;strong&gt;DINO&lt;/strong&gt;和&lt;strong&gt;MAE&lt;/strong&gt;更大的预训练数据集上进行预训练的，这表明通过增加精心选择的样本预训练源的规模可以提高基础模型的适应性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ed02dd29ded1a8cbb5fa0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2叶片分割&quot;&gt;（2）叶片分割&lt;/h3&gt;

&lt;p&gt;叶片分割的评估指标为所有叶子之间的最佳&lt;strong&gt;DICE&lt;/strong&gt;分数（&lt;strong&gt;BestDice&lt;/strong&gt;），前景掩模上的&lt;strong&gt;DICE&lt;/strong&gt; (&lt;strong&gt;FgBgDice&lt;/strong&gt;)，叶子计数差异（&lt;strong&gt;DiffFG&lt;/strong&gt;）和叶子计数绝对差异（&lt;strong&gt;|DiffFG|&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;结果表明，将&lt;strong&gt;MAE&lt;/strong&gt;用于叶片实例分割的结果略差于&lt;strong&gt;SOTA&lt;/strong&gt;模型，而微调后的&lt;strong&gt;DINO&lt;/strong&gt;模型的效果则差得多。由于解码器和&lt;strong&gt;LoRA&lt;/strong&gt;添加的参数数量之间的严重不平衡，&lt;strong&gt;LoRA&lt;/strong&gt;的性能可能不会超过&lt;strong&gt;DT&lt;/strong&gt;。可视化结果也表明被评估的模型可能会遗漏叶或茎。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ed220d29ded1a8cbce1ee.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://pic.imgdb.cn/item/673ed22dd29ded1a8cbcecba.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3叶片分类&quot;&gt;（3）叶片分类&lt;/h3&gt;

&lt;p&gt;叶片分类的评估指标为分类准确率。&lt;/p&gt;

&lt;p&gt;结果表明，&lt;strong&gt;LoRA&lt;/strong&gt;持续提高叶片病害分类的模型适应性。在评估的模型中，&lt;strong&gt;DINOv2-LoRA&lt;/strong&gt;表现最好，性能略低于&lt;strong&gt;SoTA&lt;/strong&gt;。&lt;strong&gt;LoRA&lt;/strong&gt;显示了解决类别不平衡的潜力。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ed2b3d29ded1a8cbd57af.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4实验总结&quot;&gt;（4）实验总结&lt;/h3&gt;

&lt;p&gt;在评估的基础模型中，&lt;strong&gt;DINOv2-LoRA&lt;/strong&gt;在叶片计数和病害分类上的表现最好，在各任务上与&lt;strong&gt;SoTA&lt;/strong&gt;模型的表现接近。在叶子分割中，&lt;strong&gt;MAE&lt;/strong&gt;优于&lt;strong&gt;DINO&lt;/strong&gt;，接近&lt;strong&gt;SoTA&lt;/strong&gt;水平。一般来说，每个基础模型都可以有效地适应所有这三种任务，并具有可接受的性能。&lt;/p&gt;

&lt;p&gt;在微调方法方面，&lt;strong&gt;LoRA&lt;/strong&gt;在叶片计数和疾病分类方面始终优于解码器微调。此外，&lt;strong&gt;LoRA&lt;/strong&gt;显示了缓解与数据稀缺、领域转移和类不平衡相关的问题的潜力。然而在叶子分割中，&lt;strong&gt;LoRA&lt;/strong&gt;无法超越解码器微调来进一步改进基础模型，这可能是由于分割解码器比&lt;strong&gt;LoRA&lt;/strong&gt;引入了更多的参数。&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Nov 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/02/avfm.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/02/avfm.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
      <item>
        <title>表型图像分析(Phenotypic Image Analysis)</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Phenotypic Image Analysis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;表型(&lt;strong&gt;Phenotype&lt;/strong&gt;)是指生物体的外部特征和性状（比如形态、结构、颜色、纹理等）。作为生物信息和深度学习的交叉领域，表型图像分析旨在利用计算机视觉技术对生物体的表型特征进行可视化和量化，进一步自动捕获和分析与生物体的形态、结构和生长模式相关的数据。&lt;/p&gt;

&lt;p&gt;表型图像分析广泛应用于植物学、医学、遗传学等领域，以帮助研究人员理解生物体的特征与其基因型和环境之间的关系。通过分析这些图像数据，研究人员可以进行更详细的表型评估，有助于加速生物研究和育种等应用。&lt;/p&gt;

&lt;p&gt;本文前序章节记录了一些&lt;strong&gt;前沿的&lt;/strong&gt;表型图像分析相关工作，并按照顶会的投递顺序进行整理。（值得一提的是，相关工作均是通过&lt;a href=&quot;https://openreview.net/&quot;&gt;&lt;strong&gt;OpenReview&lt;/strong&gt;&lt;/a&gt;在顶会出分的时候进行整理的，并不代表该工作最后&lt;strong&gt;被接收&lt;/strong&gt;。检索关键词包括：&lt;strong&gt;phenotype, plant, argi, bio&lt;/strong&gt;）本文最后一章汇总了表型图像分析领域的&lt;strong&gt;视觉基础模型&lt;/strong&gt;相关工作。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ICLR 2025 Submissions&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale&lt;/li&gt;
      &lt;li&gt;DODA: Diffusion for Object-detection Domain Adaptation in Agriculture&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vision Foundation Model of Plant Phenotyping&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;Adapting the Segment Anything Model for Plant Recognition and Automated Phenotypic Parameter Measurement&lt;/li&gt;
      &lt;li&gt;Adapting Vision Foundation Models for Plant Phenotyping&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;-iclr-2025-submissions&quot;&gt;👉 ICLR 2025 Submissions&lt;/h2&gt;

&lt;h3 id=&quot;-clibd-bridging-vision-and-genomics-for-biodiversity-monitoring-at-scale&quot;&gt;⚪ &lt;a href=&quot;https://0809zheng.github.io/2024/11/15/clibd.html&quot;&gt;&lt;font color=&quot;blue&quot;&gt;CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale&lt;/font&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;CLIBD&lt;/strong&gt;旨在融合生物图像、分类类别和DNA数据，以构建一个强大的多模态表示空间，以提高生物多样性监测的准确性。该模型包括一个图像编码器、一个DNA编码器和一个文本编码器。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLIBD&lt;/strong&gt;在训练时采用对比学习策略，通过最大化相同物种的图像、DNA和文本特征之间的相似性，同时最小化不同物种之间的相似性，来学习一个有效的多模态表示空间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CLIBD&lt;/strong&gt;在推理时可以在零样本设置下对新物种进行评估。为了预测分类标签，计算输入图像与从可用物种中采样的 DNA 嵌入之间的余弦相似度，使用与最接近的键匹配的分类标签（目、科、属、种）作为预测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/67371781d29ded1a8c856edd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;-doda-diffusion-for-object-detection-domain-adaptation-in-agriculture&quot;&gt;⚪ &lt;a href=&quot;https://0809zheng.github.io/2024/11/22/doda.html&quot;&gt;&lt;font color=&quot;blue&quot;&gt;DODA: Diffusion for Object-detection Domain Adaptation in Agriculture&lt;/font&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;本文提出了一个具有领域特征自适应和布局图像条件生成的框架&lt;strong&gt;DODA&lt;/strong&gt;，以增强扩散模型生成新的农业领域检测数据的能力。只需从目标域获取少量参考图像，&lt;strong&gt;DODA&lt;/strong&gt;就可以为其生成数据，而无需额外的训练。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DODA&lt;/strong&gt;通过将领域信息集成到&lt;strong&gt;L2I&lt;/strong&gt;扩散中来实现领域感知图像的生成。首先将布局表示为图像，然后使用预训练的布局编码器提取特征作为域嵌入，最后通过特征加法融合整合到&lt;strong&gt;L2I&lt;/strong&gt;扩散中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/674051e4d29ded1a8cda4c4a.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-vision-foundation-model-of-plant-phenotyping&quot;&gt;👉 Vision Foundation Model of Plant Phenotyping&lt;/h2&gt;

&lt;h3 id=&quot;-adapting-the-segment-anything-model-for-plant-recognition-and-automated-phenotypic-parameter-measurement-horticulturae-2024&quot;&gt;⚪ &lt;a href=&quot;https://0809zheng.github.io/2024/11/03/eclip.html&quot;&gt;&lt;font color=&quot;blue&quot;&gt;Adapting the Segment Anything Model for Plant Recognition and Automated Phenotypic Parameter Measurement&lt;/font&gt;&lt;/a&gt; (Horticulturae 2024)&lt;/h3&gt;

&lt;p&gt;本文开发了一套零样本植物成分识别和表型性状测量框架：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;预处理：颜色校准和图像对齐，计算坐标转换的比例因子；&lt;/li&gt;
  &lt;li&gt;无标签分割：使用&lt;strong&gt;ECLIP&lt;/strong&gt;通过植物部位的文本描述在图像上生成前景和背景点位置；通过这些点引导&lt;strong&gt;SAM&lt;/strong&gt;识别和分割植物成分；最后消除和合并错误分割的区域。&lt;/li&gt;
  &lt;li&gt;表型性状测量：对输出掩码进行骨架化并通过b样条曲线拟合，可以精确测量植物的各种表型性状，如宽度和长度。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ee546d29ded1a8ccc6f68.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;-adapting-vision-foundation-models-for-plant-phenotyping-iccvw-2023&quot;&gt;⚪ &lt;a href=&quot;https://0809zheng.github.io/2024/11/02/avfm.html&quot;&gt;&lt;font color=&quot;blue&quot;&gt;Adapting Vision Foundation Models for Plant Phenotyping&lt;/font&gt;&lt;/a&gt; (ICCVW 2023)&lt;/h3&gt;

&lt;p&gt;本文研究了视觉基础模型对植物表型任务的适应性。作者对三个植物表型任务（叶片计数、实例分割和疾病分类）进行了实验，对&lt;strong&gt;MAE&lt;/strong&gt;、&lt;strong&gt;DINO&lt;/strong&gt;和&lt;strong&gt;DINOv2&lt;/strong&gt;三个基础模型进行了微调，同时评估两种不同的微调方法：&lt;strong&gt;LoRA&lt;/strong&gt;和解码器微调。实验结果表明，视觉基础模型可以有效地适应多种植物表型任务，其性能与专门为每种任务设计&lt;strong&gt;SoTA&lt;/strong&gt;模型相似。但是在某些情况下，微调的基础模型比特定任务的&lt;strong&gt;SoTA&lt;/strong&gt;模型执行得稍微差一些。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673ece96d29ded1a8cba1a40.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Nov 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/01/phenotype.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/01/phenotype.html</guid>
        
        <category>深度学习</category>
        
        
      </item>
    
      <item>
        <title>Pan-Mamba: Effective pan-sharpening with State Space Model</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Pan-Mamba：通过状态空间模型实现高效全色锐化.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;paper：&lt;a href=&quot;https://arxiv.org/abs/2402.12192&quot;&gt;Pan-Mamba: Effective pan-sharpening with State Space Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;/h2&gt;

&lt;p&gt;本文提出了一种新颖的全色锐化网络&lt;strong&gt;Pan-Mamba&lt;/strong&gt;，该网络利用了状态空间模型（&lt;strong&gt;State Space Model&lt;/strong&gt;）来处理低分辨率多光谱（&lt;strong&gt;LRMS&lt;/strong&gt;）图像和高分辨率全色（&lt;strong&gt;PAN&lt;/strong&gt;）图像的信息融合。&lt;strong&gt;Pan-Mamba&lt;/strong&gt;通过初步的跨模态交互、高效的特征提取与融合，以及冗余模态特征的过滤，实现了高分辨率多光谱图像的生成。实验结果表明，&lt;strong&gt;Pan-Mamba&lt;/strong&gt;在多个数据集上均优于当前最先进的方法，不仅在定量指标上有所提升，还在真实世界的全分辨率数据集上展现了强大的泛化能力。&lt;/p&gt;

&lt;h2 id=&quot;1-背景介绍&quot;&gt;1. 背景介绍&lt;/h2&gt;

&lt;p&gt;全色锐化（&lt;strong&gt;Pan-sharpening&lt;/strong&gt;）技术旨在结合低分辨率多光谱图像和高分辨率全色图像的信息，以生成高分辨率多光谱图像。多光谱图像包含丰富的光谱信息，但分辨率较低；而全色图像则具有较高的空间分辨率，但缺乏光谱信息。通过全色锐化技术，可以融合这两种互补信息，从而生成既具有高空间分辨率又包含丰富光谱信息的图像。&lt;/p&gt;

&lt;p&gt;近年来，状态空间模型在计算机视觉领域取得了显著进展，特别是在长距离依赖建模方面。&lt;strong&gt;Mamba&lt;/strong&gt;模型通过引入输入自适应机制，增强了状态空间模型的能力，实现了更高的推理速度、吞吐量和整体性能。然而，状态空间模型在多模态图像融合（如全色锐化）方面的潜力尚未得到充分探索。因此，本文提出了&lt;strong&gt;Pan-Mamba&lt;/strong&gt;模型，旨在利用状态空间模型的优势来解决全色锐化问题。&lt;/p&gt;

&lt;h2 id=&quot;2-方法介绍&quot;&gt;2. 方法介绍&lt;/h2&gt;

&lt;p&gt;状态空间模型的概念最初在&lt;strong&gt;S4&lt;/strong&gt;模型中提出，该模型具有全局信息建模的有效架构，相较于传统的卷积神经网络（&lt;strong&gt;CNN&lt;/strong&gt;）或&lt;strong&gt;Transformer&lt;/strong&gt;架构具有显著优势。随后，&lt;strong&gt;S5&lt;/strong&gt;模型进一步降低了复杂度，实现了线性级别的计算复杂度。&lt;strong&gt;H3&lt;/strong&gt;模型在此基础上进行了精炼和扩展，使模型在语言模型任务中能够与&lt;strong&gt;Transformer&lt;/strong&gt;相媲美。&lt;strong&gt;Mamba&lt;/strong&gt;模型则通过引入输入自适应机制，增强了状态空间模型的能力，相比同等规模的&lt;strong&gt;Transformer&lt;/strong&gt;具有更高的性能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pan-Mamba&lt;/strong&gt;模型架构由三个核心组件组成：&lt;strong&gt;Mamba&lt;/strong&gt;块、通道交换&lt;strong&gt;Mamba&lt;/strong&gt;块和跨模态&lt;strong&gt;Mamba&lt;/strong&gt;块。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mamba&lt;/strong&gt;块：用于建模&lt;strong&gt;PAN&lt;/strong&gt;和&lt;strong&gt;LRMS&lt;/strong&gt;特征内的长距离依赖关系。&lt;/li&gt;
  &lt;li&gt;通道交换&lt;strong&gt;Mamba&lt;/strong&gt;块：通过交换&lt;strong&gt;PAN&lt;/strong&gt;和&lt;strong&gt;LRMS&lt;/strong&gt;特征的通道来鼓励特征交互，并引发它们之间的相关性。&lt;/li&gt;
  &lt;li&gt;跨模态&lt;strong&gt;Mamba&lt;/strong&gt;块：基于交叉注意力机制，实现&lt;strong&gt;PAN&lt;/strong&gt;和&lt;strong&gt;LRMS&lt;/strong&gt;特征的有效融合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673f1b56d29ded1a8c061be9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pan-Mamba&lt;/strong&gt;模型首先使用卷积层将两种图像投影到特征空间，并沿空间维度扁平化为令牌；然后利用状态空间模型的离散表示来更新隐藏状态，并生成输出；之后通过通道交换和跨模态&lt;strong&gt;Mamba&lt;/strong&gt;块实现特征的深度融合和冗余特征的过滤；最后通过反卷积层将融合后的特征重构为高分辨率多光谱图像。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673f1bfed29ded1a8c069c46.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://pic.imgdb.cn/item/673f1c17d29ded1a8c06b05f.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-实验分析&quot;&gt;3. 实验分析&lt;/h2&gt;

&lt;p&gt;实验采用了&lt;strong&gt;WorldView-II&lt;/strong&gt;、&lt;strong&gt;WorldView-III&lt;/strong&gt;和&lt;strong&gt;Gaofen-2&lt;/strong&gt;等多个数据集进行评估。评估指标包括峰值信噪比（&lt;strong&gt;PSNR&lt;/strong&gt;）、结构相似性指数（&lt;strong&gt;SSIM&lt;/strong&gt;）、光谱角映射（&lt;strong&gt;SAM&lt;/strong&gt;）和相对全局无量纲合成误差（&lt;strong&gt;ERGAS&lt;/strong&gt;）等。&lt;/p&gt;

&lt;p&gt;实验结果表明，&lt;strong&gt;Pan-Mamba&lt;/strong&gt;在多个数据集上均优于当前最先进的方法。在&lt;strong&gt;WorldView-II&lt;/strong&gt;和&lt;strong&gt;WorldView-III&lt;/strong&gt;数据集上，&lt;strong&gt;Pan-Mamba&lt;/strong&gt;在&lt;strong&gt;PSNR&lt;/strong&gt;指标上分别提升了&lt;strong&gt;0.38&lt;/strong&gt;和&lt;strong&gt;0.29&lt;/strong&gt;，同时在&lt;strong&gt;SSIM&lt;/strong&gt;和&lt;strong&gt;SAM&lt;/strong&gt;指标上也表现出显著的改进。此外，在真实世界的全分辨率&lt;strong&gt;WV2&lt;/strong&gt;数据集上，&lt;strong&gt;Pan-Mamba&lt;/strong&gt;也取得了优越的性能，验证了模型的强大泛化能力。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673f1ccfd29ded1a8c073c2c.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://pic.imgdb.cn/item/673f1d01d29ded1a8c075eb6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了验证状态空间模型的有效性，进行了消融实验。实验结果表明，状态空间模型在图像恢复任务中表现优于其他广泛认可的操作符，如卷积核和&lt;strong&gt;Transformer&lt;/strong&gt;等。这进一步证明了&lt;strong&gt;Pan-Mamba&lt;/strong&gt;模型中状态空间模型的核心作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673f1d4fd29ded1a8c07970c.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;与其他先进方法相比，&lt;strong&gt;Pan-Mamba&lt;/strong&gt;在保持高性能的同时，也具有较高的计算效率。这得益于状态空间模型的高效特征提取和融合能力，以及模型架构的轻量化设计。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic.imgdb.cn/item/673f1d40d29ded1a8c078c31.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Oct 2024 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2024/10/20/panmamba.html</link>
        <guid isPermaLink="true">http://localhost:4000/2024/10/20/panmamba.html</guid>
        
        <category>论文阅读</category>
        
        
      </item>
    
  </channel>
</rss>
