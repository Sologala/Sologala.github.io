<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Transformer">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="Transformer - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/04/25/transformer.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>Transformer</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-04-25"><i class="iconfont icon-date"></i>25 Apr 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5ea28751c2a9a83be5467bc1.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Transformer，基于Multi-head self-attention的Seq2Seq模型.</p>
</blockquote>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
  <li>code：<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
</ul>

<p><strong>Transformer</strong>是一个基于<a href="https://0809zheng.github.io/2020/04/24/self-attention.html#3-multi-head-self-attention">多头自注意力</a>(<strong>Multi-head Self-Attention</strong>)机制的模型，成为继多层感知机、卷积神经网络和循环神经网络之后又一个常用的深度学习模型。在原文中<strong>Transformer</strong>被提出用于进行<a href="https://0809zheng.github.io/2020/04/21/sequence-2-sequence.html">序列到序列</a>(<strong>Seq2Seq</strong>)建模，并适用于机器翻译等任务。目前该模型也被广泛应用于其他自然语言处理以及计算机视觉等领域。</p>

<p><strong>Transformer</strong>的基本结构如下图所示。
网络结构可以分成<strong>编码器Encoder</strong>和<strong>解码器Decoder</strong>两部分。根据不同的任务，有时候会用到不同的部分，如<strong>编码器</strong>部分常用于文本编码分类，<strong>解码器</strong>部分用于语言模型生成，完整的<strong>编码器-解码器</strong>结构用于机器翻译。</p>

<p><img src="https://pic.imgdb.cn/item/618b94ea2ab3f51d91f6d24e.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>   <span class="c1"># 词嵌入 Embedding 的维度
</span><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>     <span class="c1"># 前馈神经网络的隐藏层维度
</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_v</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># K(=Q), V向量的维度 
</span><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>    <span class="c1"># 编码器和解码器堆叠层数
</span><span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>     <span class="c1"># 自注意力头数
</span>
<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Encoder</span> <span class="o">=</span> <span class="nc">Encoder</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Decoder</span> <span class="o">=</span> <span class="nc">Decoder</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">):</span>                         <span class="c1"># enc_inputs: [batch_size, src_len]  
</span>                                                                       <span class="c1"># dec_inputs: [batch_size, tgt_len]
</span>        <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attns</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">Encoder</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>         <span class="c1"># enc_outputs: [batch_size, src_len, d_model], 
</span>                                                                       <span class="c1"># enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]
</span>        <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">Decoder</span><span class="p">(</span>
            <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">)</span>                       <span class="c1"># dec_outpus    : [batch_size, tgt_len, d_model], 
</span>                                                                       <span class="c1"># dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], 
</span>                                                                       <span class="c1"># dec_enc_attn  : [n_layers, batch_size, tgt_len, src_len]
</span>        <span class="n">dec_logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">projection</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">)</span>                      <span class="c1"># dec_logits: [batch_size, tgt_len, tgt_vocab_size]
</span>        <span class="k">return</span> <span class="n">dec_logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dec_logits</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">enc_self_attns</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span>
</code></pre></div></div>

<h2 id="1-网络结构">1. 网络结构</h2>

<h3 id="-编码器">① 编码器</h3>

<p><img src="https://pic.imgdb.cn/item/60ebe1f55132923bf8acdd67.jpg" alt="" /></p>

<p>编码器由$N$层模块堆叠而成(设置<code class="language-plaintext highlighter-rouge">n_layers=6</code>)。序列数据首先经过<strong>词嵌入</strong>(<strong>embedding</strong>)变换为词向量(长度为<code class="language-plaintext highlighter-rouge">d_model=512</code>)，与位置编码(<strong>positional encoding</strong>)相加后作为输入。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">src_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>                     <span class="c1"># 词嵌入
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>                               <span class="c1"># 位置编码
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">):</span>                                               <span class="c1"># enc_inputs: [batch_size, src_len]
</span>        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">src_emb</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>                                   <span class="c1"># enc_outputs: [batch_size, src_len, d_model]
</span>        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_emb</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span>                                  <span class="c1"># enc_outputs: [batch_size, src_len, d_model]   
</span>        <span class="n">enc_self_attn_mask</span> <span class="o">=</span> <span class="nf">get_attn_pad_mask</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">)</span>           <span class="c1"># enc_self_attn_mask: [batch_size, src_len, src_len]
</span>        <span class="n">enc_self_attns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attn</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attn_mask</span><span class="p">)</span>  <span class="c1"># enc_outputs :   [batch_size, src_len, d_model], 
</span>                                                                                 <span class="c1"># enc_self_attn : [batch_size, n_heads, src_len, src_len]
</span>            <span class="n">enc_self_attns</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">enc_self_attn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_self_attns</span>
</code></pre></div></div>

<p>由于输入序列中可能存在占位符等没有意义的<strong>token</strong>，因此使用<code class="language-plaintext highlighter-rouge">get_attn_pad_mask</code>函数生成注意力<strong>mask</strong>，在计算注意力时将这些位置置零。实现过程是首先找出这些位置(标记为$1$)，并在后续的注意力计算中将这些位置赋予一个较大的负值(如$-1e9$)，这样经过<strong>softmax</strong>函数后该位置就趋近于$0$。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_attn_pad_mask</span><span class="p">(</span><span class="n">seq_q</span><span class="p">,</span> <span class="n">seq_k</span><span class="p">):</span>                       <span class="c1"># seq_q: [batch_size, seq_len] ,seq_k: [batch_size, seq_len]
</span>    <span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span> <span class="o">=</span> <span class="n">seq_q</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">len_k</span> <span class="o">=</span> <span class="n">seq_k</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
    <span class="n">pad_attn_mask</span> <span class="o">=</span> <span class="n">seq_k</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>          <span class="c1"># 判断占位符P(=0),用1标记 ,[batch_size, 1, len_k]
</span>    <span class="k">return</span> <span class="n">pad_attn_mask</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">len_q</span><span class="p">,</span> <span class="n">len_k</span><span class="p">)</span>  <span class="c1"># 扩展成多维度
</span></code></pre></div></div>

<p>编码器的每层模块包含两个子层，即一个<a href="https://0809zheng.github.io/2020/04/24/self-attention.html#3-multi-head-self-attention">多头自注意力</a>(<strong>Multi-head self-attention</strong>)层和一个逐位置的前馈神经网络(<strong>Feed Forward</strong>)层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">enc_self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">()</span>                                     <span class="c1"># 多头注意力机制
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pos_ffn</span> <span class="o">=</span> <span class="nc">PoswiseFeedForwardNet</span><span class="p">()</span>                                        <span class="c1"># 前馈神经网络
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_self_attn_mask</span><span class="p">):</span>                                <span class="c1"># enc_inputs: [batch_size, src_len, d_model]
</span>                                                                                      <span class="c1"># enc_self_attn_mask: [batch_size, src_len, src_len]
</span>        <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">enc_self_attn</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span>    <span class="c1"># enc_outputs: [batch_size, src_len, d_model], 
</span>                                               <span class="n">enc_self_attn_mask</span><span class="p">)</span>                    <span class="c1"># attn: [batch_size, n_heads, src_len, src_len]                                                                   
</span>        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_ffn</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">)</span>                                       <span class="c1"># enc_outputs: [batch_size, src_len, d_model]
</span>        <span class="k">return</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">attn</span>
</code></pre></div></div>

<p>多头自注意力机制如下。基本的注意力计算采用缩放点积注意力，序列每个位置的<strong>query, key, value</strong>向量是由其自身(单头)或自身的线性变换(多头)表示的，因此称为“自”(<strong>self</strong>)注意力。其中<strong>query, key</strong>向量的长度为<code class="language-plaintext highlighter-rouge">d_k=64</code>，<strong>value</strong>向量的长度为<code class="language-plaintext highlighter-rouge">d_v=64</code>。</p>

\[\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]

<p>引入缩放因子$1/\sqrt{d_k}$的原因是<strong>softmax</strong>函数将输入的每一行规范化为概率分布，由于<strong>softmax</strong>函数对较大的数值比较敏感，数值较大的位置更有可能趋近于$1$，使得其他位置趋近于$0$，为了减少这种过度的“二值化”，对注意力计算的数值进行缩放。</p>

<p>由于单头的自注意力运算没有可学习参数，因此其表示能力受限。多头自注意力机制是指将输入序列映射到$h$个不同的子空间(设置<code class="language-plaintext highlighter-rouge">n_head=8</code>，应满足<code class="language-plaintext highlighter-rouge">n_head*d_k=n_model</code>)，在每个子空间中应用自注意力运算，将结果连接起来再映射回原空间中。这种做法类似于卷积网络中使用多个卷积核，使得模型具有$h$次机会倾向于学习合适的注意力关系，从而增强模型的表达能力。</p>

<p>多头自注意力机制后还应用了残差连接和<a href="https://0809zheng.github.io/2020/03/04/normalization.html#9-layer-normalization">Layer Norm</a>。使用<strong>LayerNorm</strong>而不是<strong>BatchNorm</strong>的原因是，序列数据通常具有不同的长度，通过补$0$进行长度对齐。若在所有样本的某一个特征维度上进行标准化(<strong>BatchNorm</strong>)，其计算得到的均值和方差变化较大，不利于存储滑动平均值。而对每个样本的所有特征维度进行标准化(<strong>LayerNorm</strong>)则比较稳定。</p>

<p><img src="https://pic.imgdb.cn/item/618b82b52ab3f51d91ec31ba.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ScaledDotProductAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>                             <span class="c1"># Q: [batch_size, n_heads, len_q, d_k]
</span>                                                                       <span class="c1"># K: [batch_size, n_heads, len_k, d_k]
</span>                                                                       <span class="c1"># V: [batch_size, n_heads, len_v(=len_k), d_v]
</span>                                                                       <span class="c1"># attn_mask: [batch_size, n_heads, seq_len, seq_len]
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>   <span class="c1"># scores : [batch_size, n_heads, len_q, len_k]
</span>        <span class="n">scores</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>                           <span class="c1"># 如果是占位符P就等于 0 
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>                                <span class="c1"># [batch_size, n_heads, len_q, d_v]
</span>        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">attn</span>

<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_v</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_Q</span><span class="p">,</span> <span class="n">input_K</span><span class="p">,</span> <span class="n">input_V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>    <span class="c1"># input_Q: [batch_size, len_q, d_model]
</span>                                                                <span class="c1"># input_K: [batch_size, len_k, d_model]
</span>                                                                <span class="c1"># input_V: [batch_size, len_v(=len_k), d_model]
</span>                                                                <span class="c1"># attn_mask: [batch_size, seq_len, seq_len]
</span>        <span class="n">residual</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_Q</span><span class="p">,</span> <span class="n">input_Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_Q</span><span class="p">(</span><span class="n">input_Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Q: [batch_size, n_heads, len_q, d_k]
</span>        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_K</span><span class="p">(</span><span class="n">input_K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_k</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># K: [batch_size, n_heads, len_k, d_k]
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_V</span><span class="p">(</span><span class="n">input_V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_v</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># V: [batch_size, n_heads, len_v(=len_k), d_v]
</span>        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>              <span class="c1"># attn_mask : [batch_size, n_heads, seq_len, seq_len]
</span>        <span class="n">context</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="nc">ScaledDotProductAttention</span><span class="p">()(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>          <span class="c1"># context: [batch_size, n_heads, len_q, d_v]
</span>                                                                                 <span class="c1"># attn: [batch_size, n_heads, len_q, len_k]
</span>        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_v</span><span class="p">)</span> <span class="c1"># context: [batch_size, len_q, n_heads * d_v]
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>                                                <span class="c1"># [batch_size, len_q, d_model]
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layernorm</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="n">residual</span><span class="p">),</span> <span class="n">attn</span>
</code></pre></div></div>

<p>前馈神经网络层采用两层全连接层，全连接层作用于序列的每个位置，其中间特征维度为<code class="language-plaintext highlighter-rouge">d_ff=2048</code>。该层最后也使用了残差连接和<strong>Layer Norm</strong>：</p>

\[\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PoswiseFeedForwardNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PoswiseFeedForwardNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>                             <span class="c1"># inputs: [batch_size, seq_len, d_model]
</span>        <span class="n">residual</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()(</span><span class="n">output</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>   <span class="c1"># [batch_size, seq_len, d_model]  
</span></code></pre></div></div>

<h3 id="-解码器">② 解码器</h3>

<p><img src="https://pic.imgdb.cn/item/60ebe1f55132923bf8acdd67.jpg" alt="" /></p>

<p>解码器也由$N$层模块堆叠而成(设置<code class="language-plaintext highlighter-rouge">n_layers=6</code>)。解码器采用自回归式的输入方式，即每次输入应为目标句子的一部分(右移<strong>shifted right</strong>的目标序列，初始为<code class="language-plaintext highlighter-rouge">[START]</code>)，经过词嵌入后与位置编码相加。在实践中可以对解码器的输入序列进行<strong>mask</strong>，即对每一个输入<strong>token</strong>，在计算注意力时<strong>mask</strong>掉其后所有<strong>token</strong>，使得每一个输入<strong>token</strong>只能和其之前的输入<strong>token</strong>交互，通过这种<strong>mask</strong>机制可以在一次前向传播过程中实现所有自回归过程。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tgt_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">):</span>                               <span class="c1"># dec_inputs: [batch_size, tgt_len]
</span>                                                                                          <span class="c1"># enc_intpus: [batch_size, src_len]
</span>                                                                                          <span class="c1"># enc_outputs: [batsh_size, src_len, d_model]
</span>        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tgt_emb</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>                                            <span class="c1"># [batch_size, tgt_len, d_model]       
</span>        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_emb</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">)</span>                                           <span class="c1"># [batch_size, tgt_len, d_model]
</span>        <span class="n">dec_self_attn_pad_mask</span> <span class="o">=</span> <span class="nf">get_attn_pad_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>                <span class="c1"># [batch_size, tgt_len, tgt_len]
</span>        <span class="n">dec_self_attn_subsequence_mask</span> <span class="o">=</span> <span class="nf">get_attn_subsequence_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">)</span>            <span class="c1"># [batch_size, tgt_len, tgt_len]
</span>        <span class="n">dec_self_attn_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gt</span><span class="p">((</span><span class="n">dec_self_attn_pad_mask</span> <span class="o">+</span> 
                                       <span class="n">dec_self_attn_subsequence_mask</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>                <span class="c1"># [batch_size, tgt_len, tgt_len]
</span>        <span class="n">dec_enc_attn_mask</span> <span class="o">=</span> <span class="nf">get_attn_pad_mask</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">)</span>                     <span class="c1"># [batc_size, tgt_len, src_len]
</span>        <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>                             <span class="c1"># dec_outputs: [batch_size, tgt_len, d_model]
</span>                                                              <span class="c1"># dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]
</span>                                                              <span class="c1"># dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]
</span>            <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attn</span><span class="p">,</span> <span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">dec_self_attn_mask</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="p">)</span>
            <span class="n">dec_self_attns</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">dec_self_attn</span><span class="p">)</span>
            <span class="n">dec_enc_attns</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">dec_enc_attn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attns</span><span class="p">,</span> <span class="n">dec_enc_attns</span>
</code></pre></div></div>

<p>除了使用<code class="language-plaintext highlighter-rouge">get_attn_pad_mask</code>函数<strong>mask</strong>掉解码器输入和编码器输入中没有意义的占位符，还使用<code class="language-plaintext highlighter-rouge">get_attn_subsequence_mask</code>函数生成自回归的<strong>mask</strong>，表现为一个上三角矩阵(值为$1$即会被<strong>mask</strong>掉)。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_attn_subsequence_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>                               <span class="c1"># seq: [batch_size, tgt_len]
</span>    <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">seq</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">seq</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">seq</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>          <span class="c1"># 注意力矩阵：QK^T
</span>    <span class="n">subsequence_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>          <span class="c1"># 生成上三角矩阵,[batch_size, tgt_len, tgt_len]
</span>    <span class="n">subsequence_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">subsequence_mask</span><span class="p">).</span><span class="nf">byte</span><span class="p">()</span>  <span class="c1">#  [batch_size, tgt_len, tgt_len]
</span>    <span class="k">return</span> <span class="n">subsequence_mask</span>  
</code></pre></div></div>

<p>解码器的每层模块包含三个子层，即一个带掩码的多头自注意力层、一个多头自注意力层和一个逐位置的前馈神经网络层。其中带掩码的多头自注意力层将自回归<strong>mask</strong>应用到注意力计算中；多头自注意力层中的<strong>query</strong>来自前一个输出，<strong>key, value</strong>来自编码器的输出。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dec_self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pos_ffn</span> <span class="o">=</span> <span class="nc">PoswiseFeedForwardNet</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">dec_self_attn_mask</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="p">):</span> <span class="c1"># dec_inputs: [batch_size, tgt_len, d_model]
</span>                                                                                       <span class="c1"># enc_outputs: [batch_size, src_len, d_model]
</span>                                                                                       <span class="c1"># dec_self_attn_mask: [batch_size, tgt_len, tgt_len]
</span>                                                                                       <span class="c1"># dec_enc_attn_mask: [batch_size, tgt_len, src_len]
</span>        <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dec_self_attn</span><span class="p">(</span><span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">,</span> 
                                                 <span class="n">dec_inputs</span><span class="p">,</span> <span class="n">dec_self_attn_mask</span><span class="p">)</span>   <span class="c1"># dec_outputs: [batch_size, tgt_len, d_model]
</span>                                                                                   <span class="c1"># dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]
</span>        <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_enc_attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dec_enc_attn</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> 
                                                <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">dec_enc_attn_mask</span><span class="p">)</span>    <span class="c1"># dec_outputs: [batch_size, tgt_len, d_model]
</span>                                                                                   <span class="c1"># dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]
</span>        <span class="n">dec_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_ffn</span><span class="p">(</span><span class="n">dec_outputs</span><span class="p">)</span>                                    <span class="c1"># dec_outputs: [batch_size, tgt_len, d_model]
</span>        <span class="k">return</span> <span class="n">dec_outputs</span><span class="p">,</span> <span class="n">dec_self_attn</span><span class="p">,</span> <span class="n">dec_enc_attn</span>
</code></pre></div></div>

<h3 id="-位置编码">③ 位置编码</h3>
<p>自注意力机制无法捕捉位置信息，这是因为其计算注意力时的无序性，导致打乱任意顺序的序列其每个对应位置会得到相同的结果。通过引入位置编码把位置信息直接编码到输入序列中。</p>

<p>每个位置的位置编码也应具有长度<code class="language-plaintext highlighter-rouge">d_model=512</code>。作者使用一种三角形式的位置编码，使得每一位置的编码表示为之前位置编码的线性函数(三角函数的和差公式)。第$pos$位置的第$i$和$i+1$个编码表示为：</p>

\[PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})\]

\[PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})\]

<p>在实践中由于词嵌入的数值相对于位置编码较小，因此将词嵌入的结果乘以$\sqrt{d_{model}}$后与位置编码相加。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span> 
        <span class="n">pos_table</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
        <span class="p">[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">d_model</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">pos</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">)])</span>
        <span class="n">pos_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">pos_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>                  <span class="c1"># 字嵌入维度为偶数时
</span>        <span class="n">pos_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">pos_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>                  <span class="c1"># 字嵌入维度为奇数时
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pos_table</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">pos_table</span><span class="p">)</span>                      <span class="c1"># enc_inputs: [seq_len, d_model]
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">enc_inputs</span><span class="p">):</span>                                         <span class="c1"># enc_inputs: [batch_size, seq_len, d_model]
</span>        <span class="n">enc_inputs</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">pos_table</span><span class="p">[:</span><span class="n">enc_inputs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">enc_inputs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="-模型比较">④ 模型比较</h3>

<p><img src="https://pic.imgdb.cn/item/618b97032ab3f51d91f83cfd.jpg" alt="" /></p>

<p>上表展示了自注意力机制、循环网络、卷据网络以及一种受限的自注意力机制的计算性能对比。其中$n$是序列长度，$d$是序列每个<strong>token</strong>的特征维度(词嵌入维度)，$k$是(1d)卷积核尺寸，$r$表示对每个位置只计算其附近$r$个位置的注意力。</p>
<ul>
  <li><strong>Complexity per Layer</strong>：即每层的计算复杂度。循环网络和卷积网络的复杂度接近，与自注意力的复杂度比较主要取决于$n$和$d$的大小。</li>
  <li><strong>Sequential Operations</strong>：所需等待的序列操作数，只有循环网络需要顺序执行(当前位置依赖于之前位置的计算)。</li>
  <li><strong>Maximum Path Length</strong>：连接任意两位置所需路径长度的最大值。自注意力可以建立任意两个位置之间的关系；循环网络需要顺序遍历完整的序列才能建立全局关系；卷积网络受卷积核(感受野)限制，需要堆叠多层才能获得全局感受野。</li>
</ul>

<h2 id="2-实验分析">2. 实验分析</h2>

<h3 id="-网络设置">① 网络设置</h3>
<p>作者设计了几种不同大小的模型，如下表所示：</p>

<p><img src="https://pic.imgdb.cn/item/618b994d2ab3f51d91f97b8a.jpg" alt="" /></p>

<h2 id="-训练设置">② 训练设置</h2>

<p>训练集使用<strong>WMT 2014</strong>英语-德语数据集和英语-法语数据集。前者包含$450$万对句子，使用<strong>byte-pair</strong>编码句子，即按照划分词根进行编码，减少同一个单词不同时态造成的冗余。源域和目标域语言共享包含$37000$个<strong>token</strong>的词典。后者则更大，包含$3600$万对句子。</p>

<p>训练使用了$8$块<strong>P100 GPU</strong>。<strong>base</strong>模型每次训练耗时$0.4$秒，共进行了$10$万次训练，总耗时$12$小时。<strong>big</strong>模型每次训练耗时$1$秒，共进行了$30$万次训练，总耗时$3.5$天。</p>

<p>使用<strong>Adam</strong>优化器，基本参数$\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}$。设置$warmup_steps=4000$，学习率公式如下：</p>

\[lrate=d_{model}^{-0.5}\cdot \min (step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})\]

<p>在每个子层的残差连接、编码器和解码器的词嵌入和位置编码相加处使用了<strong>dropout</strong>，设置$P_{drop}=0.1$。</p>

<p>设置$\epsilon_{ls}=0.1$的<strong>label smoothing</strong>降低学习难度，即当概率超过$0.1$时认为是对的结果(总类别数较多)。尽管这降低了模型预测的困惑度，但提高了准确率。</p>

<h3 id="3-实验结果">3. 实验结果</h3>
<p>作者给出了在机器翻译任务上的模型表现：</p>

<p><img src="https://pic.imgdb.cn/item/618b9d192ab3f51d91fb53d3.jpg" alt="" /></p>

<p>使用<strong>multi-head</strong>机制，既可以捕捉到近距离依赖关系，又可以捕捉到远距离依赖关系；且模型具有较好的可解释性。由于计算得到每一个<strong>token</strong>与其他所有<strong>token</strong>的自注意力，因此可以定量衡量不同<strong>token</strong>之间的相关性程度。下图展示了两个句子，其每个句子的每个<strong>token</strong>（此处为单词）与句子中其他单词之间的相关性：</p>

<p><img src="https://pic.downk.cc/item/5ea2a8e3c2a9a83be570cfb4.jpg" alt="" /></p>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/04/27/elmo-bert-gpt.html" class="read-next-link"></a>
        <section>
          <span>预训练语言模型(Pretrained Language Model)</span>
          <p>  Pretrained Language Models.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.downk.cc/item/5ea4013dc2a9a83be5b17721.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/04/24/self-attention.html" class="read-next-link"></a>
          <section>
            <span>自注意力机制(Self-Attention Mechanism)</span>
            <p>  Self-Attention Mechanism.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5ea28825c2a9a83be5477d93.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
