<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>线性回归(Linear Regression) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="线性回归(Linear Regression)">
    <meta name="keywords"  content="机器学习">
    <!-- Open Graph -->
    <meta property="og:title" content="线性回归(Linear Regression) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/03/12/regression.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="post-tag">机器学习</a>
          
        
      </div>
      <h1>线性回归(Linear Regression)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-03-12"><i class="iconfont icon-date"></i>12 Mar 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5ed758f4c2a9a83be54c0e00.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Linear Regression.</p>
</blockquote>

<p>本文目录：</p>
<ol>
  <li>线性回归 <strong>Linear Regression</strong></li>
  <li>广义线性模型 <strong>Generalized Linear Model</strong></li>
  <li>非线性回归 <strong>Non-Linear Regression</strong></li>
  <li>过拟合 <strong>Overfitting</strong></li>
</ol>

<h1 id="1-线性回归">1. 线性回归</h1>
<p><strong>线性回归(Linear Regression)</strong>是最基本的回归方法，其假设空间的假设函数是线性函数。</p>

<p>对于每一个样本数据$x = (x_1,x_2,…,x_d)^T \in \Bbb{R}^{d}$，其中每一个元素$x_i$称为样本的<strong>特征(feature)</strong>或<strong>属性(attribute)</strong>。为样本的每一个特征引入一个重要性<strong>权重(weight)</strong>参数$w_i$，并额外引入一个<strong>偏置(bias)</strong>参数$b$，则线性回归具有很好的<strong>可解释性(comprehensibility)</strong>：输出$\hat{y}$为样本输入特征的加权线性组合。计算如下：</p>

\[\hat{y} = \sum_{j=1}^{d} {w_jx_j} + b\]

<p>若把偏置参数$b$看作$w_0 \cdot 1$，
即为每一个样本点额外增加一个维度$x = (1,x_1,x_2,…,x_d)^T \in \Bbb{R}^{d+1}$，模型权重参数也额外增加一个维度$w = (w_0,w_1,w_2,…,w_d)^T \in \Bbb{R}^{d+1}$，则线性回归模型可以被简化为：</p>

\[\hat{y} = \sum_{j=0}^{d} {w_jx_j} = w^Tx\]

<p>若所有样本组成样本集\(X=\{x^{(1)},...,x^{(N)}\}\)，对应的标签集\(y=\{y^{(1)},...,y^{(N)}\}\)。该问题也被称作<strong>多元(multivariate,多变量)</strong>线性回归，使用<strong>均方误差(mean squared error，MSE)</strong>衡量模型的好坏：</p>

\[L(w) = \frac{1}{N} \sum_{i=1}^{N} {(\sum_{j=0}^{d} {w_jx_j^{(i)}}-y^{(i)})^2} = \frac{1}{N} \sum_{i=1}^{N} {(w^Tx^{(i)}-y^{(i)})^2}\]

<p>求解上述均方误差最小化问题有两种可行的思路，即<strong>最小二乘法</strong>和<strong>正规方程法</strong>。</p>

<h3 id="讨论11均方误差损失可否用于分类问题">讨论1.1：均方误差损失可否用于分类问题？</h3>
<p>由下图可以看出，对于分类任务，$0/1$损失始终要比均方误差损失小，故误差上界小；虽然均方误差损失使得上界变得宽松了，但是由于优化问题变得简单(从<strong>NP-hard</strong>变为凸优化)，也能得到不错的结果。</p>

<p><img src="https://pic.downk.cc/item/5ed0baaec2a9a83be5867638.jpg" alt="" /></p>

<h2 id="1-最小二乘法-least-square-method">(1) 最小二乘法 Least Square Method</h2>
<p>由于均方误差是凸函数(见本节讨论1.2)，因此$w$的最优解在均方误差关于$w$的导数为$0$时取得。为简化问题，假设样本只有一个特征(即单变量线性回归)，对应的均方误差损失如下：</p>

\[L(w,b) = \frac{1}{N} \sum_{i=1}^{N} {(wx^{(i)}+b-y^{(i)})^2}\]

<p>令上式关于$w$和$b$的导数为$0$，计算如下：</p>

\[\begin{aligned} \frac{\partial L(w,b)}{\partial w} &amp;= \frac{\partial}{\partial w}[\frac{1}{N} \sum_{i=1}^{N} {(wx^{(i)}+b-y^{(i)})^2}] = \frac{1}{N} \sum_{i=1}^{N} {2(wx^{(i)}+b-y^{(i)})x^{(i)}} = 0 \\ \frac{\partial L(w,b)}{\partial b} &amp;= \frac{\partial}{\partial b}[\frac{1}{N} \sum_{i=1}^{N} {(wx^{(i)}+b-y^{(i)})^2}] = \frac{1}{N} \sum_{i=1}^{N} {2(wx^{(i)}+b-y^{(i)})} = 0 \end{aligned}\]

<p>联立上述两式可得该问题的<strong>闭式(closed-form)解</strong>：</p>

\[\begin{aligned} w &amp;= \frac{\sum_{i=1}^{N} y^{(i)}(x^{(i)}-\frac{1}{N} \sum_{i=1}^{N} x^{(i)})}{\sum_{i=1}^{N}(x^{(i)})^2-\frac{1}{N}(\sum_{i=1}^{N}(x^{(i)}))^2} = \frac{\sum_{i=1}^{N} y^{(i)}(x^{(i)}-\overline{x})}{\sum_{i=1}^{N}(x^{(i)})^2-\frac{1}{N}(\sum_{i=1}^{N}(x^{(i)}))^2}  \\ b &amp;= \frac{1}{N} \sum_{i=1}^{N} (y^{(i)}-wx^{(i)}) \end{aligned}\]

<p>上述利用均方误差最小化求解线性回归的方法被称为<strong>最小二乘法(least square method)</strong>。该方法的几何意义是在样本空间中寻找一个超平面，使得所有样本点到该超平面的距离平方最小：</p>

<p><img src="https://pic.downk.cc/item/5ed0bbb7c2a9a83be5882cf8.jpg" alt="" /></p>

<h3 id="讨论12均方误差损失是凸函数">讨论1.2：均方误差损失是凸函数</h3>
<p>定义在区间$[a,b]$上的<strong>凸(convex)函数</strong> $f$，是指对于区间中任意两点$x_1&lt;x_2$均有：</p>

\[f(\frac{x_1+x_2}{2}) ≤ \frac{f(x_1)+f(x_2)}{2}\]

<p>$f(x)=x^2$就是一个典型的凸函数，该凸函数的全局最小值位于$\nabla f=0$处。对于实数集上的函数$f$，可由二阶导数$\nabla^2 f$判断其凸性。若二阶导数$\nabla^2 f$在区间上<strong>非负</strong>则为凸函数。验证均方误差的凸性：</p>

\[\begin{aligned} \nabla^2 L(w) &amp;= \nabla^2 \frac{1}{N} \sum_{i=1}^{N} {(w^Tx^{(i)}-y^{(i)})^2} \\ &amp;= \nabla \frac{1}{N} \sum_{i=1}^{N} 2(w^Tx^{(i)}-y^{(i)})x^{(i)} = \frac{1}{N} \sum_{i=1}^{N} 2(x^{(i)})^2 ≥0 \end{aligned}\]

<h3 id="讨论13最小二乘法等价于噪声服从高斯分布的极大似然估计">讨论1.3：最小二乘法等价于噪声服从高斯分布的极大似然估计</h3>
<p>引入高斯噪声$ε$~$N(0,σ^2)$，对线性回归建模：</p>

\[y = w^Tx + ε\]

<p>其中$(x,y)$是样本数据，$w$是未知的常数，每个样本点受到了高斯噪声的干扰。则条件变量$y | x;w$服从于$N(w^Tx,σ^2)$。列出条件概率：</p>

\[P(y | x;w) = \frac{1}{\sqrt{2\pi}σ}\exp(-\frac{(y-w^Tx)^2}{2σ^2})\]

<p>采用极大似然估计的方法估计参数$w$，即：</p>

\[\begin{aligned} \hat{w} &amp;= \mathop{\arg \max}_{w} \log(\prod_{i=1}^{N} {\frac{1}{\sqrt{2\pi}σ}\exp(-\frac{(y^{(i)}-w^Tx^{(i)})^2}{2σ^2})}) \\ &amp;= \mathop{\arg \max}_{w} \sum_{i=1}^{N} {\log(\frac{1}{\sqrt{2\pi}σ}\exp(-\frac{(y^{(i)}-w^Tx^{(i)})^2}{2σ^2}))} \\ &amp;= \mathop{\arg \max}_{w} \sum_{i=1}^{N} {(-\frac{(y^{(i)}-w^Tx^{(i)})^2}{2σ^2})} \\ &amp;= \mathop{\arg \max}_{w} \sum_{i=1}^{N} {-(y^{(i)}-w^Tx^{(i)})^2} \\ &amp;= \mathop{\arg \min}_{w} \sum_{i=1}^{N} {(y^{(i)}-w^Tx^{(i)})^2} \end{aligned}\]

<p>问题等价于最小二乘法。</p>

<h2 id="2-正规方程法">(2) 正规方程法</h2>
<p>将线性回归表示为矩阵形式：记样本矩阵\(X=[x^{(1)};...;x^{(N)}]^T \in \Bbb{R}^{N×(d+1)}\)，标签向量\(y=[y^{(1)};...;y^{(N)}]^T \in \Bbb{R}^{N}\)，</p>

<p>待求解权重参数\(w \in \Bbb{R}^{d+1}\)，预测结果\(\hat{y} \in \Bbb{R}^{N}\)，则：</p>

\[\hat{y} = Xw\]

<p>损失函数：</p>

\[L(w) = \frac{1}{N} || Xw-y ||^2 = \frac{1}{N} (Xw-y)^T(Xw-y)\]

<p>该目标函数是凸函数，可以直接求梯度令其为零，得到全局最小解：</p>

\[\nabla_wL(w) = \nabla_w \frac{1}{N} (Xw-y)^T(Xw-y) = 2X^TXw - 2Xy = 0\]

<p>整理得到：</p>

\[X^TXw = Xy\]

<p>上式叫做<strong>正规方程（normal equation）</strong>。</p>

<p>通常样本数远大于特征维度$N$»$d$，$X^TX$是可逆的，可以得到权重参数的<strong>解析解（closed-form）</strong>：</p>

\[w = (X^TX)^{-1}Xy = X^+y\]

<p>其中\(X^+=(X^TX)^{-1}\)称为$X$的<strong>伪逆（pseudo inverse）</strong>，当$X^TX$不可逆时(如样本的特征维度大于样本总数)，也有其他方法可求。</p>

<p>使用正规方程法求解线性回归问题的程序如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">LinearRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)),</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="讨论14从线性代数的角度理解线性回归">讨论1.4：从线性代数的角度理解线性回归</h3>
<p>对于线性回归问题：</p>

\[\hat{y} = Xw\]

<p>上述矩阵方程有解的条件是向量$\hat{y}$在矩阵$X$的列空间中，即向量$\hat{y}$是矩阵$X$的列向量的线性组合。</p>

<h3 id="讨论15-正规方程法的误差分析">讨论1.5： 正规方程法的误差分析</h3>
<p>预测标签：</p>

\[\hat{y} = Xw = X(X^TX)^{-1}Xy = XX^+y = Hy\]

<p>其中$H$为<strong>投影矩阵</strong>，几何意义是将向量$y$投影到样本矩阵$X$的列空间：</p>

<p><img src="https://pic.downk.cc/item/5ed0be5fc2a9a83be58c1e5f.jpg" alt="" /></p>

<p>投影误差：</p>

\[L(w) = \frac{1}{N} || Xw-y ||^2 = \frac{1}{N} || XX^+y-y ||^2 = \frac{1}{N} || (H-I)y ||^2\]

<p>由此不难得出，$H-I$为也是投影矩阵，将向量$y$投影到与样本矩阵$X$的列空间正交的子空间（左零空间）中。</p>

<p>线性回归的样本内误差$E_{in}$和总样本误差$E_{out}$表示如下：</p>

\[E_{in} = (noise)(1-\frac{d+1}{N})\]

\[E_{out} = (noise)(1+\frac{d+1}{N})\]

<p>绘制<strong>学习曲线（learning curve）</strong>，误差最终趋近于一个噪声值$σ^2$:</p>

<p><img src="https://pic.downk.cc/item/5ed0c0d3c2a9a83be58f9ff8.jpg" alt="" /></p>

<p><strong>学习曲线</strong>是指对于一个给定的模型，训练稳定时的$E_{in}$、$E_{out}$随训练样本总数$N$的变化。</p>

<h1 id="2-广义线性模型">2. 广义线性模型</h1>
<p>一般地，线性回归模型可以简写为：</p>

\[y=w^Tx+b\]

<p>引入单调可微函数$g(\cdot)$，可以建立<strong>广义线性模型(generalized linear model)</strong>，其中$g(\cdot)$称为<strong>联系函数(link function)</strong>:</p>

\[y=g^{-1}(w^Tx+b)\]

<p>线性回归模型建立的是输入空间到输出空间的线性函数映射；广义线性模型可以通过联系函数$g(\cdot)$建立从输入空间到输出空间的非线性函数映射。比如<strong>对数线性回归(log-linear regression)</strong>：</p>

\[\ln(y) = w^Tx+b\]

<h1 id="3-非线性回归">3. 非线性回归</h1>
<p><strong>非线性回归(non-linear regression)</strong>是指先对样本的特征进行非线性变换，再通过线性回归方法进行建模。其中对样本进行非线性变换，通常是将样本从低维空间变换到高维空间中:</p>

\[z = Φ(x)\]

<p>如$Q$阶的多项式变换：</p>

\[Φ_Q(x) = (1,x_1,x_2,...,x_d,x_1^2,x_1x_2,...,x_d^2,...,x_1^Q,x_1^{Q-1}x_2,...,x_d^Q)\]

<p>样本的原始特征维度为$d+1$，变换后的特征维度为$C_{Q+d}^{Q}=O(Q^d)$。</p>

<p>上述多项式变换存在问题，即若输入的范围限定在$±1$之间，则高阶幂的值会比低阶幂的值小得多，需要给高阶幂更大的权重。为了避免这种数据差异很大的情况，可以使用<strong>勒让德多项式（Legendre Polynomials）</strong>：</p>

<p><img src="https://pic.downk.cc/item/5ed37d6dc2a9a83be53bb90c.jpg" alt="" /></p>

<h1 id="4-过拟合">4. 过拟合</h1>
<p>下面是使用$2$阶非线性回归和$10$阶非线性回归对应的<strong>学习曲线(learning curve)</strong>：</p>

<p><img src="https://pic.downk.cc/item/5ed2478fc2a9a83be5a5cff6.jpg" alt="" /></p>

<p>由图可以观察到，当样本数足够的时候，两个模型都可以收敛到期望的误差，高阶模型的误差更小；但当样本数不足的时候，高阶模型更容易出现\(E_{in}&lt;E_{out}\)的情况，即出现过拟合。</p>

<p>过拟合的原因：</p>
<ol>
  <li>样本数$N$不足；</li>
  <li><strong>stochastic noise</strong> $σ^2$：样本中的随机误差；</li>
  <li><strong>deterministic noise</strong>：目标函数过于复杂带来的系统误差；</li>
  <li>模型复杂度过高。</li>
</ol>

<p>过拟合的解决方法：</p>
<ol>
  <li><strong>data cleaning</strong>：对数据集中标注错误的样本进行修正；</li>
  <li><strong>data pruning</strong>：删除数据集中标注错误的样本；</li>
  <li><strong>data hinting</strong>：增加更多的样本（数据增广）；</li>
  <li><strong>regularization</strong>：正则化，限制模型的复杂度。</li>
</ol>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/03/13/logistic-regression.html" class="read-next-link"></a>
        <section>
          <span>逻辑回归(Logistic Regression)</span>
          <p>  Logistic Regression.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.downk.cc/item/5ed75927c2a9a83be54c53ef.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/03/11/perceptron.html" class="read-next-link"></a>
          <section>
            <span>感知机(Perceptron)</span>
            <p>  Perceptron.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5eca7715c2a9a83be521b412.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
