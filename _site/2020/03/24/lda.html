<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>线性判别分析(Linear Discriminant Analysis, LDA) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="线性判别分析(Linear Discriminant Analysis, LDA)">
    <meta name="keywords"  content="机器学习">
    <!-- Open Graph -->
    <meta property="og:title" content="线性判别分析(Linear Discriminant Analysis, LDA) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/03/24/lda.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="post-tag">机器学习</a>
          
        
      </div>
      <h1>线性判别分析(Linear Discriminant Analysis, LDA)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-03-24"><i class="iconfont icon-date"></i>24 Mar 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5e95b7cac2a9a83be5deb0bd.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Linear Discriminant Analysis.</p>
</blockquote>

<p><strong>线性判别分析(Linear Discriminant Analysis,LDA)</strong>，又称<strong>Fisher</strong>判别分析，是一种线性分类方法。<strong>LDA</strong>的思想是，给定训练样本集，将所有样本投影到空间中的一个超平面上，使得相同类别的样本的投影点尽可能接近，不同类别的样本的投影点尽可能远离。在对一个新的样本进行分类时，先将其投影到该超平面上，再根据投影点的位置(以及预先设置的阈值)确定该样本的类别。<strong>LDA</strong>也可以看作是一种监督的线性降维方法。</p>

<p><img src="https://pic.imgdb.cn/item/611ccead4907e2d39c6d2031.jpg" alt="" /></p>

<h1 id="1-二分类线性判别分析">1. 二分类线性判别分析</h1>

<p>若记$N$个样本点\(\{x_1,x_2,...,x_N\}\)，属于类别$0$和类别$1$的样本总数分别为$N_0$和$N_1$。在样本空间中寻找一个向量$w$，将样本$x$投影到向量$w$上，得到投影$w^Tx$(注意到投影是<strong>标量</strong>)。</p>

<p>对投影的解释如下。若向量$x$与向量$w$之间的夹角为$\theta$，则将向量$x$直接投影到向量$w$上的长度为$|x|\cos \theta$。直接计算两向量内积$w^Tx= |w| |x| \cos \theta$；当$|w|=1$时两者等价，因此用内积表示投影。</p>

<p>注意到向量$w$可以被看作是分离超平面的法向量，通过选择使得样本投影的<strong>类间差距大,类内方差小</strong>的向量$w$，可以找到将$w$作为法向量的分离超平面$f(x)=w^Tx$，从而对样本点进行分类。</p>

<h3 id="1-类间差距大松耦合">(1) 类间差距大(松耦合)</h3>

<p>不同类别样本点之间的<strong>类间差距</strong>可以用均值(代表样本中心)投影之间的差距衡量。若记类别$c$的样本均值为$\mu_c = \frac{1}{N_c} \sum_{x \in X_c}^{} x$，则均值投影为$\frac{1}{N_c} \sum_{x \in X_c}^{} w^Tx = w^T\mu_c$。对于二分类问题，类别$0$和类别$1$的均值投影差异表示为\(\|w^T\mu_0-w^T\mu_1\|_2^2\)。</p>

<h3 id="2-类内方差小高内聚">(2) 类内方差小(高内聚)</h3>

<p>同一类别样本点之间的<strong>类内方差</strong>可以用投影后的协方差矩阵衡量。若记类别$c$的样本协方差为$\Sigma_c = \frac{1}{N_c} \sum_{x \in X_c}^{} (x-\mu_c)(x-\mu_c)^T$，则类别$c$投影后的协方差矩阵表示为：</p>

\[\frac{1}{N_c} \sum_{x \in X_c}^{} (w^Tx-w^T\mu_c)(w^Tx-w^T\mu_c)^T = \frac{1}{N_c} \sum_{x \in X_c}^{} w^T(x-\mu_c)(x-\mu_c)^T w = w^T\Sigma_cw\]

<p>对于二分类问题，类别$0$和类别$1$的类内方差之和表示为$w^T\Sigma_0w+w^T\Sigma_1w$。</p>

<h3 id="3-lda的目标函数">(3) LDA的目标函数</h3>
<ul>
  <li>相同类别的样本的投影点尽可能接近，可以让同一类别样本投影点的协方差$w^T\Sigma_0w+w^T\Sigma_1w$尽可能小；</li>
  <li>不同类别的样本的投影点尽可能远离，可以使不同类别的中心投影之间的距离\(\|w^T\mu_0-w^T\mu_1\|_2^2\)尽可能大。</li>
</ul>

<p>二分类的线性判别分析最大化以下目标函数：</p>

\[J = \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w} = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}\]

<p>定义：</p>
<ul>
  <li><strong>类间散度矩阵(between-class scatter matrix)</strong></li>
</ul>

\[S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T\]

<ul>
  <li><strong>类内散度矩阵(within-class scatter matrix)</strong></li>
</ul>

\[S_w = \Sigma_0+\Sigma_1\]

<p>则目标函数可写作：</p>

\[J = \frac{w^TS_bw}{w^TS_ww}\]

<p>上述问题等价于求<a href="https://0809zheng.github.io/2021/06/22/rayleigh.html">广义瑞利商</a>的极值。由于对向量$w$<strong>等比例缩放不影响广义瑞利商的值</strong>，不失一般性，令$w^TS_ww=1$，此时对广义瑞利商求极值，就是在约束$w^TS_ww=1$下，求$w^TS_bw$的极值。
采用拉格朗日乘子法，定义拉格朗日函数：</p>

\[L(w,\lambda)= w^TS_bw-\lambda(w^TS_ww-1)\]

<p>上式对$w$求梯度，并令梯度为$0$，可得：</p>

\[S_bw=\lambda S_ww\]

<p>注意到$(\mu_0-\mu_1)^Tw$是标量，因此$S_bw = (\mu_0-\mu_1)(\mu_0-\mu_1)^Tw$与$\mu_0-\mu_1$同方向，不妨令$S_bw = \lambda(\mu_0-\mu_1)$，代入上式得：</p>

\[w = S_w^{-1}(\mu_0-\mu_1)\]

<p>若不同类别之间的数据无关(线性可分)，即$S_w$是各向同性的对角矩阵$S_w∝I$，则有$w∝(\mu_0-\mu_1)$。</p>

<p>求得$w$后，即求得分类超平面的法向量，从而可得最终分类超平面：</p>

\[f(x)=w^Tx=S_w^{-1}(\mu_0-\mu_1)x\]

<h1 id="2-多分类线性判别分析">2. 多分类线性判别分析</h1>
<p>可以将<strong>LDA</strong>从二分类推广到多分类的情形。若记$N$个样本点\(\{x_1,x_2,...,x_N\}\)，共有$C$种类别，所有样本点的均值为$\mu = \frac{1}{N} \sum_{i=1}^{N} x_i$，定义<strong>全局散度矩阵</strong> $S_t$：</p>

\[S_t = \sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T\]

<p><strong>类内散度矩阵</strong> $S_w$定义为每个类别的散度矩阵之和：</p>

\[S_w = \sum_{c=1}^{C} S_{w_c} = \sum_{c=1}^{C} \sum_{x \in X_c}^{} (x-\mu_c)(x-\mu_c)^T\]

<p>则<strong>类间散度矩阵</strong> $S_b$可以通过$S_t$和$S_w$计算如下：</p>

\[\begin{aligned} S_b &amp;= S_t - S_w = \sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T - \sum_{c=1}^{C} \sum_{x \in X_c}^{} (x-\mu_c)(x-\mu_c)^T \\ &amp;= \sum_{c=1}^{C} \sum_{x \in X_c}^{}(x-\mu)(x-\mu)^T - \sum_{c=1}^{C} \sum_{x \in X_c}^{} (x-\mu_c)(x-\mu_c)^T \\ &amp;= \sum_{c=1}^{C} \sum_{x \in X_c}^{}  (xx^T-x\mu^T-\mu x^T + \mu\mu^T - xx^T+x\mu_c^T+\mu_c x^T - \mu_c\mu_c^T) \\ &amp;= \sum_{c=1}^{C} \sum_{x \in X_c}^{} (-x\mu^T-\mu x^T + \mu\mu^T +x\mu_c^T+\mu_c x^T - \mu_c\mu_c^T) \\ &amp;= \sum_{c=1}^{C} (-\sum_{x \in X_c}^{}x\mu^T-\sum_{x \in X_c}^{}\mu x^T + \sum_{x \in X_c}^{}\mu\mu^T +\sum_{x \in X_c}^{}x\mu_c^T+\sum_{x \in X_c}^{}\mu_c x^T - \sum_{x \in X_c}^{}\mu_c\mu_c^T) \\ &amp;= \sum_{c=1}^{C} (-N_c\mu_c\mu^T-N_c\mu\mu_c^T + N_c \mu\mu^T +N_c\mu_c\mu_c^T+N_c\mu_c\mu_c^T - N_c\mu_c\mu_c^T) \\ &amp;= \sum_{c=1}^{C} (-N_c\mu_c\mu^T-N_c\mu\mu_c^T + N_c \mu\mu^T +N_c\mu_c\mu_c^T) \\ &amp;= \sum_{c=1}^{C} N_c(\mu_c-\mu)(\mu_c-\mu)^T \end{aligned}\]

<p>上式表示类间散度矩阵可以表示成所有类别样本的中心与总体中心之间的差距之和。多分类<strong>LDA</strong>的优化目标函数表示为：</p>

\[\mathop{\max}_w \frac{tr(W^TS_bW)}{tr(W^TS_wW)}\]

<p>上式也是广义瑞利商的形式，可以将问题转化为广义特征值问题：</p>

\[S_w^{-1}S_bW = \lambda W\]

<h1 id="3-核线性判别分析">3. 核线性判别分析</h1>
<p>将<a href="https://0809zheng.github.io/2021/07/23/kernel.html">核方法</a>引入线性判别分析，即可得到<strong>核线性判别分析(Kernelized Linear Discriminant Analysis)</strong>，从而将线性模型扩展成非线性模型。</p>

<p>引入映射$\phi:\mathcal{X}→\mathcal{F}$将样本空间$\mathcal{X}$变换到高维的特征空间$\mathcal{F}$，在$\mathcal{F}$中构造线性判别分析，以求得：</p>

\[f(x)=w^T \phi(x)\]

<p>第$c$类样本在特征空间$\mathcal{F}$中的均值计算为：</p>

\[\mu_c^{\phi} = \frac{1}{N_c} \sum_{x \in X_c}^{} \phi(x)\]

<p>引入核函数$K(x_i,x_j)=\phi(x_i)^T\phi(x_j)$隐式地表示特征映射$\phi$及其内积，则由表示定理，函数$f(x)$表示为：</p>

\[f(x)=w^T\phi(x)=\sum_{i=1}^{N}\alpha_iK(x,x_i), \quad w = \sum_{i=1}^{N}\alpha_i\phi(x_i)\]

<p>若记训练样本在特征空间$\mathcal{F}$中的<strong>类间散度矩阵</strong> $S_b^{\phi}$和<strong>类内散度矩阵</strong> $S_w^{\phi}$，两个散度矩阵计算为：</p>

\[\begin{aligned} S_b^{\phi} &amp;= (\mu_0^{\phi}-\mu_1^{\phi})(\mu_0^{\phi}-\mu_1^{\phi})^T  \\ S_w^{\phi} &amp;= \Sigma_0^{\phi}+\Sigma_1^{\phi} = \sum_{c=0,1}^{} \sum_{x \in X_c}^{}(\phi(x)-\mu_c^{\phi})(\phi(x)-\mu_c^{\phi})^T \end{aligned}\]

<p>则核线性判别分析的目标函数可表示为：</p>

\[J = \frac{w^TS_b^{\phi}w}{w^TS_w^{\phi}w}\]

<p>使用线性判别分析的方法求解上述问题即可得到$\alpha$，进而求得函数$f(x)$。</p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/03/25/variational-inference.html" class="read-next-link"></a>
        <section>
          <span>变分推断(Variational Inference)</span>
          <p>  Variational Inference.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/63b40d63be43e0d30e2f4cdf.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/03/23/knn.html" class="read-next-link"></a>
          <section>
            <span>k近邻算法(k-Nearest Neighbor, kNN)</span>
            <p>  k-Nearest Neighbor.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/60cdbf50844ef46bb2ec654a.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
