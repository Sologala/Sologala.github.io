<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>支持向量机(Support Vector Machine, SVM) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="支持向量机(Support Vector Machine, SVM)">
    <meta name="keywords"  content="机器学习">
    <!-- Open Graph -->
    <meta property="og:title" content="支持向量机(Support Vector Machine, SVM) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/03/14/SVM.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="post-tag">机器学习</a>
          
        
      </div>
      <h1>支持向量机(Support Vector Machine, SVM)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-03-14"><i class="iconfont icon-date"></i>14 Mar 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/5ed75966c2a9a83be54cb0f5.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Support Vector Machine.</p>
</blockquote>

<p>本文目录：</p>
<ol>
  <li><strong>线性支持向量机</strong></li>
  <li><strong>对偶支持向量机</strong>：降低算法复杂度</li>
  <li><strong>核支持向量机</strong>：解决线性不可分(引入核方法)</li>
  <li><strong>软间隔支持向量机</strong>：解决线性不可分(引入松弛变量)</li>
  <li><strong>序列最小最优化(SMO)</strong>算法</li>
  <li><strong>概率支持向量机</strong></li>
  <li><strong>最小二乘支持向量机</strong></li>
</ol>

<p>（注：为推导方便，若无特别说明以下讨论均在输出为$±1$的二分类任务上进行。）</p>

<h1 id="1-线性支持向量机">1. 线性支持向量机</h1>
<p><strong>线性支持向量机(Linear SVM)</strong>也叫<strong>硬间隔支持向量机(Hard-margin SVM)</strong>。</p>

<h3 id="1问题分析">(1)问题分析</h3>
<p>假设数据集\(\{(x^{(1)},y^{(1)}),...,(x^{(N)},y^{(N)})\}\)是线性可分的，其中\(x \in \Bbb{R}^d\)，\(y \in \{-1,+1\}\)。</p>

<p>特征空间中的一个超平面可以表示为$w^Tx+b=0$，希望能找到一个<strong>分离超平面(separating hyperplane)</strong>对样本正确的分类：</p>

\[\begin{cases} w^Tx^{(i)}+b&gt;0, &amp; y^{(i)} = +1 \\ w^Tx^{(i)}+b&lt;0, &amp; y^{(i)} = -1 \end{cases}\]

<p>这个条件记为：</p>

\[y^{(i)}(w^Tx^{(i)}+b) &gt; 0, \quad i=1,2,...,N\]

<p>$w$为超平面的法向量，决定超平面的方向；$b$是位移项，决定超平面与原点的距离。超平面仅仅区分样本是不够的，如果分离超平面距离某些样本点过近，当样本点受到噪声的扰动时，会使得分类结果出错，因此希望每一个样本点到该超平面得距离尽可能大：</p>

<p><img src="https://pic.imgdb.cn/item/5ec4cf98c2a9a83be56e416f.jpg" alt="" /></p>

<p>定义超平面到所有样本点的距离为<strong>间隔(margin)</strong>，问题是找到间隔最大的超平面，下面解释如何求间隔。</p>

<h3 id="2函数间隔与几何间隔">(2)函数间隔与几何间隔</h3>
<p>称上面定义的$y^{(i)}(w^Tx^{(i)}+b)$为样本点到超平面的<strong>函数间隔(functional margin)</strong>，可以定性地判断样本点到超平面的距离。</p>

<p>超平面$w^Tx+b=0$中的$w$是其法向量，证明如下：</p>

<p>任取超平面上两点$x’$、$x’’$，则满足$w^Tx’+b=0$、$w^Tx’‘+b=0$，两式做差得：
$w^T(x’-x’’)=0$,
即向量$w$正交于超平面。</p>

<p>空间中任意一点$x$到该超平面\(w^Tx+b=0\)的距离$dis(x,w,b)$是该点到超平面上任意一点$x’$的连线(记连线与超平面法线的夹角为$\theta$)在法向量$w$方向上的投影：</p>

<p><img src="https://pic.imgdb.cn/item/63a859d508b68301638171d1.jpg" alt="" /></p>

\[\begin{aligned} dis(x,w,b) &amp;= | x-x' | cosθ   \\ &amp;=  \frac{|w^T(x-x')|}{|| w ||} \quad (\text{由 }||w|| \cdot |x-x'| cosθ = |w^T(x-x')|)\\ &amp;= \frac{| w^Tx+b |}{|| w ||} \quad(\text{由 } w^Tx'+b=0 ) \end{aligned}\]

<p>为了去掉绝对值运算，上式修改为：</p>

\[dis(x,w,b) = \frac{y(w^Tx+b)}{|| w ||}\]

<p>定义超平面的<strong>几何间隔(geometric margin)</strong>为所有样本点到该超平面距离的最小值：</p>

\[\text{margin}(w,b) = \mathop{\min}_{i=1,...,N}dis(x^{(i)},w,b) = \mathop{\min}_{i=1,...,N} \frac{y(w^Tx^{(i)}+b)}{|| w ||}\]

<p>则线性支持向量机的问题列为：</p>

\[\begin{aligned} \mathop{\max}&amp; \text{margin}(w,b) \\ \text{s.t. } &amp;y^{(i)}(w^Tx^{(i)}+b) &gt; 0, \quad i=1,2,...,N \end{aligned}\]

<h3 id="3一些化简">(3)一些化简</h3>
<p>注意到超平面$w^Tx+b=0$的系数$w$和$b$的数值可以放缩，不会影响该超平面(超平面仅由$w$和$b$的比值决定)，为简化计算，可通过选取合适的$w$和$b$使得样本点到该超平面的最新函数间隔为$1$：</p>

\[\mathop{\min}_{i=1,...,N}y^{(i)}(w^Tx^{(i)}+b) = 1\]

<p>则原约束可以修改为：</p>

\[y^{(i)}(w^Tx^{(i)}+b) ≥ 1, \quad i=1,2,...,N\]

<p>超平面的几何间隔为：</p>

\[\text{margin}(w,b) = \frac{1}{|| w ||}\]

<p>目标函数变为：</p>

\[\mathop{\max} \text{margin}(w,b) = \mathop{\max} \frac{1}{|| w ||}\]

<p>将目标函数转换为：</p>

\[\mathop{\max} \frac{1}{|| w ||} = \mathop{\min} || w || = \mathop{\min} \frac{1}{2}w^Tw\]

<p>则线性支持向量机的问题转化为：</p>

\[\begin{aligned} \mathop{\min}&amp; \frac{1}{2}w^Tw \\ \text{s.t. } &amp;y^{(i)}(w^Tx^{(i)}+b) ≥ 1, \quad i=1,2,...,N \end{aligned}\]

<p>观察这个最优化问题，特点：</p>
<ol>
  <li>目标函数是二次函数</li>
  <li>约束条件是变量的一次函数</li>
</ol>

<p>该问题是一个<strong>凸二次规划(convex quadratic programming)</strong>问题，可以直接使用对应的工具包求解：</p>

<p><img src="https://pic.imgdb.cn/item/5ec4d895c2a9a83be57c7214.jpg" alt="" /></p>

<p>由该方法得到的超平面称为这些样本点的<strong>最大间隔分离超平面(large-margin separating hyperplane)</strong>，若样本集线性可分，则该超平面是存在且唯一。</p>

<h3 id="4支持向量">(4)支持向量</h3>
<p>线性支持向量机问题求解后，会有一些点$(x^{(i)},y^{(i)})$满足$y^{(i)}(w^Tx^{(i)}+b) = 1$，其余点$(x^{(j)},y^{(j)})$满足$y^{(j)}(w^Tx^{(j)}+b) &gt; 1$,</p>

<p>采用反证法证明上述结论，即假设所有点均满足$y^{(i)}(w^Tx^{(i)}+b) &gt; 1$，此时的$w$应满足$\mathop{\min} \frac{1}{2}w^Tw$，
对$w$和$b$同时缩小一点，仍然满足不等式，但打破了最小目标函数的条件，故假设不成立。</p>

<p>称满足$y^{(i)}(w^Tx^{(i)}+b) = 1$的样本点$(x^{(i)},y^{(i)})$为<strong>支持向量(support vector)</strong>，最大间隔分离超平面由这些支持向量唯一确定。</p>

<p>可以计算得到超平面的最大间隔为\(\frac{2}{\| w \|}\)。</p>

<p><img src="https://pic.imgdb.cn/item/63a85a8d08b683016382a185.jpg" alt="" /></p>

<h1 id="2-对偶支持向量机">2. 对偶支持向量机</h1>
<p>在支持向量机问题中，数据集\(\{(x^{(1)},y^{(1)}),...,(x^{(N)},y^{(N)})\},x \in \Bbb{R}^d\)，即共有$N$个样本，其中每个样本的特征是$d$维的；</p>

<p>上一节得到的二次规划问题称为支持向量机的<strong>原问题(prime problem)</strong>，该优化问题有$d+1$个待求解的变量，有$N$个约束条件。</p>

<p>有时样本的特征维度远高于样本个数，即$d»N$，使得原问题求解较为复杂；当引入核方法之后，样本的特征维度将变成无限维，导致原问题是不可解的。因此引入<strong>对偶支持向量机(dual SVM)</strong>。</p>

<h3 id="1拉格朗日函数">(1)拉格朗日函数</h3>
<p>原二次规划问题是一个求解参数$w,b$的<strong>约束(constrained)</strong>问题，通过引入<strong>拉格朗日乘子 (Lagrange Multiplier)</strong> $\alpha$将其转化成对参数$w,b$的<strong>无约束(unconstrained)</strong>问题。</p>

<p>记<strong>拉格朗日函数(Lagrange function)</strong>：</p>

\[L(w,b,α) = \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))}, \quad α_i≥0\]

<p>则将原问题转化为：</p>

\[\mathop{\min}_{w,b} \mathop{\max}_{α} L(w,b,α), \quad α_i≥0\]

<p>这个优化问题没有显式地给出对参数$w,b$的约束，但仍与原问题等价，分析如下：</p>
<ul>
  <li>对于满足原约束条件$y^{(i)}(w^Tx^{(i)}+b) ≥ 1$的$w,b$，为使拉格朗日函数最大需$α_i=0$，此时目标函数退化为$\frac{1}{2}w^Tw$;</li>
  <li>对于使$y^{(i)}(w^Tx^{(i)}+b) &lt; 1$的$w,b$，$α_i$可取$∞$，此时目标函数为$∞$；</li>
  <li>最终的优化问题从上述两种情况中取较小者，即第一种情况，恰好就是原二次规划的问题。</li>
</ul>

<p><img src="https://pic.imgdb.cn/item/5ec61217c2a9a83be5640991.jpg" alt="" /></p>

<h3 id="2拉格朗日对偶问题">(2)拉格朗日对偶问题</h3>
<p>由不等式：</p>

\[\mathop{\min}_{w,b} \mathop{\max}_{α} L(w,b,α) ≥ \mathop{\min}_{w,b} L(w,b,α)\]

<p>上式左端恒大于等于右端，自然大于等于右端给定$α$的情况：</p>

\[\mathop{\min}_{w,b} \mathop{\max}_{α} L(w,b,α) ≥ \mathop{\max}_{α} \mathop{\min}_{w,b} L(w,b,α)\]

<p>上式左端是引入拉格朗日函数的原问题，右端是其<strong>拉格朗日对偶问题(Lagrange dual problem)</strong>，具有<strong>弱对偶关系(weak duality)</strong>。</p>

<p>当优化问题是二次规划时，上式具有<strong>强对偶关系(strong duality)</strong>，即：</p>

\[\mathop{\min}_{w,b} \mathop{\max}_{α} L(w,b,α) = \mathop{\max}_{α} \mathop{\min}_{w,b} L(w,b,α)\]

<p>最终得到对偶支持向量机的优化问题：</p>

\[\begin{aligned} \mathop{\max}_{α} \mathop{\min}_{w,b} &amp;\frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))} \\ \text{s.t. } &amp; α_i≥0 \end{aligned}\]

<h3 id="3求解对偶问题">(3)求解对偶问题</h3>
<p>对偶问题是$w,b$的无约束问题，令其偏导数为零：</p>

\[\frac{\partial L(w,b,α)}{\partial w} = 0 \\ \frac{\partial L(w,b,α)}{\partial b} = 0\]

<p>解得：</p>

\[w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}} \\ \sum_{i=1}^{N} {α_iy^{(i)}} = 0\]

<p>对拉格朗日函数进行化简：</p>

\[\begin{aligned} L(w,b,α) =&amp; \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))} \\ =&amp; \frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} \\&amp;- \sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} - \sum_{i=1}^{N} {α_iy^{(i)}b} \\ =&amp; -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} \end{aligned}\]

<p>对偶问题转化为只与$α$有关的形式：</p>

\[\begin{aligned} \mathop{\max}_{α}&amp; -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} \\ \text{s.t. }&amp; α_i≥0,i=1,...,N \\ &amp; \sum_{i=1}^{N} {α_iy^{(i)}} = 0 \end{aligned}\]

<p>该优化问题有$N$个待求解的变量，有$N+1$个约束条件。</p>

<p>这是一个二次规划问题，可以使用第$5$节介绍的序列最小最优化算法求解；也可以直接使用对应的工具包求解，注意其中的等式约束可以拆解成两个不等式约束：</p>

<p><img src="https://pic.imgdb.cn/item/5ec61ac8c2a9a83be56e7964.jpg" alt="" /></p>

<h3 id="4kkt条件">(4)KKT条件</h3>
<p>求解得到$α$后，可以用<strong>KKT条件</strong>得到$w,b$。</p>

<p><strong>KKT条件(Karush-Kuhn-Tucker condition)</strong>是强对偶关系的等价条件，可用于求解优化问题；</p>

<p>对于对偶支持向量机问题，<strong>KKT条件</strong>包括：</p>
<ul>
  <li><strong>原问题的约束条件</strong>：$y^{(i)}(w^Tx^{(i)}+b) ≥ 1, \quad i=1,2,…,N$</li>
  <li><strong>对偶问题的约束条件</strong>：$α_i≥0, \quad i=1,2,…,N$</li>
  <li><strong>拉格朗日函数的一阶导数为零</strong>：</li>
</ul>

\[w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}} \\ \sum_{i=1}^{N} {α_iy^{(i)}} = 0\]

<ul>
  <li><strong>互补松弛条件(complementary slackness)</strong>：</li>
</ul>

\[α_i(1-y^{(i)}(w^Tx^{(i)}+b)) = 0, \quad i=1,2,...,N\]

<p>当得到$α$后，可直接得到$w$:</p>

\[w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}}\]

<p>由互补松弛条件，任取一个$α_s≠0$，则$1-y^{(s)}(w^Tx^{(s)}+b) = 0$，解得：</p>

\[b = y^{(s)}-w^Tx^{(s)}\]

<p>称$α_i≠0$对应的样本点为<strong>支持向量(support vector)</strong>，支持向量机的模型参数仅由这些支持向量确定(若$α_i=0$则不会出现在$w$的计算中)。</p>

<p>则支持向量机的判别函数为：</p>

\[\begin{aligned} y &amp;= \text{sign}(w^Tx+b) \\&amp;= \text{sign}(\sum_{i=1}^{N} {α_iy^{(i)}{x^{(i)}}^Tx} + y^{(s)} - \sum_{i=1}^{N} {α_iy^{(i)}{x^{(i)}}^Tx^{(s)}}) \end{aligned}\]

<h1 id="3-核支持向量机">3. 核支持向量机</h1>
<p>对偶支持向量机把优化问题转化成$N$个待求解的变量、$N+1$个约束条件的问题，与样本的特征维度$d$无关。但是在求解问题时，需计算内积${(x^{(i)})}^Tx^{(j)}$，仍需要$O(d)$的运算复杂度。</p>

<p>与此同时，线性支持向量机要求数据集是线性可分的，根据<a href="https://0809zheng.github.io/2021/07/23/kernel.html">核方法</a>，对于<strong>线性不可分</strong>的数据集，通常引入一个<strong>特征变换</strong>$z = φ(x)$把原始输入空间映射为一个更高维度的特征空间，使得样本在这个特征空间内线性可分(如果原始空间为有限维，一定存在一个高维线性空间使得样本可分)，此时的问题变为：</p>

\[\begin{aligned} \mathop{\max}_{α} &amp; -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{φ(x^{(i)})}^Tφ(x^{(j)})}} + \sum_{i=1}^{N} {α_i} \\ \text{s.t. } &amp;α_i≥0,i=1,...,N \\ &amp; \sum_{i=1}^{N} {α_iy^{(i)}} = 0 \end{aligned}\]

<p>问题的解：</p>

\[\begin{aligned} w &amp;= \sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})} \\ b &amp;= y^{(s)}-w^Tx^{(s)} = y^{(s)}-\sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})^Tφ(x^{(s)})} \end{aligned}\]

<p>则支持向量机的判别函数为：</p>

\[\begin{aligned} y &amp;= \text{sign}(w^Tz+b) \\&amp;= \text{sign}(\sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})^Tφ(x)} + y^{(s)} - \sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})^Tφ(x^{(s)})}) \end{aligned}\]

<p>当特征空间维度较高时(甚至是无穷维)，求解特征变换$φ(x)$以及特征变换的内积$φ(x)^Tφ(x’)$运算困难。因此引入<strong>核技巧(kernel trick)</strong>(参考<a href="https://0809zheng.github.io/2021/07/23/kernel.html">核方法</a>)。即在求解支持向量机问题中，不显式地计算特征变换$φ(x)$以及特征变换的内积$φ(x)^Tφ(x’)$，而是把计算特征变换$φ(x)$以及特征变换的内积$φ(x)^Tφ(x’)$转化为计算一个函数的值，这个函数称为<strong>核函数(kernel function)</strong>：</p>

\[K(x,x') = φ(x)^Tφ(x')\]

<p>则<strong>核支持向量机(kernelized SVM)</strong>问题：</p>

\[\begin{aligned} \mathop{\max}_{α} &amp; -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})}} + \sum_{i=1}^{N} {α_i} \\ \text{s.t. }&amp; α_i≥0,i=1,...,N \\ &amp; \sum_{i=1}^{N} {α_iy^{(i)}} = 0 \end{aligned}\]

<p>求解后可得最终的判别函数为：</p>

\[\begin{aligned} y &amp;= \text{sign}(w^Tφ(x)+b) \\&amp;= \text{sign}(\sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x)} + y^{(s)} - \sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x^{(s)})}) \end{aligned}\]

<h1 id="4-软间隔支持向量机">4. 软间隔支持向量机</h1>
<p>之前介绍的都是<strong>硬间隔(hard margin)</strong>支持向量机，在输入空间或特征空间中寻找一个最大间隔超平面。</p>

<p>对于线性不可分的数据，也可以适当放松条件，允许一些样本点不满足间隔条件甚至被错误分类，这种方法称为<strong>软间隔(soft margin)</strong>支持向量机。</p>

<p>定义<strong>松弛变量(slack variable</strong>,容忍度)对每个样本点到分离超平面的最小距离放松$ξ_i$，并使这些放松条件尽可能小：</p>

\[\begin{aligned} \mathop{\min} &amp;\frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} \\ \text{s.t. } &amp; y^{(i)}(w^Tx^{(i)}+b) ≥ 1-ξ_i, \quad i=1,2,...,N \\ &amp; ξ_i ≥ 0, \quad i=1,2,...,N \end{aligned}\]

<p>超参数<strong>惩罚系数</strong>$C$权衡间隔大小和分类错误点的容忍程度。</p>
<ul>
  <li>$C$越大，样本点到超平面的距离越大，分类错误的点数减少，最大间隔减小；</li>
  <li>$C$越小，样本点到超平面的距离限制减小，最大间隔增大，但可能分类错误的点数增加。</li>
</ul>

<h3 id="1对偶问题">(1)对偶问题</h3>
<p>软间隔支持向量机的优化问题也是二次规划问题，引入拉格朗日函数：</p>

\[\begin{aligned} L(w,b,ξ,α,β) = &amp;\frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} \\&amp;+ \sum_{i=1}^{N} {α_i(1-ξ_i-y^{(i)}(w^Tx^{(i)}+b))} \\&amp;+ \sum_{i=1}^{N} {-β_iξ_i}, \quad α_i≥0,β_i≥0 \end{aligned}\]

<p>则对偶问题可写作：</p>

\[\begin{aligned} \mathop{\max}_{α,β} \mathop{\min}_{w,b,ξ} &amp;L(w,b,ξ,α,β) \\ \text{s.t. } &amp; α_i≥0,β_i≥0,i=1,2,...,N \end{aligned}\]

<p>令$\frac{\partial L(w,b,ξ,α,β)}{\partial ξ_i} = 0$，得：</p>

\[C - α_i - β_i = 0\]

<p>把$β_i = C - α_i$代入对偶问题并化简得：</p>

\[\begin{aligned} \mathop{\max}_{α} \mathop{\min}_{w,b,ξ} &amp;\frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} \\&amp;+ \sum_{i=1}^{N} {α_i(1-ξ_i-y^{(i)}(w^Tx^{(i)}+b))} + \sum_{i=1}^{N} {-(C - α_i)ξ_i} \\ &amp;= \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))} \\ \text{s.t. } &amp; C≥α_i≥0,i=1,2,...,N \end{aligned}\]

<p>化简该对偶问题：</p>

\[\begin{aligned} \mathop{\max}_{α} &amp; -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} \\ \text{s.t. }&amp; 0≤α_i≤C,i=1,...,N \\ &amp; \sum_{i=1}^{N} {α_iy^{(i)}} = 0 \end{aligned}\]

<p>该优化问题有$N$个待求解的变量，有$2N+1$个约束条件；与硬间隔支持向量机问题相比，仅仅是多了对$α$的<strong>上界条件</strong>。</p>

<p>通过二次规划方法或<strong>SMO</strong>算法（见第$5$章）解得$α_i$后，利用<strong>KKT</strong>条件求解$w$和$b$：</p>

\[w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}}\]

<p>由互补松弛条件：</p>
<ul>
  <li>$α_i(1-ξ_i-y^{(i)}(w^Tx^{(i)}+b))=0$</li>
  <li>$-β_iξ_i = (C - α_i)ξ_i = 0$</li>
</ul>

<p>任选一个满足$0&lt;α_s&lt;C$的支持向量$α_s$，使得：</p>

\[b = y^{(s)}-w^Tx^{(s)} = y^{(s)}-\sum_{i=1}^{N} {α_iy^{(i)}{x^{(i)}}^Tx^{(s)}}\]

<h3 id="2几何解释">(2)几何解释</h3>
<p>根据$α_i$的取值不同，样本点可分为三类：</p>
<ul>
  <li>$α_i=0$：此时$ξ_i = 0$，对应最大间隔超平面完全分类正确的样本点；</li>
  <li>$0&lt;α_i&lt;C$：此时$ξ_i = 0$，$y^{(i)}(w^Tx^{(i)}+b)=1$，对应恰好在间隔边界上的点，称为<strong>free support vector</strong>，对应图中正方形框的点；</li>
  <li>$α_i=C$：此时$ξ_i ≠ 0$，$y^{(i)}(w^Tx^{(i)}+b)=1-ξ_i$，对应放松了间隔条件的点，称为<strong>bounded support vector</strong>，对应图中三角形框的点，此时若$ξ_i &lt; 1$则样本点分类正确，$ξ_i &gt; 1$则样本点分类错误。</li>
</ul>

<p><img src="https://pic.imgdb.cn/item/5ec8b980c2a9a83be592f424.jpg" alt="" /></p>

<h3 id="3超参数c的选择">(3)超参数C的选择</h3>
<p>通常用<strong>验证集</strong>选择超参数$C$。</p>

<p>对于$N$个样本采用<strong>留一交叉验证(leave-one-out cross validation)</strong>，其验证集误差满足以下不等式：</p>

\[\text{error}_{\text{loocv}} ≤ \frac{\#SV}{N}\]

<p>此时验证误差不超过样本集中支持向量所占的比例，证明如下：</p>
<ul>
  <li>若其中某个样本点是支持向量，将其划分出来作为验证集时，会使最终超平面改变，此时该点可能被错误分类，故误差$≤1$；</li>
  <li>若其中某个样本点不是支持向量，将其划分出来作为验证集时，对最终超平面没有影响，此时该点仍会被分类正确，故误差$=0$。</li>
</ul>

<p>故<strong>支持向量的个数</strong>一定程度上可以作为超参数选择的依据：</p>

<p><img src="https://pic.imgdb.cn/item/63a85c8808b683016385ee2e.jpg" alt="" /></p>

<h3 id="4合页损失hinge-loss函数">(4)合页损失(Hinge Loss)函数</h3>
<p>软间隔支持向量机的优化问题：</p>

\[\begin{aligned} \mathop{\min} &amp; \frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} \\ \text{s.t. } &amp;y^{(i)}(w^Tx^{(i)}+b) ≥ 1-ξ_i, \quad i=1,2,...,N \\ &amp; ξ_i ≥ 0, \quad i=1,2,...,N \end{aligned}\]

<p>分析约束条件，$ξ_i$描述的是样本点$(x^{(i)},y^{(i)})$到分离间隔$y(w^Tx+b)=1$的距离：</p>

\[ξ_i = \begin{cases} 0, &amp; y^{(i)}(w^Tx^{(i)}+b)≥1 \\ 1-y^{(i)}(w^Tx^{(i)}+b), &amp; y^{(i)}(w^Tx^{(i)}+b)&lt;1 \end{cases}\]

<p>将上式表达为：</p>

\[ξ_i = \mathop{\max}[1-y^{(i)}(w^Tx^{(i)}+b),0] = [1-y^{(i)}(w^Tx^{(i)}+b)]_+\]

<p>则软间隔支持向量机的优化问题等价于目标函数为<strong>合页损失函数(hinge loss)</strong>的优化问题：</p>

\[\mathop{\min}  \sum_{i=1}^{N} {[1-y^{(i)}(w^Tx^{(i)}+b)]_+} + λw^Tw\]

<p>其中正则化系数$λ=\frac{1}{2C}$。</p>

<p>由此可以看出，支持向量机的出发点“间隔最大化”具有一定的$L2$正则化的作用。</p>

<p>对于软间隔支持向量机，之所以采用对偶方法求解约束规划而不是求解<strong>hinge loss</strong>，主要原因如下：</p>
<ol>
  <li>凸二次规划问题具有良好的解析性质，可以求得解析解；而上述无约束目标函数需要用梯度下降方法，且存在求最大值的操作，不利于微分；</li>
  <li>直接解上面的无约束目标函数无法直接使用核方法。</li>
</ol>

<p>当标签为$±1$时，比较感知机、逻辑回归和支持向量机的损失函数：</p>
<ul>
  <li>感知机：<strong>0/1损失</strong>：\(E_{0/1}=[\text{sign}(wx)=y]=[\text{sign}(ywx)=1]\)</li>
  <li>逻辑回归：以$2$为底的<strong>交叉熵</strong>：\(E_{\text{scaled-ce}}=\text{log}_2(1+\text{exp}(-ywx))=\frac{1}{ln2}E_{\text{ce}}\)</li>
  <li>支持向量机：<strong>Hinge Loss</strong>：\(E_{\text{svm}}=[1-ywx]_+\)</li>
</ul>

<p>经过换底后的交叉熵损失和支持向量机损失都是$0/1$损失的上界：</p>

<p><img src="https://pic.imgdb.cn/item/5ed49547c2a9a83be5ad4673.jpg" alt="" /></p>

<h1 id="5-序列最小最优化算法">5. 序列最小最优化算法</h1>
<p><strong>序列最小最优化(sequential minimal optimization，SMO)</strong>算法是用来解支持向量机对偶问题的数值方法。</p>

<p>引入核函数的软间隔支持向量机的优化问题如下：</p>

\[\begin{aligned} \mathop{\min}_{α} &amp; \frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})}} - \sum_{i=1}^{N} {α_i} \\ \text{s.t. } &amp; 0≤α_i≤C,i=1,...,N \\ &amp; \sum_{i=1}^{N} {α_iy^{(i)}} = 0 \end{aligned}\]

<p>该优化问题需要求解$N$个参数$α_1,…,α_N$。</p>

<p>序列最小最优化算法的思想是，在循环中每次选择其中的两个参数$α_1,α_2$，固定其他参数进行优化(不能只选择一个参数$\alpha_1$优化，因为这样会由约束$\sum_{i=1}^{N} {α_iy^{(i)}} = 0$直接导出$\alpha_1$)，固定其余参数，用解析的方法解两个变量的二次规划问题。</p>

<p>注意到子问题中只有一个变量是自由变量，另外一个变量可以被表示为：</p>

\[α_1 = -y^{(1)}\sum_{i=2}^{N} {α_iy^{(i)}}\]

<h3 id="1两个变量的二次规划问题">(1)两个变量的二次规划问题</h3>
<p>当选择$α_1,α_2$作为变量时，子问题为：</p>

\[\begin{aligned} \mathop{\min}_{α}&amp; \frac{1}{2} α_1^2K_{11} + \frac{1}{2} α_2^2K_{22} + α_1α_2y^{(1)}y^{(2)}K_{12} \\ &amp;+ α_1y^{(1)} \sum_{i=3}^{N} {α_iy^{(i)}K_{1i}} + α_2y^{(2)} \sum_{i=3}^{N} {α_iy^{(i)}K_{2i}} - α_1 - α_2 \\ \text{s.t. }&amp; 0≤α_i≤C,i=1,2 \\ &amp; α_1y^{(1)} + α_2y^{(2)} = -\sum_{i=3}^{N} {α_iy^{(i)}} \end{aligned}\]

<p>其中记$K_{ij}=K(x^{(i)},x^{(j)})$。</p>

<p>记$g(x)$为当前时刻支持向量机的判别函数，</p>

\[g(x) = \sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x)}+b\]

<p>记当前时刻与真实标签的差值为$E(x^{(i)}) = g(x^{(i)})-y^{(i)}$；</p>

<p>若先求解变量$α_2$，将等式约束$α_1 = y^{(1)}(α_2y^{(2)}-\sum_{i=3}^{N} {α_iy^{(i)}})$代入目标函数后，对其求关于$α_2$的偏导数，令其为零，得：</p>

\[α_2^{new} = α_2^{old} +\frac{y^{(2)}(E(x^{(1)})-E(x^{(2)}))}{K_{11}+K_{22}-2K_{12}}\]

<p>考虑不等式约束$0≤α_1≤C,0≤α_2≤C$，需要限制$α_2^{new}$的更新范围：</p>

\[L≤α_2^{new}≤H\]

<p>当$y^{(1)} ≠ y^{(2)}$时:</p>

\[\begin{cases} L=max(α_2^{old}-α_1^{old},0) \\ H = min(C-α_1^{old}+α_2^{old},C) \end{cases}\]

<p>当$y^{(1)} = y^{(2)}$时:</p>

\[\begin{cases} L=max(α_1^{old}+α_2^{old}-C,0) \\ H = min(α_1^{old}+α_2^{old},C) \end{cases}\]

<p><img src="https://pic.imgdb.cn/item/5ec8e88fc2a9a83be5cb3cdb.jpg" alt="" /></p>

<p>因此更新后的$α_2^{new}$：</p>

\[α_2^{new} = \begin{cases} L, &amp; α_2^{new}≤L \\ H, &amp; α_2^{new}≥H \\ α_2^{new}, &amp; otherwise \end{cases}\]

<p>由约束条件$y^{(1)}α_1^{old} + y^{(2)}α_2^{old} = y^{(1)}α_1^{new} + y^{(2)}α_2^{new}$得：</p>

\[α_1^{new} = α_1^{old} + y^{(1)}y^{(2)}(α_2^{old}-α_2^{new})\]

<p>算法每一步迭代得到新的$α_1,α_2$后需要更新参数$b$:</p>

<p>若$0&lt;α_1^{new}&lt;C$，则：</p>

\[\begin{aligned} b_1^{new} &amp;= y^{(1)}-\sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x^{(1)})} \\ &amp;= -E(x^{(1)}) -y^{(1)}K_{11}(α_1^{new} - α_1^{old}) - y^{(2)}K_{21}(α_2^{new} - α_2^{old}) \end{aligned}\]

<p>若$0&lt;α_2^{new}&lt;C$，则：</p>

\[\begin{aligned} b_2^{new} &amp;= y^{(2)}-\sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x^{(2)})} \\ &amp;= -E(x^{(2)}) -y^{(1)}K_{12}(α_1^{new} - α_1^{old}) - y^{(2)}K_{22}(α_2^{new} - α_2^{old}) \end{aligned}\]

<p>取$b^{new}=\frac{b_1^{new}+b_2^{new}}{2}$。</p>

<h3 id="2变量的选择">(2)变量的选择</h3>
<p>每次循环更新当前时刻与真实标签的差值：</p>

\[E(x^{(i)}) = g(x^{(i)})-y^{(i)} = \sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x)}+b-y^{(i)}\]

<ul>
  <li>外层循环选择第一个变量：选择误差$\mid E(x^{(i)}) \mid$最大的样本对应的$α_i$(违反<strong>KKT</strong>条件程度最大的变量)；</li>
  <li>内层循环选择第二个变量：选择使$\mid E(x^{(i)})-E(x^{(j)}) \mid$最大的样本对应的$α_j$(选择使目标函数数值增长最快的变量，启发式地选择两变量使得所对应样本之间的间隔最大)。</li>
</ul>

<h1 id="6-概率支持向量机">6. 概率支持向量机</h1>
<p><strong>概率支持向量机(probabilistic SVM)</strong>是将支持向量机和<a href="https://0809zheng.github.io/2020/03/13/logistic-regression.html">逻辑回归</a>结合起来的方法。既具有支持向量机问题解析的优良性，又具有逻辑回归中样本概率极大似然的思想。</p>

<p>该方法先求解核函数支持向量机问题，得到（未经过符号函数）的模型输出：</p>

\[w_{\text{SVM}}^Tφ(x)+b_{\text{SVM}}\]

<p>将该标量通过<strong>scaling</strong>参数$A$和<strong>shifting</strong>参数$B$后，喂入Logistic函数，得到预测概率：</p>

\[g(x) = σ(A(w_{\text{SVM}}^Tφ(x)+b_{\text{SVM}})+B)\]

<p>该问题只有两个变量$A$和$B$。</p>

<p>概率支持向量机的熵损失函数（标签为$±1$）：</p>

\[\begin{aligned} L(w) &amp;= \sum_{i=1}^{N} {-\ln(A(w_{\text{SVM}}^Tφ(x)+b_{\text{SVM}})+B)} \\&amp;= \sum_{i=1}^{N} {\ln(1+\exp(A(w_{\text{SVM}}^Tφ(x)+b_{\text{SVM}})+B))} \end{aligned}\]

<p>这是一个无约束的凸优化问题，可以使用梯度下降方法。</p>

<h1 id="7-最小二乘支持向量机">7. 最小二乘支持向量机</h1>
<p><strong>最小二乘支持向量机(least-squares SVM，LSSVM)</strong>就是将<a href="https://0809zheng.github.io/2020/03/30/ridge.html#2-kernel-ridge-regression">kernel ridge regression</a>用于分类任务。</p>

<p>由引入核方法的岭回归对数据拟合，得到回归方程：</p>

\[y = \sum_{n=1}^{N} {β_nK(x^{(n)},x)}\]

<p>用该方程作为分类超平面，就得到最小二乘支持向量机模型。其中超平面是由回归的均方误差确定的，故称为“最小二乘”。</p>

<p>比较软间隔支持向量机和最小二乘支持向量机：</p>

<p><img src="https://pic.imgdb.cn/item/5ed5fc84c2a9a83be564976b.jpg" alt="" /></p>

<p>两者的分类边界是类似的，但前者的支持向量少（对应$α≠0$的样本），后者的支持向量多（对应$β≠0$的样本），计算量更大。</p>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/03/15/support-vector-regression.html" class="read-next-link"></a>
        <section>
          <span>支持向量回归(Support Vector Regression, SVR)</span>
          <p>  Support Vector Regression.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.downk.cc/item/5ed759b9c2a9a83be54d254c.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/03/13/logistic-regression.html" class="read-next-link"></a>
          <section>
            <span>逻辑回归(Logistic Regression)</span>
            <p>  Logistic Regression.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5ed75927c2a9a83be54c53ef.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
