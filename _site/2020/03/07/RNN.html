<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>循环神经网络(Recurrent Neural Network) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="循环神经网络(Recurrent Neural Network)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="循环神经网络(Recurrent Neural Network) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/03/07/RNN.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>循环神经网络(Recurrent Neural Network)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-03-07"><i class="iconfont icon-date"></i>07 Mar 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5e9fda52c2a9a83be551d194.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Recurrent Neural Networks.</p>
</blockquote>

<p><strong>循环神经网络(Recurrent Neural Network，RNN)</strong>可以建模序列数据之间的相关性，可以处理输入长度不固定的文本等时序数据。<strong>RNN</strong>每一时刻的输出$y_t$不仅和当前时刻的输入$x_t$相关，也和上一时刻的输出$y_{t-1}$相关:</p>

\[y_t = f(y_{t-1},x_t)\]

<p><strong>本文目录</strong>：</p>
<ol>
  <li><strong>vanilla RNN</strong></li>
  <li><strong>RNN</strong>的梯度下降：随时间反向传播, 实时循环学习</li>
  <li><strong>RNN</strong>的长程依赖问题</li>
  <li><strong>RNN</strong>门控机制：<strong>LSTM</strong>, <strong>GRU</strong>, <strong>QRNN</strong>, <strong>SRU</strong>, <strong>ON-LSTM</strong></li>
  <li>深层<strong>RNN</strong>：<strong>Stacked RNN</strong>, <strong>Bidirectional RNN</strong></li>
</ol>

<h1 id="1-vanilla-rnn">1. vanilla RNN</h1>
<p>一个简单的循环神经网络包括输入层、一层隐藏层和输出层。</p>

<p><img src="https://pic.downk.cc/item/5e9fdc29c2a9a83be5533395.jpg" alt="" /></p>

<p>令向量\(x_t \in \Bbb{R}^M\)表示t时刻网络的输入，\(h_t \in \Bbb{R}^D\)表示<strong>隐藏层状态(hidden state)</strong>，则：</p>

\[\begin{aligned}  h_t &amp;= f(W_{hh}h_{t-1}+W_{xh}x_t+b) \\ y_t &amp;= W_{hy}h_t \end{aligned}\]

<p>其中$f(\cdot)$是激活函数，常用<strong>Sigmoid</strong>或<strong>Tanh</strong>函数；参数$W_{hh}$、$W_{xh}$、$W_{hy}$、$b$在时间维度上<strong>权值共享</strong>。</p>

<h3 id="-性质1通用近似定理universal-approximation-theory">⚪ 性质1：通用近似定理（Universal Approximation Theory）</h3>

<p>如果一个完全连接的循环神经网络有足够数量的<strong>sigmoid</strong>神经元，它可以以任意的准确率去近似任何一个非线性动力系统。</p>

<p>一个非线性动力系统可以用常微分方程(<strong>Ordinary Differential Equation, ODE</strong>)描述：</p>

\[\dot{x}(t) = f(x(t),t)\]

<p><strong>ODE</strong>通常比较难以求出解析解，可以采用欧拉解法，即用$\frac{x(t+h)-x(t)}{h}$近似导数项$\dot{x}(t)$，则迭代公式为：</p>

\[x(t+h)=x(t)+hf(x(t),t)\]

<p>因此<strong>ODE</strong>的欧拉解法是<strong>RNN</strong>的一个特例，这也说明了<strong>RNN</strong>对于时间序列数据具有很强的拟合能力。</p>

<h3 id="-性质2图灵完备性turing-completeness">⚪ 性质2：图灵完备性（Turing Completeness）</h3>

<p>图灵完备是指一种数据操作规则，比如一种计算机编程语言，可以实现图灵机（<strong>Turing Machine</strong>）的所有功能，解决所有的可计算问题。目前主流的编程语言（比如<strong>C++</strong>、<strong>Java</strong>、<strong>Python</strong>等）都是图灵完备的。</p>

<p><strong>RNN</strong>的图灵完备性是指所有的图灵机都可以被一个由使用<strong>Sigmoid</strong>神经元构成的全连接循环网络来进行模拟。</p>

<h3 id="-rnn的pytorch实现">⚪ RNN的Pytorch实现</h3>

<p>可以通过<a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN"><code class="language-plaintext highlighter-rouge">torch.nn.RNN</code></a>构建<strong>RNN</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">RNN</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># 输入序列的特征维度
</span>    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="c1"># 隐藏层状态的特征维度
</span>    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># 循环层的数量，默认为1，用于实现Stacked RNN
</span>    <span class="n">nonlinearity</span><span class="o">=</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># 激活函数，可选'relu'
</span>     <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># 是否使用偏差项b
</span>    <span class="n">batch_first</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 若设置为True，则输入尺寸应为[Batch Size, Sequence Length, Input Size]
</span>    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 设置神经元的dropout
</span>    <span class="n">bidirectional</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 设置双向RNN
</span>    <span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># [Sequence Length, Batch Size, Input Size]
</span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="c1"># [Bidirectional, Batch Size, Hidden Size]
</span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="nf">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="2-rnn的梯度下降">2. RNN的梯度下降</h1>
<p><strong>RNN</strong>的参数可以通过梯度下降方法来进行学习，在<strong>RNN</strong>中主要有两种计算梯度的方式：</p>
<ol>
  <li>随时间反向传播（<strong>BPTT</strong>）算法</li>
  <li>实时循环学习（<strong>RTRL</strong>）算法</li>
</ol>

<p>为了更好地说明，<strong>RNN</strong>的参数更新过程，引入中间变量$z_t$表示应用激活函数前的隐状态，则<strong>RNN</strong>的前向传播过程可写作：</p>

\[\begin{aligned} z_t &amp;= W_{hh}h_{t-1}+W_{xh}x_t+b \\ h_t &amp;= f(z_t) \\ y_t &amp;= W_{hy}h_t \end{aligned}\]

<h2 id="1-随时间反向传播">(1) 随时间反向传播</h2>
<p><strong>随时间反向传播（BackPropagation Through Time，BPTT）</strong>算法将循环神经网络看作一个展开的多层前馈网络，其中“每一层”对应循环网络中的“每个时刻”。</p>

<p>定义误差项\(δ_{t,k} = \frac{\partial L_t}{\partial z_k}\)，则误差的反向传播：</p>

\[δ_{t,k} = \frac{\partial L_t}{\partial z_k} = \frac{\partial L_t}{\partial z_{k+1}} \frac{\partial z_{k+1}}{\partial z_k} = \frac{\partial L_t}{\partial z_{k+1}} \frac{\partial z_{k+1}}{\partial h_k} \frac{\partial h_k}{\partial z_k} = δ_{t,k+1}W_{hh}f'(z_k)\]

<p><img src="https://pic.downk.cc/item/5e9fe01dc2a9a83be5561566.jpg" alt="" /></p>

<p><strong>RNN</strong>所有层的参数是共享的，因此参数的真实梯度是所有“展开层”的参数梯度之和：</p>

\[\begin{aligned} \frac{\partial L_t}{\partial W_{hh}} &amp;= \sum_{t=1}^{T} {\sum_{k=1}^{t-1} {δ_{t,k}h_{k-1}}} \\ \frac{\partial L_t}{\partial W_{xh}} &amp;= \sum_{t=1}^{T} {\sum_{k=1}^{t-1} {δ_{t,k}x_{k}}} \\ \frac{\partial L_t}{\partial b} &amp;= \sum_{t=1}^{T} {\sum_{k=1}^{t-1} {δ_{t,k}}} \end{aligned}\]

<h2 id="2-实时循环学习">(2) 实时循环学习</h2>
<p><strong>实时循环学习（Real-Time Recurrent Learning，RTRL）</strong>是通过前向传播的方式来计算梯度，以$W_{hh}$为例：</p>

\[\begin{aligned} \frac{\partial h_{t+1}}{\partial W_{hh}} &amp;= \frac{\partial h_{t+1}}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial W_{hh}} = \frac{\partial h_{t+1}}{\partial z_{t+1}} \frac{\partial W_{hh}h_t}{\partial W_{hh}} \\ \frac{\partial L_{t}}{\partial W_{hh}} &amp;= \frac{\partial h_{t}}{\partial W_{hh}} \frac{\partial L_{t+1}}{\partial h_{t}}  \end{aligned}\]

<p><strong>RTRL</strong>算法和<strong>BPTT</strong>算法都是基于梯度下降的算法，分别通过前向模式和反向模式应用链式法则来计算梯度。</p>

<p>在循环神经网络中，一般网络输出维度远低于输入维度，因此<strong>BPTT</strong>算法的计算量会更小，但是<strong>BPTT</strong>算法需要保存所有时刻的中间梯度，空间复杂度较高。</p>

<p><strong>RTRL</strong>算法不需要梯度回传，因此非常适合用于需要在线学习或无限序列的任务。</p>

<h1 id="3-rnn的长程依赖问题">3. RNN的长程依赖问题</h1>
<p>RNN反向传播中的误差项\(δ_{t,k}\)满足：</p>

\[δ_{t,k} = δ_{t,k+1}W_{hh}f'(z_k)\]

<p>若记\(γ ≈ W_{hh}f'(z_k)\)，则：</p>

\[δ_{t,k} = γ^{t-k}δ_{t,t}\]

<ul>
  <li>若$γ&lt;1$，当$t-k → ∞$时，$γ^{t-k} → 0$，出现<strong>梯度消失(Vanishing Gradient)</strong>问题;</li>
  <li>若$γ&gt;1$，当$t-k → ∞$时，$γ^{t-k} → ∞$，出现<strong>梯度爆炸(Exploding Gradient)</strong>问题;</li>
</ul>

<p>由于<strong>RNN</strong>经常使用<strong>Sigmoid</strong>函数或<strong>Tanh</strong>函数作为非线性激活函数，其导数值都小于<strong>1</strong>，因而经常会出现梯度消失问题。</p>

<p>值得注意的是，梯度消失并不是参数$W$的梯度\(\frac{\partial L_{t}}{\partial W}\)消失了，而是隐藏层状态$h_{k}$的梯度\(\frac{\partial L_{t}}{\partial h_{k}}\)消失了。也就说参数$W$的更新主要靠最近时刻的几个相邻状态更新，而长距离的状态则无法产生影响。</p>

<p>虽然<strong>RNN</strong>理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度消失问题，实际上只能学习到短期的依赖关系。这个问题称作<strong>长程依赖问题（Long-Term Dependencies Problem）</strong>。</p>

<p>为了减缓上述问题，可以采取以下措施：</p>
<ul>
  <li><strong>梯度爆炸</strong>：权重衰减、梯度截断</li>
  <li><strong>梯度消失</strong>（主要问题）：引入门控机制</li>
</ul>

<h1 id="4-rnn的门控机制">4. RNN的门控机制</h1>
<p>为了改善循环神经网络的长程依赖问题，引入了<strong>门控机制(Gated Mechanism)</strong>。</p>

<h2 id="1-长短期记忆网络-lstm">(1) 长短期记忆网络 LSTM</h2>
<p><strong>长短期记忆网络（Long Short-Term Memory Network，LSTM）</strong>引入了门控机制来控制信息传递的路径，可以有效地解决<strong>RNN</strong>的梯度消失问题。</p>

<p><strong>LSTM</strong>网络引入了输入门$i$ (<strong>input gate</strong>)、遗忘门$f$ (<strong>forget gate</strong>)，和输出门$o$ (<strong>output gate</strong>)；并把输入和隐状态整合为记忆状态$c$（<strong>cell state</strong>）；根据遗忘门和输入门更新记忆状态后，根据输出门更新隐状态。记忆状态通过线性的循环信息控制缓解了梯度消失问题。</p>

\[\begin{aligned} i_t &amp;= \sigma(W_{i}x_t+U_{i}h_{t-1}+b_i) \\ f_t &amp;= \sigma(W_{f}x_t+U_{f}h_{t-1}+b_f) \\ o_t &amp;= \sigma(W_{o}x_t+U_{o}h_{t-1}+b_o) \\ \tilde{c}_t &amp;= \text{tanh}(W_{c}x_t+U_{c}h_{t-1}+b_c) \\ c_t &amp;= c_{t-1} \odot f_t + i_t \odot \tilde{c}_t \\ h_{t} &amp;= o_t \odot \text{tanh}(c_t) \end{aligned}\]

<p><img src="https://pic.downk.cc/item/5ea12bdbc2a9a83be5a8834d.jpg" alt="" /></p>

<ul>
  <li><strong>短期记忆（Short-Term Memory）</strong>：隐状态$h$每个时刻都会重写；</li>
  <li><strong>长期记忆（Long-Term Memory）</strong>：网络参数更新周期要远远慢于短期记忆；</li>
  <li><strong>长短期记忆（Long Short-Term Memory）</strong>：记忆单元$c$中保存信息的生命周期要长于短期记忆$h$，但又远远短于长期记忆。</li>
</ul>

<p>针对<strong>LSTM</strong>的一些改进：</p>
<ol>
  <li>遗忘门参数的初始化如果比较小会丢弃前一时刻的大部分信息，很难捕捉到长距离的依赖信息。因此遗忘门的参数初始值一般都设得比较大；</li>
  <li><strong>peephole</strong>：输入门$i$、遗忘门$f$和输出门$o$不但依赖于输入$x_t$和上一时刻的隐状态$h_{t-1}$，也依赖于记忆单元$c$；
<img src="https://pic.downk.cc/item/5ea13528c2a9a83be5b242d2.jpg" alt="" /></li>
  <li>输入门和遗忘门有些互补关系，可以耦合输入门和遗忘门：</li>
</ol>

\[\begin{aligned} f_t &amp;= 1-i_t \\ c_t &amp;= c_{t-1} \odot (1-i_t) + i_t \odot \tilde{c}_t \end{aligned}\]

<p>下面给出单个<strong>LSTM</strong>模块的定义和序列的循环处理过程：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LSTMCell</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">ih</span><span class="p">,</span> <span class="n">hh</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">ih</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">))</span>
                <span class="n">hh</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ih</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">))</span>
                <span class="n">hh</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_ih</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span><span class="n">ih</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w_hh</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span><span class="n">hh</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">:</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span>
                <span class="n">torch</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">hidden</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>

        <span class="n">hy</span><span class="p">,</span> <span class="n">cy</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hx</span><span class="p">,</span> <span class="n">cx</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
            <span class="n">gates</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">w_ih</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">w_hh</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hx</span><span class="p">)</span>
            <span class="n">i_gate</span><span class="p">,</span> <span class="n">f_gate</span><span class="p">,</span> <span class="n">c_gate</span><span class="p">,</span> <span class="n">o_gate</span> <span class="o">=</span> <span class="n">gates</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">i_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">i_gate</span><span class="p">)</span>
            <span class="n">f_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">f_gate</span><span class="p">)</span>
            <span class="n">c_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">c_gate</span><span class="p">)</span>
            <span class="n">o_gate</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">o_gate</span><span class="p">)</span>
            <span class="n">ncx</span> <span class="o">=</span> <span class="p">(</span><span class="n">f_gate</span> <span class="o">*</span> <span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">i_gate</span> <span class="o">*</span> <span class="n">c_gate</span><span class="p">)</span>
            <span class="n">nhx</span> <span class="o">=</span> <span class="n">o_gate</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">ncx</span><span class="p">)</span>
            <span class="n">cy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ncx</span><span class="p">)</span>
            <span class="n">hy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nhx</span><span class="p">)</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">nhx</span><span class="p">)</span>

        <span class="n">hy</span><span class="p">,</span> <span class="n">cy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">hy</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">cy</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># number of layer * batch * hidden
</span>        <span class="k">return</span> <span class="n">hy</span><span class="p">,</span> <span class="n">cy</span>

<span class="n">lstm</span> <span class="o">=</span> <span class="nc">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># [Sequence Length, Batch Size, Input Size]
</span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="c1"># [Batch Size, Hidden Size]
</span><span class="n">cx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="c1"># [Batch Size, Cell Size]
</span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">hx</span><span class="p">,</span> <span class="n">cx</span> <span class="o">=</span> <span class="nf">lstm</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="n">hx</span><span class="p">,</span> <span class="n">cx</span><span class="p">))</span>
    <span class="n">output</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>也可以通过<a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM"><code class="language-plaintext highlighter-rouge">torch.nn.LSTM</code></a>构建<strong>LSTM</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># 输入序列的特征维度
</span>    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="c1"># 隐藏层状态的特征维度
</span>    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># 循环层的数量，默认为1，用于实现Stacked RNN
</span>    <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># 是否使用偏差项b
</span>    <span class="n">batch_first</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 若设置为True，则输入尺寸应为[Batch Size, Sequence Length, Input Size]
</span>    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 设置神经元的dropout
</span>    <span class="n">bidirectional</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 设置双向RNN
</span>    <span class="n">proj_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 记忆状态的特征维度，默认等于hidden_size
</span>    <span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># [Sequence Length, Batch Size, Input Size]
</span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="c1"># [Bidirectional, Batch Size, Hidden Size]
</span><span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="c1"># [Bidirectional, Batch Size, Cell Size]
</span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="nf">lstm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="2-门控循环单元-gru">(2) 门控循环单元 GRU</h2>
<p><strong>门控循环单元（Gated Recurrent Unit，GRU）</strong>比<strong>LSTM</strong>结构更加简单。</p>

<p><strong>GRU</strong>没有引入新的记忆状态，而是引入了更新门$z$ (<strong>update gate</strong>) 和重置门$r$ (<strong>reset gate</strong>):</p>

\[\begin{aligned} z_t &amp;= \sigma(W_{z}x_t+U_{z}h_{t-1}+b_z) \\ r_t &amp;= \sigma(W_{r}x_t+U_{r}h_{t-1}+b_r) \\  \tilde{h}_t &amp;= \text{tanh}(W_{h}x_t+U_{h}h_{t-1}\odot r_t+b_h) \\h_{t} &amp;= z_t \odot h_{t-1} + (1-z_t) \odot  h_{t} \end{aligned}\]

<p><img src="https://pic.downk.cc/item/5ea12d0ac2a9a83be5a9da10.jpg" alt="" /></p>

<p>可以通过<a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU"><code class="language-plaintext highlighter-rouge">torch.nn.GRU</code></a>构建<strong>GRU</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GRU</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># 输入序列的特征维度
</span>    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="c1"># 隐藏层状态的特征维度
</span>    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># 循环层的数量，默认为1，用于实现Stacked RNN
</span>    <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># 是否使用偏差项b
</span>    <span class="n">batch_first</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 若设置为True，则输入尺寸应为[Batch Size, Sequence Length, Input Size]
</span>    <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># 设置神经元的dropout
</span>    <span class="n">bidirectional</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 设置双向RNN
</span>    <span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># [Sequence Length, Batch Size, Input Size]
</span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="c1"># [Bidirectional, Batch Size, Hidden Size]
</span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="nf">gru</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="3-准循环神经网络-qrnn">(3) 准循环神经网络 QRNN</h2>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/1611.01576">Quasi-Recurrent Neural Networks</a></li>
</ul>

<p>标准的循环神经网络需要循环地实现，即每次处理输入序列的一个<strong>token</strong>，无法并行化处理输入序列。这是因为在参数化处理输入序列时依赖于上一时刻的隐状态。<strong>准循环神经网络 (Quasi-Recurrent Neural Network, QRNN)</strong>通过把卷积层引入<strong>RNN</strong>，实现了输入序列的并行处理，同时输出结果依赖于序列顺序。</p>

<p><img src="https://pic.imgdb.cn/item/63b537f8be43e0d30eead471.jpg" alt="" /></p>

<p><strong>QRNN</strong>使用一维卷积处理输入序列，设置卷积核大小为$k$，则根据最近$k$时刻的输入$x_{t-k+1:t}$生成当前时刻的输入门$z_t$, 遗忘门$f_t$和输出门$o_t$。与<strong>LSTM</strong>不同，这一步不依赖于隐状态$h_{t-1}$，因此可以以矩阵方式并行地运算：</p>

\[\begin{aligned} z_t &amp;= \text{tanh}(W_{z}^{1}x_{t-k+1}+W_{z}^{2}x_{t-k+2}+\cdots + W_{z}^{k}x_{t}) \\ f_t &amp;= \sigma(W_{f}^{1}x_{t-k+1}+W_{f}^{2}x_{t-k+2}+\cdots + W_{f}^{k}x_{t}) \\ o_t &amp;= \sigma(W_{o}^{1}x_{t-k+1}+W_{o}^{2}x_{t-k+2}+\cdots + W_{o}^{k}x_{t}) \end{aligned}\]

<p>然后通过动态平均池化构造序列的输出(隐状态)，这一步是循环实现的，是一个无参数函数：</p>

\[\begin{aligned}  c_t &amp;= c_{t-1} \odot f_t + z_t \odot (1-f_t) \\ h_{t} &amp;= o_t \odot c_t \end{aligned}\]

<p>使用<strong>torchqrnn</strong>库构造<strong>QRNN</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchqrnn</span> <span class="kn">import</span> <span class="n">QRNN</span>

<span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">256</span>
<span class="n">qrnn</span> <span class="o">=</span> <span class="nc">QRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nf">qrnn</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="4-简单循环单元-sru">(4) 简单循环单元 SRU</h2>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/1709.02755">Simple Recurrent Units for Highly Parallelizable Recurrence</a></li>
</ul>

<p><strong>简单循环单元 (Simple Recurrent Unit, SRU)</strong>的设计思路与<strong>QRNN</strong>类似，通过把矩阵乘法放在串行循环之外，能够并行地处理输入序列，提升了运算速度。</p>

<p><strong>SRU</strong>中每个时间步的门控计算只依赖于当前时间步的输入，并在输出(隐状态)中添加了跳跃连接：</p>

\[\begin{aligned} \tilde{x}_t &amp;= Wx_t \\ f_t &amp;= \sigma(W_{f}x_{t}+b_f) \\ r_t &amp;= \sigma(W_{r}x_{t}+b_r) \\ c_t &amp;= c_{t-1} \odot f_t + \tilde{x}_t \odot (1-f_t) \\ h_{t} &amp;= r_t \odot g(c_t) + (1-r_t) \odot x_t \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63b53adabe43e0d30eefdcbf.jpg" alt="" /></p>

<h2 id="5-有序神经元lstm-on-lstm">(5) 有序神经元LSTM ON-LSTM</h2>

<ul>
  <li>paper：<a href="https://0809zheng.github.io/2021/01/22/onlstm.html"><font color="blue">Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks</font></a></li>
</ul>

<p><strong>ON-LSTM</strong>通过有序神经元(<strong>Ordered Neuron</strong>)把层级结构（树结构）整合到<strong>LSTM</strong>中，从而允许<strong>LSTM</strong>无监督地学习到层级结构信息（如句子的句法结构）。</p>

<p><strong>ON-LSTM</strong>假设神经元$c_t$已经排好序，$c_t$中索引值越小的元素表示越低层级的信息，而索引值越大的元素表示越高层级的信息；并引入了主遗忘门\(\tilde{f}_t\)和主输入门\(\tilde{i}_t\)，分别代表$c_{t-1}$中的历史信息层级$d_f$和\(\hat{c}_t\)中的输入信息层级$d_i$。</p>

<p>当前输入\(\hat{c}_t\)更容易影响低层信息，所以对神经元影响的索引范围是$[0,d_i]$；历史信息$c_{t-1}$保留的是高层信息，所以影响的范围是$[d_f,d_{\max}]$；在重叠部分$[d_f,d_i]$通过<strong>LSTM</strong>的形式更新记忆状态。</p>

\[\begin{aligned} i_t &amp;= \sigma(W_{i}x_t+U_{i}h_{t-1}+b_i) \\ f_t &amp;= \sigma(W_{f}x_t+U_{f}h_{t-1}+b_f) \\ o_t &amp;= \sigma(W_{o}x_t+U_{o}h_{t-1}+b_o) \\ \hat{c}_t &amp;= \text{tanh}(W_{c}x_t+U_{c}h_{t-1}+b_c) \\ \tilde{f}_t &amp;= \text{cumsum}(\text{softmax}(W_{\tilde{f}}x_t+U_{\tilde{f}}h_{t-1}+b_{\tilde{f}})) \\ \tilde{i}_t &amp;= 1- \text{cumsum}(\text{softmax}(W_{\tilde{i}}x_t+U_{\tilde{i}}h_{t-1}+b_{\tilde{i}})) \\ w_t &amp;= \tilde{f}_t \odot \tilde{i}_t \\ c_t &amp;= w_t\odot (f_t \odot c_{t-1} + i_t \odot \hat{c}_t) + (\tilde{f}_t-w_t) \cdot c_{t-1} + (\tilde{i}_t-w_t) \cdot \hat{c}_t \\ h_{t} &amp;= o_t \odot \text{tanh}(c_t) \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63b6749ebe43e0d30eb12cec.jpg" alt="" /></p>

<h1 id="5-深层rnn">5. 深层RNN</h1>
<p>深层<strong>RNN</strong>通过增加循环神经网络的深度（即堆叠循环层的数量）增强循环神经网络的特征提取能力，即增加同一时刻网络输入到输出之间的路径。</p>

<h2 id="1-堆叠循环神经网络-stacked-rnn">(1) 堆叠循环神经网络 Stacked RNN</h2>
<p><strong>堆叠循环神经网络（Stacked RNN）</strong>是将多个循环网络堆叠起来。</p>

<p><img src="https://pic.downk.cc/item/5ea1325ac2a9a83be5af5547.jpg" alt="" /></p>

<h2 id="2-双向循环神经网络-bidirectional-rnn">(2) 双向循环神经网络 Bidirectional RNN</h2>
<p><strong>双向循环神经网络（Bidirectional RNN）</strong>由两层循环神经网络组成，它们的输入相同，只是信息传递的方向不同。</p>

<p><img src="https://pic.downk.cc/item/5ea132bbc2a9a83be5afa1c3.jpg" alt="" /></p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/03/08/recursive-neural-network.html" class="read-next-link"></a>
        <section>
          <span>递归神经网络(Recursive Neural Network)</span>
          <p>  Recursive Neural Networks.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.downk.cc/item/5ea14499c2a9a83be5c09f98.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/03/06/CNN.html" class="read-next-link"></a>
          <section>
            <span>卷积神经网络(Convolutional Neural Network)</span>
            <p>  Convolutional Neural Networks.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/63ac493e08b6830163362152.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
