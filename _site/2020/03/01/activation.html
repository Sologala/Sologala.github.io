<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习中的激活函数(Activation Function) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="深度学习中的激活函数(Activation Function)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="深度学习中的激活函数(Activation Function) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/03/01/activation.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>深度学习中的激活函数(Activation Function)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-03-01"><i class="iconfont icon-date"></i>01 Mar 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/5e7b4db6504f4bcb040071f1.png') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Activation Functions in Deep Learning.</p>
</blockquote>

<p>本文目录：</p>
<ol>
  <li>激活函数的意义</li>
  <li>激活函数应具有的性质</li>
  <li>一些常用的激活函数</li>
</ol>

<h1 id="1-激活函数的意义">1. 激活函数的意义</h1>

<h2 id="1-从生物学的角度理解激活函数">(1) 从生物学的角度理解激活函数</h2>
<p>早期激活函数的设计受到生物神经网络中<a href="http://cs231n.github.io/neural-networks-1/">神经元</a>的启发，即对神经元进行简单的建模。</p>

<p><img src="https://pic.imgdb.cn/item/5e7b4db6504f4bcb040071f1.png" alt="" /></p>

<p>大脑中的<strong>神经元(neuron)</strong>通过<strong>树突(dendrites)</strong>接收其他神经元的输入信号，在胞体中进行信号的处理，通过<strong>轴突(axon)</strong>分发信号。当神经元中的信号累积达到一定阈值时产生电脉冲将信号输出，这个阈值称为<strong>点火率(firing rate)</strong>。</p>
<ul>
  <li>其他神经元的输入信号建模为$x_i$</li>
  <li>树突的信号接收过程建模为$w_ix_i$</li>
  <li>胞体的信号处理过程建模为$\sum_{i}^{}w_ix_i$</li>
  <li>点火率建模为$-b$</li>
  <li>信号累计与阈值的比较建模为$\sum_{i}^{}w_ix_i+b$</li>
  <li>产生电脉冲建模为$f(\cdot)=\text{step}(\cdot)$</li>
  <li>轴突的输出信号建模为$f(\sum_{i}^{}w_ix_i+b)$</li>
</ul>

<p>激活函数是用来模拟“信号累积达到阈值并产生电脉冲”的过程。
值得一提的是，这种对神经元的建模是<strong>coarse</strong>的。真实神经元有很多不同的种类；突触是一个复杂的的非线性动态系统，树突进行的是复杂的非线性运算，轴突的输出时刻也很重要。因此近些年来神经网络中神经元的<strong>生物可解释性(Biological Plausibility)</strong>被逐渐弱化。</p>

<h2 id="2-从非线性的角度理解激活函数">(2) 从非线性的角度理解激活函数</h2>

<p>在神经网络中，使用<strong>激活函数(activation function)</strong>能够为网络引入非线性，增强网络的非线性表示能力；当不使用激活函数时（或激活函数为<strong>恒等函数 identity function</strong>），多层神经网络退化为单层网络：</p>

\[W_2(W_1X+b_1)+b_2\\=W_2W_1X+W_2b_1+b_2\\=(W_2W_1)X+(W_2b_1+b_2)\\=W'x+b'\]

<h1 id="2-激活函数应具有的性质">2. 激活函数应具有的性质</h1>
<p>激活函数能为神经网络引入非线性，因此理论上任何非线性函数都可以作为激活函数。在选择激活函数时应考虑以下性质：</p>

<h3 id="-性质1连续可导">⚪ 性质1：连续可导</h3>
<p>激活函数需要参与反向传播过程，因此需要计算激活函数的导数，这就要求激活函数需要<strong>连续可导</strong>。</p>

<p>例如<strong>ReLU</strong>族激活函数在$x=0$处不可导。既可以人工指定该点处的梯度；又可以选用形状接近的连续函数进行近似(如<strong>softplus</strong>替代<strong>ReLU</strong>, <strong>CELU</strong>替代<strong>ELU</strong>)。</p>

<p>更多关于不可导函数的光滑化的相关内容可参考<a href="https://0809zheng.github.io/2021/11/16/mollifier.html">博客</a>。</p>

<h3 id="-性质2计算量小">⚪ 性质2：计算量小</h3>
<p>激活函数应具有尽可能小的<strong>计算量</strong>，通常线性运算(如<strong>ReLU</strong>族)比指数运算(如<strong>S</strong>型曲线)具有更低的计算量。</p>

<p>通常可以对指数函数进行<a href="https://0809zheng.github.io/2021/08/20/taylor.html#3-%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E7%9A%84%E5%BA%94%E7%94%A8hard-sigmoid%E4%B8%8Ehard-tanh">Taylor展开</a>(如<strong>HardSigmoid</strong>,<strong>HardTanh</strong>)降低激活函数的计算复杂度。</p>

<h3 id="-性质3没有饱和区">⚪ 性质3：没有饱和区</h3>
<p><strong>饱和</strong>的定义是导数很接近$0$。若激活函数存在饱和区，则会使反向传播的梯度为$0$，从而导致<strong>梯度消失(gradient vanishing)</strong>现象。</p>

<p>早期的激活函数通常使用<strong>S</strong>型函数，如<strong>Sigmoid,Tanh</strong>；这类函数会把输出挤压到一个区域内，导致产生饱和区；这类函数也被称为<strong>squashing function</strong>。</p>

<p><strong>ReLU</strong>等无上界、有下界的激活函数，在正半轴没有饱和区，减缓了梯度消失现象；在负半轴则会置零(产生<strong>died ReLU</strong>现象, 即由于梯度为$0$阻断了反向传播过程)或趋于饱和。</p>

<h3 id="-性质4没有偏置偏移">⚪ 性质4：没有偏置偏移</h3>
<p>若激活函数的输出不是<strong>zero-centered</strong>的，会使得后一层神经元的输入产生<strong>偏置偏移(bias shift)</strong>，从而减慢梯度下降的收敛速度。</p>

<p>对于某一层神经元的计算，假设具有两个参数$w_1,w_2$，则$y=\sigma(w_1x_1+w_2x_2+b)$，反向传播时两个参数$w_1,w_2$的梯度为：</p>

\[\nabla_{w_1}L=\frac{\partial L}{\partial {w_1}} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial {w_1}} = \frac{\partial L}{\partial y}\cdot \sigma' \cdot x_1 = \nabla_yL \cdot \sigma' \cdot x_1\]

\[\nabla_{w_2}L= \nabla_yL \cdot \sigma' \cdot x_2\]

<p>若上一层的激活函数使得该层神经元的输入值大于$0$，则$\text{sign}(\nabla_{w_1}L)=\text{sign}(\nabla_{w_2}L)$，则梯度只能沿着$w_1,w_2$同时增大或减小的方向进行更新，从而减慢梯度下降的收敛速度。</p>

<p>当激活函数的值域同时包含正和负值的输出，则能够有效缓解偏置偏移现象。</p>

<h3 id="-性质5具有生物可解释性">⚪ 性质5：具有生物可解释性</h3>
<p>生物神经元通常具有<strong>单侧抑制</strong>(即大于阈值才会被激活)、<strong>宽兴奋边界</strong>(即输出范围较宽,如$[0,+∞)$)、<strong>稀疏激活</strong>(即同时被激活的神经元较少)等特性。</p>

<p><strong>ReLU</strong>及之前的激活函数在设计时受到生物学的启发，而其后的激活函数在设计时逐渐淡化了生物可解释性。</p>

<h3 id="-性质6提取上下文信息">⚪ 性质6：提取上下文信息</h3>
<p>通常的激活函数是标量函数，如<strong>ReLU</strong>对神经元输入的每一个标量值分别进行计算。如果能够将激活函数拓展为多输入函数，则能够捕捉输入的上下文信息，增强神经元的表达能力。</p>

<p>某个特征位置的上下文信息既可以从所有输入特征中获取(如<strong>maxout</strong>，<strong>Dynamic ReLU</strong>)，也可以由在该特征的一个邻域上获取(如<strong>Dynamic Shift-Max</strong>，<strong>FReLU</strong>)。</p>

<h3 id="-性质7具有通用近似性">⚪ 性质7：具有通用近似性</h3>
<p>直观上神经网络每一层的每个神经元都应具有不同的激活曲线。可以设计一些由超参数控制的通用近似激活函数，使得每个神经元学习不同的激活曲线。每个神经元的激活超参数参与反向传播的梯度更新。</p>

<p>设计通用近似的激活函数主要有两种思路。第一种是使用一些通用的函数逼近方法，如分段线性近似(<strong>APL, PWLU</strong>)、<strong>Padé</strong>近似(<strong>PAU, OPAU</strong>)。第二种是寻找现有激活函数的光滑逼近，如手工设计近似(<strong>ACON, SMU</strong>)、使用<strong>Dirac</strong>函数寻找光滑近似(<strong>SAU</strong>)。</p>

<h1 id="3-一些常用的激活函数">3. 一些常用的激活函数</h1>
<ul>
  <li>Reference：<a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">Pytorch中的激活函数层</a></li>
</ul>

<p>下面介绍的激活函数根据设计思路也可分类如下：</p>
<ul>
  <li><strong>S</strong>型激活函数：形如<strong>S</strong>型曲线的激活函数。包括<strong>Step</strong>，<strong>Sigmoid</strong>，<strong>HardSigmoid</strong>，<strong>Tanh</strong>，<strong>HardTanh</strong></li>
  <li><strong>ReLU</strong>族激活函数：形如<strong>ReLU</strong>的激活函数。包括<strong>ReLU</strong>，<strong>Softplus</strong>，<strong>ReLU6</strong>，<strong>LeakyReLU</strong>，<strong>PReLU</strong>，<strong>RReLU</strong>，<strong>ELU</strong>，<strong>GELU</strong>，<strong>CELU</strong>，<strong>SELU</strong></li>
  <li>自动搜索激活函数：通过自动搜索解空间得到的激活函数。包括<strong>Swish</strong>，<strong>HardSwish</strong>，<strong>Elish</strong>，<strong>HardElish</strong>，<strong>Mish</strong></li>
  <li>基于梯度的激活函数：通过梯度下降为每个神经元学习独立函数。包括<strong>APL</strong>，<strong>PAU</strong>，<strong>ACON</strong>，<strong>PWLU</strong>，<strong>OPAU</strong>，<strong>SAU</strong>，<strong>SMU</strong></li>
  <li>基于上下文的激活函数：多输入单输出函数，输入上下文信息。包括<strong>maxout</strong>，<strong>Dynamic ReLU</strong>，<strong>Dynamic Shift-Max</strong>，<strong>FReLU</strong></li>
</ul>

<style>
table th:first-of-type {
    width: 20%;
}
table th:nth-of-type(2) {
    width: 40%;
}
table th:nth-of-type(3) {
    width: 40%;
}
</style>

<table>
  <thead>
    <tr>
      <th>激活函数</th>
      <th>表达式</th>
      <th>函数图像</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Step</td>
      <td>\(\begin{cases} 1, &amp; x≥0 \\ 0, &amp;x&lt;0 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962ecd2ab3f51d913852ce.png" alt="" /></td>
    </tr>
    <tr>
      <td>Sigmoid</td>
      <td>\(\frac{1}{1+e^{-x}}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e8f2ab3f51d913837b8.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/20/taylor.html#3-%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E7%9A%84%E5%BA%94%E7%94%A8hard-sigmoid%E4%B8%8Ehard-tanh"><font color="Blue">Hardsigmoid</font></a>:降低Sigmoid计算量</td>
      <td>\(\begin{cases} 1, &amp; x≥1 \\ (x+1)/2, &amp; -1&lt;x&lt;1 \\ 0, &amp;x≤-1 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e462ab3f51d91380e56.png" alt="" /></td>
    </tr>
    <tr>
      <td>Tanh</td>
      <td>\(2\text{Sigmoid}(2x)-1\\=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962ecd2ab3f51d913852e3.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/20/taylor.html#3-%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E7%9A%84%E5%BA%94%E7%94%A8hard-sigmoid%E4%B8%8Ehard-tanh"><font color="Blue">Hardtanh</font></a>:降低Tanh计算量</td>
      <td>\(\begin{cases} 1, &amp; x&gt;1 \\ x, &amp; -1≤x≤1 \\ -1, &amp;x&lt;-1 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e462ab3f51d91380e5e.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://www.researchgate.net/publication/4933639_Incorporating_Second-Order_Functional_Knowledge_for_Better_Option_Pricing">Softplus</a>:连续形式的ReLU</td>
      <td>\(\int_{}^{}\text{Sigmoid}(x)dx \\=\ln(1+e^x)\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962ecd2ab3f51d913852c7.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf">ReLU</a></td>
      <td>\(\max(x,0) \\=\begin{cases} x, &amp; x≥0 \\ 0, &amp;x&lt;0 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e8f2ab3f51d913837b0.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/13/mobilenetv1.html"><font color="Blue">ReLU6</font></a>:部署移动端</td>
      <td>\(\min(\max(x,0),6) \\=\begin{cases} 6, &amp; x\geq 6 \\ x, &amp; 0\leq x&lt;6 \\ 0, &amp;x&lt;0 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619630e92ab3f51d91394de8.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/10/23/maxout.html"><font color="Blue">Maxout</font></a>：分段线性单元</td>
      <td>\(\mathop{\max}_{j\in [1,k]}x^TW_{i,j}+b_{ij}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619785362ab3f51d91ee819c.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/29/lrelu.html"><font color="Blue">LeakyReLU</font></a>:解决Dead ReLU</td>
      <td>\(\max(x,0.01x) \\=\begin{cases} x, &amp; x≥0 \\ 0.01x, &amp;x&lt;0 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e8f2ab3f51d913837a5.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/10/26/apl.html"><font color="Blue">APL</font></a>:通过ReLU构造分段线性</td>
      <td>\(\max(0,x)\\+\sum_{s=1}^{S}a^s\max (0,-x+b^s)\)</td>
      <td><img src="https://pic.imgdb.cn/item/619618512ab3f51d912c24a3.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/30/prelu.html"><font color="Blue">PReLU</font></a>:可学习参数$\alpha$</td>
      <td>\(\max(x,\alpha x) \\=\begin{cases} x, &amp; x≥0 \\ \alpha x, &amp;x&lt;0 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962f902ab3f51d9138b61c.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/31/rrelu.html"><font color="Blue">RReLU</font></a>:均匀分布采样$\alpha$</td>
      <td>\(\max(x,\alpha x) \\=\begin{cases} x, &amp; x≥0 \\ \alpha x, &amp;x&lt;0 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619631e82ab3f51d9139c736.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/25/elu.html"><font color="Blue">ELU</font></a>:解决bias shift</td>
      <td>\(\begin{cases}x,  &amp; x≥0 \\α(e^x-1), &amp; x&lt;0\end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962fcd2ab3f51d9138cec7.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/24/gelu.html"><font color="Blue">GELU</font></a>:引入正则化</td>
      <td>\(x\Phi(x)=x\int_{-∞}^{x} \frac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}}dt \\  = x\cdot \frac{1}{2}(1+\text{erf}(\frac{x}{\sqrt{2}}))\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e462ab3f51d91380e52.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/22/celu.html"><font color="Blue">CELU</font></a>:连续可微的ELU</td>
      <td>\(\begin{cases}x,  &amp; x≥0 \\α(e^{\frac{x}{\alpha}}-1), &amp; x&lt;0\end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619631b52ab3f51d9139acb4.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/02/selu.html"><font color="Blue">SELU</font></a>:自标准化的ELU</td>
      <td>\(\begin{cases}\lambda x,  &amp; x≥0 \\\lambda α(e^x-1), &amp; x&lt;0\end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/6196301d2ab3f51d9138f58d.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/04/swish.html"><font color="Blue">Swish</font></a>:自动搜索</td>
      <td>\(x\cdot \text{Sigmoid}(\beta x) \\ = \frac{x}{1+e^{-\beta x}}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962ecd2ab3f51d913852d6.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/15/mobilenetv3.html"><font color="Blue">HardSwish</font></a>:降低Swish计算量</td>
      <td>\(x \cdot \frac{\text{ReLU6}(x+3)}{6} \\ = \begin{cases} x , &amp; x \geq 3 \\ \frac{x(x+3)}{6} , &amp; -3 \leq x &lt;3 \\ 0, &amp; x &lt; -3 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/6196309c2ab3f51d91392d93.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/03/elish.html"><font color="Blue">ELiSH</font></a>:遗传算法</td>
      <td>\(\text{Sigmoid}(x) \cdot \text{ELU}(x) \\= \begin{cases}\frac{x}{1+e^{-x}},  &amp; x≥0 \\\frac{e^x-1}{1+e^{-x}}, &amp; x&lt;0\end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e462ab3f51d91380e4d.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/03/elish.html"><font color="Blue">HardELiSH</font></a>:降低ELiSH计算量</td>
      <td>\(\text{HardSigmoid}(x) \cdot \text{ELU}(x) \\= \begin{cases} x, &amp; x≥1 \\ x(x+1)/2, &amp; 0 \leq x&lt;1 \\ (e^x-1)(x+1)/2, &amp; -1\leq x&lt;0 \\ 0, &amp;x≤-1 \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619632bf2ab3f51d913a2484.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/10/24/pade.html"><font color="Blue">PAU</font></a>:Padé近似</td>
      <td>\(\frac{a_0+a_1x+a_2x^2+...+a_mx^m}{1+|b_1||x|+|b_2||x|^2+...+|b_n||x|^n}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619618fc2ab3f51d912c898d.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/21/mish.html"><font color="Blue">Mish</font></a>:进一步搜索Swish</td>
      <td>\(x\cdot \text{tanh}(\text{softplus}(x)) \\ =x\cdot \text{tanh}(\ln(1+e^x))\)</td>
      <td><img src="https://pic.imgdb.cn/item/61962e8f2ab3f51d913837a8.png" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/10/27/dyrelu.html"><font color="Blue">Dynamic ReLU</font></a>：动态ReLU</td>
      <td>\(\mathop{\max}_{1\leq k \leq K} \{a_c^k(x)x_c+b_c^k(x)\}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619707a22ab3f51d919c2561.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/09/micronet.html"><font color="Blue">Dynamic Shift-Max</font></a>:循环移位多输入</td>
      <td>\(\mathcal{\max}_{1\leq k\leq K} \{\sum_{j=0}^{J-1} a_{i,j}^k(x)x_{\frac{C}{G}}(i,j)\}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619393ea2ab3f51d919f26bd.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2020/09/05/frelu.html"><font color="Blue">FReLU</font></a>:卷积窗口输入</td>
      <td>\(\mathcal{\max}_{1\leq k\leq K} \{\sum_{j=0}^{J-1} a_{i,j}^k(x)x_{\frac{C}{G}}(i,j)\}\)</td>
      <td><img src="https://pic.imgdb.cn/item/619632ec2ab3f51d913a3a3a.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/18/acon.html"><font color="Blue">ACON</font></a>:最大值函数的$\alpha$-<strong>softmax</strong>近似</td>
      <td>\((p_1-p_2)x\sigma(\beta (p_1-p_2)x)+p_2x\)</td>
      <td><img src="https://pic.imgdb.cn/item/619616512ab3f51d912aeaa5.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2020/10/22/plu.html"><font color="Blue">PWLU</font></a>:分段线性近似</td>
      <td>\(\begin{cases}  (x-B_L)*K_L+Y_P^0, &amp; x&lt;B_L \\ (x-B_R)*K_R+Y_P^N, &amp; x\geq B_R \\ (x-B_{idx})*K_{idx}+Y_P^{idx}, &amp; \text{others} \end{cases}\)</td>
      <td><img src="https://pic.imgdb.cn/item/6196180a2ab3f51d912bffcc.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/10/25/opade.html"><font color="Blue">OPAU</font></a>:正交Padé近似</td>
      <td>\(\frac{c_0+c_1f_1(x)+c_2f_2(x)+...+c_kf_k(x)}{1+|d_1||f_1(x)|+|d_2||f_2(x)|+...+|d_l||f_l(x)|}\)</td>
      <td>见<strong>PAU</strong></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/05/sau.html"><font color="Blue">SAU</font></a>: 使用Dirac函数近似</td>
      <td>\(\frac{(1-\alpha)\sigma}{\sqrt{2\pi}}  e^{-\frac{x^2}{2\sigma^2}}+  \frac{ x}{2} +  \frac{(1-\alpha) x}{2}\text{erf}(\frac{x}{\sqrt{2}\sigma})\)</td>
      <td><img src="https://pic.imgdb.cn/item/61938c232ab3f51d919b76d7.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/17/smu.html"><font color="Blue">SMU</font></a>: 最大值函数的光滑近似</td>
      <td>\(\frac{(1+\alpha)x+(1-\alpha)x \text{erf}(\mu (1-\alpha)x)}{2}\)</td>
      <td><img src="https://pic.imgdb.cn/item/6195c47d2ab3f51d91f255d6.jpg" alt="" /></td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/19/squareplus.html"><font color="Blue">Squareplus</font></a>: Softplus的代数近似</td>
      <td>\(\text{squareplus}(x,b) =\frac{1}{2}(x+\sqrt{x^2+b})\)</td>
      <td><img src="https://pic.imgdb.cn/item/61ec097b2ab3f51d9130a921.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<h1 id="-参考文献">⚪ 参考文献</h1>
<ul>
  <li><a href="https://arxiv.org/abs/1811.03378">Activation Functions: Comparison of trends in Practice and Research for Deep Learning</a>：(arXiv1811)一篇激活函数综述。</li>
  <li><a href="https://0809zheng.github.io/2021/08/29/lrelu.html"><font color="Blue">Rectifier Nonlinearities Improve Neural Network Acoustic Models</font></a>：(ICML2013)LeakyReLU：使用修正的非线性提高神经网络声学模型。</li>
  <li><a href="https://0809zheng.github.io/2021/10/23/maxout.html"><font color="Blue">Maxout Networks</font></a>：(arXiv1302)Maxout：自适应分段线性单元。</li>
  <li><a href="https://0809zheng.github.io/2021/10/26/apl.html"><font color="Blue">Learning Activation Functions to Improve Deep Neural Networks</font></a>：(arXiv1412)APL：自适应分段线性单元。</li>
  <li><a href="https://0809zheng.github.io/2021/08/30/prelu.html"><font color="Blue">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</font></a>：(arXiv1502)PReLU：分类任务超越人类表现。</li>
  <li><a href="https://0809zheng.github.io/2021/08/31/rrelu.html"><font color="Blue">Empirical Evaluation of Rectified Activations in Convolutional Network</font></a>：(arXiv1505)RReLU：受限激活函数的经验验证。</li>
  <li><a href="https://0809zheng.github.io/2021/08/25/elu.html"><font color="Blue">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</font></a>：(arXiv1511)ELU：消除偏差偏置的指数线性单元。</li>
  <li><a href="https://0809zheng.github.io/2021/08/24/gelu.html"><font color="Blue">Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units</font></a>：(arXiv1606)GELU：随机正则化的高斯误差线性单元。</li>
  <li><a href="https://0809zheng.github.io/2021/08/22/celu.html"><font color="Blue">Continuously Differentiable Exponential Linear Units</font></a>：(arXiv1704)CELU：连续可微的指数线性单元。</li>
  <li><a href="https://0809zheng.github.io/2021/09/02/selu.html"><font color="Blue">Self-Normalizing Neural Networks</font></a>：(arXiv1706)SELU：自标准化的指数线性单元。</li>
  <li><a href="https://0809zheng.github.io/2021/09/04/swish.html"><font color="Blue">Searching for Activation Functions</font></a>：(arXiv1710)Swish：自动搜索得到的一种自门控的激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/09/03/elish.html"><font color="Blue">The Quest for the Golden Activation Function</font></a>：(arXiv1808)ELiSH：使用遗传算法寻找最优激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/10/24/pade.html"><font color="Blue">Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks</font></a>：(arXiv1907)PAU：基于Padé近似的可学习激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/08/21/mish.html"><font color="Blue">Mish: A Self Regularized Non-Monotonic Activation Function</font></a>：(arXiv1908)Mish：一种自正则化的非单调激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/10/27/dyrelu.html"><font color="Blue">Dynamic ReLU</font></a>：(arXiv2003)DY-ReLU：动态整流线性单元。</li>
  <li><a href="https://0809zheng.github.io/2020/09/05/frelu.html"><font color="Blue">Funnel Activation for Visual Recognition</font></a>：(arXiv2007)FReLU：为视觉任务设计的激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/11/18/acon.html"><font color="Blue">Activate or Not: Learning Customized Activation</font></a>：(arXiv2009)ACON：学习自定义的激活函数。</li>
  <li><a href="https://0809zheng.github.io/2020/10/22/plu.html"><font color="Blue">Learning specialized activation functions with the Piecewise Linear Unit</font></a>：(arXiv2104)PWLU：使用分段线性单元学习激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/10/25/opade.html"><font color="Blue">Orthogonal-Padé Activation Functions: Trainable Activation functions for smooth and faster convergence in deep networks</font></a>：(arXiv2106)OPAU：基于正交Padé近似的可训练激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/11/05/sau.html"><font color="Blue">SAU: Smooth activation function using convolution with approximate identities</font></a>：(arXiv2109)SAU：使用Dirac函数构造激活函数的光滑近似。</li>
  <li><a href="https://0809zheng.github.io/2021/11/17/smu.html"><font color="Blue">SMU: smooth activation function for deep networks using smoothing maximum technique</font></a>：(arXiv2111)SMU：基于光滑最大值技术的光滑激活函数。</li>
  <li><a href="https://0809zheng.github.io/2021/11/19/squareplus.html"><font color="Blue">Squareplus: A Softplus-Like Algebraic Rectifier</font></a>：(arXiv2112)Squareplus：一种类似softplus的代数整流器。</li>
</ul>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/03/02/optimization.html" class="read-next-link"></a>
        <section>
          <span>深度学习中的优化算法(Optimization)</span>
          <p>  Optimization in Deep Learning.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.downk.cc/item/5e7ee115504f4bcb04298afb.png" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/02/29/data-structure-python.html" class="read-next-link"></a>
          <section>
            <span>数据结构与算法(Python)</span>
            <p>  Data Structure and Algorithm with Python.数据结构(Data Stru...</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5e89be33504f4bcb040382b2.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
