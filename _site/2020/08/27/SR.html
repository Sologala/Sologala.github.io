<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>图像超分辨率(Super Resolution) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="图像超分辨率(Super Resolution)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="图像超分辨率(Super Resolution) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/08/27/SR.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>图像超分辨率(Super Resolution)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-08-27"><i class="iconfont icon-date"></i>27 Aug 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5f47223b160a154a672d2333.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Image Super Resolution.</p>
</blockquote>

<p><strong>本文目录：</strong></p>
<ol>
  <li>问题阐述</li>
  <li>降级模型</li>
  <li>传统方法：基于插值的方法、基于重建的方法、机器学习方法</li>
  <li>深度学习方法：预定义上采样、单次上采样、渐进上采样、循环采样</li>
  <li>评估指标：峰值信噪比<strong>PSNR</strong>、结构相似度<strong>SSIM</strong></li>
  <li><strong>Benchmarks</strong></li>
</ol>

<h1 id="1-问题阐述">1. 问题阐述</h1>
<p><strong>图像超分辨率（Super Resolution，SR）</strong>技术是指通过技术手段对图像的分辨率进行放大，从而得到更清晰的图像。</p>

<p>将超分辨率问题看作监督学习任务，将<strong>高分辨率(High Resolution, HR)</strong>图像作为标签$I_y$；将对应的<strong>低分辨率(Low Resolution, LR)</strong>图像作为输入$I_x$。</p>

<p>通过训练模型，学习从<strong>LR</strong>到<strong>HR</strong>的映射$F(\cdot)$。记模型参数为$θ$，则通过模型得到超分辨率图像$\hat{I}_y$：</p>

\[\hat{I}_y = F(I_x;θ)\]

<p>若记损失函数为$L(\cdot)$，正则化函数为$\Phi(\cdot)$，模型的学习目标函数可以表示为：</p>

\[\hat{θ} = \mathop{\arg \max}_{θ} L(\hat{I}_y,I_y) + λΦ(θ)\]

<h1 id="2-降级模型">2. 降级模型</h1>

<p>通常给定<strong>HR</strong>图像，通过技术手段将其缩小为<strong>LR</strong>图像，这个过程称为<strong>降级(Degradation)</strong>。</p>

<p>记降级模型为$D$，其模型参数为$δ$，则这一过程表示为：</p>

\[I_x = D(I_y;δ)\]

<p>在实际问题中，对图像降级往往会引入压缩失真、散焦、传感器噪声等问题，因此会向图像中加入一些模拟噪声。</p>

<p><img src="https://pic.downk.cc/item/5f474976160a154a674c9f8e.jpg" alt="" /></p>

<p>常用的降级模型包括：</p>
<ul>
  <li>简单的下采样：$↓$表示下采样操作，$S$表示下采样倍数</li>
</ul>

\[I_x = (I_y)↓_S\]

<ul>
  <li>加入模糊和噪声的下采样：$\otimes$表示卷积，$k$表示卷积核，$↓$表示下采样操作，$S$表示下采样倍数，$n$表示噪声</li>
</ul>

\[I_x = (I_y \otimes k)↓_S + n\]

<h1 id="3-传统方法">3. 传统方法</h1>
<p>在深度学习时代之前，图像超分辨率重建技术主要分为三类：</p>
<ol>
  <li>基于插值的方法：最邻近插值、双线性插值、双三次插值</li>
  <li>基于重建的方法</li>
  <li>机器学习的方法</li>
</ol>

<h2 id="1基于插值的方法">（1）基于插值的方法</h2>
<p>以放大$2$倍为例，将低分辨率图像插值为对应的高分辨率图像：</p>

<p><img src="https://pic.downk.cc/item/5f47591f160a154a6750f55a.jpg" alt="" /></p>

<h3 id="最近邻插值-nearest-neighbor">⚪最近邻插值 Nearest Neighbor</h3>
<p><strong>最近邻插值</strong>是指高分辨率图像的像素点选择与其坐标位置最近的低分辨率图像像素值，其几何解释如下：</p>

<p><img src="https://pic.downk.cc/item/5f475954160a154a67510336.jpg" alt="" /></p>

<p>如上图<strong>HR</strong>中位置$H5W6$的像素点对应<strong>LR</strong>中位置$H3W3$像素点的右上区域，则该点对应的像素按照最近邻的原则应该取$1$。</p>

<h3 id="双线性插值-bilinear-interpolation">⚪双线性插值 Bilinear Interpolation</h3>
<p><strong>双线性插值</strong>是指在两个方向分别进行一次线性插值。高分辨率图像的像素点选择与其坐标位置最近的$4$个低分辨率图像像素点的像素值的距离加权平均，其几何解释如下：</p>

<p><img src="https://pic.downk.cc/item/5f475966160a154a675108c4.jpg" alt="" /></p>

<p>如上图<strong>HR</strong>中位置$H5W6$的像素点对应<strong>LR</strong>中位置$H3W3$像素点的右上区域。</p>

<p>距离该点最近的四个<strong>LR</strong>中像素点位置分别是$H2W3$、$H2W4$、$H3W3$、$H3W4$。</p>

<p>双线性插值的计算过程如下：</p>

<p><img src="https://pic.imgdb.cn/item/62d9fdb2f54cd3f937b88324.jpg" alt="" /></p>

<p>双线性插值的平滑作用可能使得图像的细节产生退化，这种现象在进行图像放大时尤其明显。</p>

<h3 id="双三次插值-bicubic-interpolation">⚪双三次插值 Bicubic Interpolation</h3>
<p><strong>双三次插值</strong>能创造出比<strong>双线性插值</strong>更平滑的图像边缘。</p>

<p>在二维空间中，函数$f$在点$(x,y)$的值可以通过矩形网格中最近的$16$个采样点的加权平均得到：</p>

<p><img src="https://pic.imgdb.cn/item/62d9fe0df54cd3f937bab06b.jpg" alt="" /></p>

<p><img src="https://pic.downk.cc/item/5f4762a2160a154a6753acd7.jpg" alt="" /></p>

<p>其中$W$为插值函数，根据<strong>LR</strong>像素点到<strong>SR</strong>像素点之间的水平距离或垂直距离赋予其不同的权重。</p>

<p><img src="https://pic.downk.cc/item/5f4762f6160a154a6753cd13.jpg" alt="" /></p>

<p>使用<code class="language-plaintext highlighter-rouge">python</code>实现双三次插值：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">).</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">new_size</span><span class="p">,</span> <span class="n">resample</span><span class="o">=</span><span class="n">Image</span><span class="p">.</span><span class="n">BICUBIC</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="2基于重建的方法">（2）基于重建的方法</h2>

<p><img src="https://pic.downk.cc/item/5f474aac160a154a674d2057.jpg" alt="" /></p>

<h2 id="3机器学习的方法">（3）机器学习的方法</h2>

<p><img src="https://pic.downk.cc/item/5f474ac3160a154a674d28f7.jpg" alt="" /></p>

<h1 id="4-深度学习方法">4. 深度学习方法</h1>
<p>使用卷积神经网络进行超分辨率任务的模型根据<strong>上采样的位置</strong>不同可以划分成：</p>

<p><img src="https://pic.downk.cc/item/5f43c9ae160a154a6741a280.jpg" alt="" /></p>

<ol>
  <li><strong>预定义上采样(Predefined upsampling)</strong>：首先对图像应用预定义的插值方法进行上采样（如<strong>Bicubic</strong>），再通过卷积网络增加细节，如<a href="https://0809zheng.github.io/2020/08/03/srcnn.html"><font color="blue">SRCNN</font></a>, <a href="https://0809zheng.github.io/2020/08/05/vdsr.html"><font color="blue">VDSR</font></a>。</li>
  <li><strong>单次上采样(Single upsampling)</strong>：先通过卷积网络提取丰富的特征，再通过单次上采样(预定义或可学习)增加分辨率，如<a href="https://0809zheng.github.io/2020/08/04/fsrcnn.html"><font color="blue">FSRCNN</font></a>, <a href="https://0809zheng.github.io/2020/08/11/pixelshuffle.html"><font color="blue">ESPCN</font></a>, <a href="https://0809zheng.github.io/2020/08/06/edsr.html"><font color="blue">EDSR</font></a>, <a href="https://0809zheng.github.io/2020/08/01/rcan.html"><font color="blue">RCAN</font></a>, <a href="https://0809zheng.github.io/2020/08/09/san.html"><font color="blue">SAN</font></a>。</li>
  <li><strong>渐进上采样(Progressive upsampling)</strong>：通过多次上采样逐渐增加分辨率，如<a href="https://0809zheng.github.io/2020/09/07/lapsrn.html"><font color="blue">LapSRN</font></a>。</li>
  <li><strong>循环采样(Iterative up and downsampling)</strong>：循环地进行上采样和下采样，增加丰富的特征信息，如<a href="https://0809zheng.github.io/2020/08/02/dbpn.html"><font color="blue">DBPN</font></a>, <a href="https://0809zheng.github.io/2020/08/17/drn.html"><font color="blue">DRN</font></a>。</li>
  <li>其他结构：如<a href="https://0809zheng.github.io/2020/08/10/srresnet.html"><font color="blue">SRGAN</font></a>, <a href="https://0809zheng.github.io/2020/08/12/esrgan.html"><font color="blue">ESRGAN</font></a>引入生成对抗网络；<a href="https://0809zheng.github.io/2020/12/22/liif.html"><font color="blue">LIIF</font></a>学习二维图像的连续表达形式。</li>
</ol>

<p><strong>SISR</strong>问题本质是一个上采样问题。预定义上采样的方法大多是基于插值的，即首先把<strong>LR</strong>图像通过双三次插值调整为和<strong>HR</strong>图像具有相同尺寸的中间图像，再通过卷积学习中间图像和<strong>HR</strong>图像的残差。这类基于插值的方法其实存在许多问题，最重要的是它间接增加了卷积神经网络的学习难度。</p>

<p>我们把一张原始<strong>HR</strong>图像和它对应的<strong>LR</strong>图像经过插值后的中间图像展示在下图中。人眼很难区分这两张图像的细微差别。对这两张图像做差后得到残差图像，这是卷积网络要学习的目标。通过离散傅里叶变换将其转变到频率域中，发现残差图像对应的频谱更复杂，这意味着它比原始图像具有更多高频部分，这对卷积网络的学习是非常困难的。</p>

<p><img src="https://pic.imgdb.cn/item/64a269f61ddac507cca21948.jpg" alt="" /></p>

<p>最近<strong>SOTA</strong>的<strong>SISR</strong>方法大多采用基于<strong>学习</strong>的上采样方法。即采用可学习的插值（如转置卷积或<strong>pixel shuffle</strong>）代替传统的插值，获得更好的效果。其中转置卷积使用不当会出现<a href="https://0809zheng.github.io/2020/03/06/CNN.html#-%E6%A3%8B%E7%9B%98%E6%95%88%E5%BA%94">棋盘效应</a>，不适合对于图像质量要求较高的场合；<strong>pixel shuffle</strong>是通过卷积操作把通道维度转换到空间维度，所需的计算量较大。</p>

<h3 id="-参考文献">⚪ 参考文献</h3>

<ul>
  <li><a href="https://0809zheng.github.io/2020/08/03/srcnn.html"><font color="blue">Image Super-Resolution Using Deep Convolutional Networks</font></a>：(arXiv1501)SRCNN：图像超分辨率的开山之作。</li>
  <li><a href="https://0809zheng.github.io/2020/08/05/vdsr.html"><font color="blue">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</font></a>：(arXiv1511)VDSR：非常深的超分辨率模型。</li>
  <li><a href="https://0809zheng.github.io/2020/08/04/fsrcnn.html"><font color="blue">Accelerating the Super-Resolution Convolutional Neural Network</font></a>：(arXiv1608)FSRCNN：加速SRCNN模型。</li>
  <li><a href="https://0809zheng.github.io/2020/08/11/pixelshuffle.html"><font color="blue">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</font></a>：(arXiv1609)ESPCN：基于PixelShuffle上采样的超分辨率网络。</li>
  <li><a href="https://0809zheng.github.io/2020/09/07/lapsrn.html"><font color="blue">Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution</font></a>：(arXiv1704)LapSRN：多尺度超分辨率的拉普拉斯金字塔网络。</li>
  <li><a href="https://0809zheng.github.io/2020/08/06/edsr.html"><font color="blue">Enhanced Deep Residual Networks for Single Image Super-Resolution</font></a>：(arXiv1707)EDSR：增强的深度超分辨率网络。</li>
  <li><a href="https://0809zheng.github.io/2020/08/10/srresnet.html"><font color="blue">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</font></a>：(arXiv1719)SRResnet/SRGAN：使用生成对抗网络进行图像超分辨率。</li>
  <li><a href="https://0809zheng.github.io/2020/08/02/dbpn.html"><font color="blue">Deep Back-Projection Networks For Super-Resolution</font></a>：(arXiv1803)DBPN：一种反复下采样与上采样的超分辨率模型。</li>
  <li><a href="https://0809zheng.github.io/2020/08/01/rcan.html"><font color="blue">Image Super-Resolution Using Very Deep Residual Channel Attention Networks</font></a>：(arXiv1807)RCAN：残差通道注意力网络。</li>
  <li><a href="https://0809zheng.github.io/2020/08/12/esrgan.html"><font color="blue">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</font></a>：(arXiv1809)ESRGAN：增强的图像超分辨率生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2020/08/09/san.html"><font color="blue">Second-order Attention Network for Single Image Super-Resolution</font></a>：(CVPR1906)SAN：超分辨率二阶注意力网络。</li>
  <li><a href="https://0809zheng.github.io/2020/08/17/drn.html"><font color="blue">Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution</font></a>：(arXiv2003)DRN：一种闭环的图像超分辨率模型。</li>
  <li><a href="https://0809zheng.github.io/2020/12/22/liif.html"><font color="blue">Learning Continuous Image Representation with Local Implicit Image Function</font></a>：(arXiv2012)LIIF：学习2D图像的连续表达形式。</li>
</ul>

<h1 id="5-评估指标">5. 评估指标</h1>
<p>图像超分辨率的评估指标分为<strong>客观指标</strong>和<strong>主观指标</strong>。主观指标通常招募志愿者人为地判断超分图像的质量；下面介绍一些客观指标。</p>

<h2 id="1峰值信噪比-psnr">（1）峰值信噪比 PSNR</h2>
<p><strong>峰值信噪比（Peak signal-to-noise ratio，PSNR）</strong>，是衡量图像失真水平的客观标准，评价结果以$dB$（分贝）为单位表示。两个图像间<strong>PSNR</strong>值越大，则越趋于无劣化，劣化程度较大时，<strong>PSNR</strong>值趋于$0dB$。</p>

<p>大小为$m×n$的噪声图像$\hat{I}_y$和干净图像$I_y$的<strong>PSNR</strong>可由其均方误差计算：</p>

\[\begin{aligned}
MSE &amp;= \frac{1}{mn}\sum_{i=1}^{m} {\sum_{i=1}^{n} {(\hat{I}_y-I_y)^2}} \\
PSNR &amp;= 10·\log_{10}(\frac{L^2}{MSE})
\end{aligned}\]

<p>其中$L$为图像像素的取值范围，如浮点型数据$1.0$或<strong>uint8</strong>数据$255$。</p>

<p>上面是针对<strong>灰度</strong>图像的计算方法，如果是<strong>彩色</strong>图像，通常有三种方法来计算。</p>
<ol>
  <li>分别计算<strong>RGB</strong>三个通道的<strong>PSNR</strong>，然后取平均值。</li>
  <li>计算<strong>RGB</strong>三通道的<strong>MSE</strong>，然后再除以$3$。</li>
  <li>将图片转化为<strong>YCbCr</strong>格式，然后只计算<strong>Y</strong>分量（亮度分量）的<strong>PSNR</strong>。</li>
</ol>

<p>使用<code class="language-plaintext highlighter-rouge">python</code>实现<strong>PSNR</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">skimage.measure</span> <span class="kn">import</span> <span class="n">compare_psnr</span>

<span class="c1"># 方法一
</span><span class="k">def</span> <span class="nf">psnr</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">img1</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">img2</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">,</span> <span class="s">'np.uint8 is supposed.'</span>
    <span class="n">img1</span> <span class="o">=</span> <span class="n">img1</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">img2</span> <span class="o">=</span> <span class="n">img2</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">img1</span> <span class="o">-</span> <span class="n">img2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mse</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="mf">255.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>
	
<span class="c1"># 方法二
</span><span class="k">def</span> <span class="nf">psnr</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">):</span><span class="err">：</span>
    <span class="k">assert</span> <span class="n">img1</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">img2</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">,</span> <span class="s">'np.uint8 is supposed.'</span>
    <span class="k">return</span> <span class="n">psnr</span> <span class="o">=</span> <span class="n">compare_psnr</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">,</span> <span class="n">data_range</span><span class="o">=</span><span class="mi">255</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>PSNR</strong>是最普遍和使用最为广泛的一种图像客观评价指标，然而它是基于对应像素点间的误差，即基于误差敏感的图像质量评价。</p>

<p>其并未考虑到人眼的视觉特性（人眼对空间频率较低的对比差异敏感度较高，人眼对亮度对比差异的敏感度较色度高，人眼对一个区域的感知结果会受到其周围邻近区域的影响等），因而经常出现评价结果与人的主观感觉不一致的情况。</p>

<h2 id="2结构相似度-ssim">（2）结构相似度 SSIM</h2>
<p><strong>结构相似度（Structural Similarity，SSIM）</strong>从<strong>亮度(luminance)</strong>、<strong>对比度(contrast)</strong>和<strong>结构(structure)</strong>三个角度出发衡量图像之间的差异。</p>

<p><strong>SSIM</strong>通过图像的均值、方差和协方差计算：</p>

<p><img src="https://pic.downk.cc/item/5f47a6ce160a154a6766bf11.jpg" alt="" /></p>

<p>其中$C_1$、$C_2$、$C_3$是避免分母为零的常数； $α, β, γ$ 设为 $1$。</p>

<p>使用<code class="language-plaintext highlighter-rouge">python</code>实现<strong>SSIM</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">skimage.measure</span> <span class="kn">import</span> <span class="n">compare_ssim</span>

<span class="k">def</span> <span class="nf">ssim</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">img1</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">img2</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">,</span> <span class="s">'np.uint8 is supposed.'</span>
    <span class="k">return</span> <span class="n">compare_ssim</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">,</span> <span class="n">data_range</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">multichannel</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="6-benchmarks">6. Benchmarks</h1>

<p>对于常见的图像超分辨率数据集，通常使用一些大型数据集进行训练，使用一些小型数据集进行测试。</p>

<p>训练集包括：</p>
<ul>
  <li><a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">DIV2K</a>：包含1000张高清图(2K分辨率)及其$2,3,4,8$倍下采样图，其中800张作为训练，100张作为验证，100张作为测试。</li>
  <li><a href="http://cv.snu.ac.kr/research/EDSR/Flickr2K.tar">Flickr2K</a>：包含2650张png图像，包含人物、动物、风景；格式与DIV2K相同。</li>
</ul>

<p>测试集则包括<strong>Set5</strong>、<strong>Set14</strong>、<strong>BSDS100</strong>、<strong>Urban100</strong>、<strong>Manga109</strong>等。</p>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/08/31/l0norm.html" class="read-next-link"></a>
        <section>
          <span>Learning Sparse Neural Networks through L0 Regularization</span>
          <p>  通过L0正则化学习稀疏神经网络.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/6496be3d1ddac507cc9991fc.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/08/18/belt.html" class="read-next-link"></a>
          <section>
            <span>讨论带传动的拉力关系</span>
            <p>  Discussing the Tension Relationship of Belt Drive.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/66868cfad9c307b7e9f2e0b4.png" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
