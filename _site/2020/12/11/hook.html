<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pytorch中的Hook机制 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Pytorch中的Hook机制">
    <meta name="keywords"  content="Python">
    <!-- Open Graph -->
    <meta property="og:title" content="Pytorch中的Hook机制 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/12/11/hook.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#Python" class="post-tag">Python</a>
          
        
      </div>
      <h1>Pytorch中的Hook机制</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-12-11"><i class="iconfont icon-date"></i>11 Dec 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Hook mechanism in Pytorch.</p>
</blockquote>

<p><strong>钩子编程（hooking）</strong>，也称作“挂钩”，是计算机程序设计术语，指通过拦截软件模块间的函数调用、消息传递、事件传递来修改或扩展操作系统、应用程序或其他软件组件的行为的各种技术。处理被拦截的函数调用、事件、消息的代码，被称为<strong>钩子（hook）</strong>。</p>

<p><strong>Hook</strong>是<strong>PyTorch</strong>中一个十分有用的特性。利用它，我们可以<strong>不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度</strong>。这个功能被广泛用于可视化神经网络中间层的特征或梯度，从而诊断神经网络中可能出现的问题，分析网络有效性。</p>

<h1 id="1-hook-for-tensors">1. Hook for Tensors</h1>
<p>本节介绍张量的<strong>hook</strong>。在<strong>PyTorch</strong>的<strong>计算图(computation graph)</strong>中，只有<strong>叶节点(leaf node)</strong>的变量会保留梯度，而所有中间变量的梯度只在反向传播中使用，一旦反向传播完成，中间变量的梯度将自动释放，从而节约内存。</p>

<p>下图是一个简单的计算图，其中$x,y,w$是叶节点（直接给定数值的变量），$z,o$是中间变量（由其他变量计算得到的变量）。</p>

<p><img src="https://pic.downk.cc/item/5fd308b83ffa7d37b37913d9.jpg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch

x = torch.Tensor([0, 1, 2, 3]).requires_grad_()
y = torch.Tensor([4, 5, 6, 7]).requires_grad_()
w = torch.Tensor([1, 2, 3, 4]).requires_grad_()
z = x + y
o = w.matmul(z)
o.backward()

print('x.requires_grad:', x.requires_grad)  # True
print('y.requires_grad:', y.requires_grad)  # True
print('z.requires_grad:', z.requires_grad)  # True
print('w.requires_grad:', w.requires_grad)  # True
print('o.requires_grad:', o.requires_grad)  # True

print('x.grad:', x.grad)  # tensor([1., 2., 3., 4.])
print('y.grad:', y.grad)  # tensor([1., 2., 3., 4.])
print('w.grad:', w.grad)  # tensor([4., 6., 8., 10.])
print('z.grad:', z.grad)  # None
print('o.grad:', o.grad)  # None
</code></pre></div></div>

<p>从上面的例子中可以看出，由于$z,o$是中间变量，它们虽然<code class="language-plaintext highlighter-rouge">requires_grad</code>的参数都是<code class="language-plaintext highlighter-rouge">True</code>，但反向传播后其梯度并没有保存下来，而是直接删除了，因此为<code class="language-plaintext highlighter-rouge">None</code>。如果想在反向传播后保留他们的梯度，则需要特殊指定：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z.retain_grad()
o.retain_grad()

print('z.requires_grad:', z.requires_grad) # True
print('o.requires_grad:', o.requires_grad) # True
print('z.grad:', z.grad)  # tensor([1., 2., 3., 4.])
print('o.grad:', o.grad)  # tensor(1.)
</code></pre></div></div>
<p>但这种使用<code class="language-plaintext highlighter-rouge">retain_grad()</code>的方案会增加内存的占用，并不是一个好的方法。可以使用<strong>hook</strong>保存中间变量的梯度。</p>

<p>对于中间变量$z$，<strong>hook</strong>的使用方法为：<code class="language-plaintext highlighter-rouge">z.register_hook(hook_fn)</code>，其中<code class="language-plaintext highlighter-rouge">hook_fn</code>为一个用户自定义的函数：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def hook_fn(grad): -&gt; Tensor or None
</code></pre></div></div>

<p>该函数输入为变量$z$的梯度，输出为一个<strong>Tensor</strong>或<code class="language-plaintext highlighter-rouge">None</code>（<code class="language-plaintext highlighter-rouge">None</code>一般用于直接打印梯度）。反向传播时，梯度传播到变量$z$后，再继续往前传播之前，将会传入<code class="language-plaintext highlighter-rouge">hook_fn</code>函数。如果<code class="language-plaintext highlighter-rouge">hook_fn</code>的返回值是<code class="language-plaintext highlighter-rouge">None</code>，则梯度不改变，继续向前传播；如果<code class="language-plaintext highlighter-rouge">hook_fn</code>的返回值是<strong>Tensor</strong>类型，则该<strong>Tensor</strong>将取代变量$z$原有的梯度，继续向前传播。</p>

<p>下面的例子中<code class="language-plaintext highlighter-rouge">hook_fn</code>打印梯度值并修改为原来的两倍：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def hook_fn(grad):
    print(g)
    g = 2 * grad
    return g

z.register_hook(hook_fn)

o.backward()  # tensor([1., 2., 3., 4.])
print('z.grad:', z.grad)  # None
</code></pre></div></div>

<p>在实际代码中，为简化表示，也可以用<code class="language-plaintext highlighter-rouge">lambda</code>表达式代替函数，简写如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z.register_hook(lambda x: print(x))
z.register_hook(lambda x: 2*x)
</code></pre></div></div>

<p>注意到一个变量可以绑定多个<code class="language-plaintext highlighter-rouge">hook_fn</code>函数，反向传播时，按绑定顺序依次执行。</p>

<h1 id="2-hook-for-modules">2. Hook for Modules</h1>
<p>本节介绍模块的<strong>hook</strong>。模块不像上一节介绍的<strong>Tensor</strong>一样拥有显式的变量名可以访问，而是被封装在神经网络中。通常只能获得网络整体的输入和输出，而对于网络中间的模块，不仅很难得到它输入和输出的梯度，甚至连输入输出的数值都无法获得。比较麻烦的做法是，在<strong>forward</strong>函数的返回值中包含中间模块的输出；或者把网络按照模块的名称拆分再组合，提取中间层的特征。</p>

<p><strong>Pytorch</strong>设计了两种<strong>hook</strong>：<code class="language-plaintext highlighter-rouge">register_forward_hook</code>和<code class="language-plaintext highlighter-rouge">register_backward_hook</code>，分别用来获取前向传播和反向传播时中间层模块的输入和输出特征及梯度，从而大大降低了获取模型内部信息流的难度。</p>

<h2 id="register_forward_hook">register_forward_hook</h2>
<p><code class="language-plaintext highlighter-rouge">register_forward_hook</code>的作用是获取前向传播过程中，网络各模块的<strong>输入和输出</strong>。对于模块<code class="language-plaintext highlighter-rouge">module</code>，其使用方法为：<code class="language-plaintext highlighter-rouge">module.register_forward_hook(hook_fn)</code>，其中<code class="language-plaintext highlighter-rouge">hook_fn</code>为一个用户自定义的函数：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def hook_fn(module, input, output): -&gt; Tensor or None
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">hook_fn</code>函数的输入变量分别为模块、模块的输入和模块的输出。输出为<strong>None</strong>，<strong>Pytorch1.2.0</strong>之后的版本也可以返回张量，用于修改模块的输出。借助这个<strong>hook</strong>，可以方便的使用预训练的神经网络提取特征，而不用改变预训练网络的结构。下面是一个简单的例子：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from torch import nn

#  全局变量，用于存储中间层的特征
total_feat_out = []
total_feat_in = []

#  定义 forward hook function
def hook_fn_forward(module, input, output):
    print(module)  # 打印模块名，用于区分模块
    print('input', input)   # 打印该模块的输入
    print('output', output) # 打印该模块的输出
    total_feat_out.append(output) # 保存该模块的输出
    total_feat_in.append(input)   # 保存该模块的输入

model = Model()

modules = model.named_children()
for name, module in modules:
    module.register_forward_hook(hook_fn_forward)

#  注意下面代码中 x 的维度，第一维是 batch size
#  forward hook 中看不出来，但是 backward hook 中是必要的。
x = torch.Tensor([[1.0, 1.0, 1.0]]).requires_grad_() 
</code></pre></div></div>

<h2 id="register_backward_hook">register_backward_hook</h2>
<p><code class="language-plaintext highlighter-rouge">register_backward_hook</code>的作用是获取反向传播过程中，网络各模块<strong>输入端和输出端的梯度值</strong>。对于模块<code class="language-plaintext highlighter-rouge">module</code>，其使用方法为：<code class="language-plaintext highlighter-rouge">module.register_backward_hook(hook_fn)</code>，其中<code class="language-plaintext highlighter-rouge">hook_fn</code>为一个用户自定义的函数：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def hook_fn(module, grad_input, grad_output): -&gt; Tensor or None
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">hook_fn</code>函数的输入变量分别为模块、模块输入端的梯度和模块输出端的梯度（这里的输入端和输出端是站在<strong>前向传播</strong>的角度来说的）。如果模块有多个输入端或输出端，则对应的梯度是<strong>tuple</strong>类型（例如对于线性模块，其<code class="language-plaintext highlighter-rouge">grad_input</code>是一个三元组，排列顺序分别为：对<code class="language-plaintext highlighter-rouge">bias</code>的导数、对输入<code class="language-plaintext highlighter-rouge">x</code>的导数、对权重<code class="language-plaintext highlighter-rouge">W</code>的导数）。下面是一个简单的例子：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from torch import nn

#  全局变量，用于存储中间层的梯度
total_grad_out = []
total_grad_in = []

# 定义 backward hook function
def hook_fn_backward(module, grad_input, grad_output):
    print(module)  # 打印模块名，用于区分模块
    print('grad_output', grad_output)  # 打印该模块输出端的梯度
    print('grad_input', grad_input)    # 打印该模块输入端的梯度
    total_grad_in.append(grad_input)   # 保存该模块输入端的梯度
    total_grad_out.append(grad_output) # 保存该模块输出端的梯度

model = Model()

modules = model.named_children()
for name, module in modules:
    module.register_backward_hook(hook_fn_backward)

#  这里的 requires_grad 很重要，如果不加，backward hook
#  执行到第一层，对 x 的导数将为 None 。
#  此外再强调一遍 x 的维度，第一维一定是 batch size
x = torch.Tensor([[1.0, 1.0, 1.0]]).requires_grad_()
</code></pre></div></div>

<h3 id="注意事项">注意事项</h3>
<p><code class="language-plaintext highlighter-rouge">register_backward_hook</code>在全连接层和卷积层中的表现是不一致的，具体如下：</p>
<ul>
  <li>形状不一致
    <ol>
      <li>在卷积层中，<strong>weight</strong>的梯度和<strong>weight</strong>的形状相同；</li>
      <li>在全连接层中，<strong>weight</strong>的梯度的形状是<strong>weight</strong>形状的转置。</li>
    </ol>
  </li>
  <li><code class="language-plaintext highlighter-rouge">grad_input</code>元组中梯度的顺序不一致
    <ol>
      <li>在卷积层中，梯度的顺序为：（对<strong>feature</strong>的梯度，对<strong>weight</strong>的梯度，对<strong>bias</strong>的梯度）；</li>
      <li>在全连接层中，梯度的顺序为：（对<strong>bias</strong>的梯度，对<strong>feature</strong>的梯度，对<strong>weight</strong>的梯度）。</li>
    </ol>
  </li>
  <li>当<strong>batch size</strong>大于$1$时，对<strong>bias</strong>的梯度处理不一致
    <ol>
      <li>在卷积层中，对<strong>bias</strong>的梯度为整个<strong>batch</strong>的数据在<strong>bias</strong>上的梯度之和：（对<strong>feature</strong>的梯度，对<strong>weight</strong>的梯度，对<strong>bias</strong>的梯度）；</li>
      <li>在全连接层中，对<strong>bias</strong>的梯度是分开的，<strong>batch</strong>中的每个数据对应一个<strong>bias</strong>的梯度：（(<strong>data1</strong>对<strong>bias</strong>的梯度，<strong>data2</strong>对<strong>bias</strong>的梯度…)，对<strong>feature</strong>的梯度，对<strong>weight</strong>的梯度）。</li>
    </ol>
  </li>
</ul>

<p>特别地，如果已知某个模块的类型，也可以用下面的方式对其加<strong>hook</strong>：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for name, module in modules:
    if isinstance(module, nn.ReLU):
        module.register_forward_hook(forward_hook_fn)
        module.register_backward_hook(backward_hook_fn)
</code></pre></div></div>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/12/12/nadam.html" class="read-next-link"></a>
        <section>
          <span>Incorporating Nesterov Momentum into Adam</span>
          <p>  Nadam：将Nesterov动量引入Adam算法.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/620089762ab3f51d917ee290.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/12/10/amsgrad.html" class="read-next-link"></a>
          <section>
            <span>On the Convergence of Adam and Beyond</span>
            <p>  AMSGrad：改进Adam算法的收敛性.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/620089762ab3f51d917ee290.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
