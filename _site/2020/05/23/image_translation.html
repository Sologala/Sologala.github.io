<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>图像到图像翻译(Image-to-Image Translation) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="图像到图像翻译(Image-to-Image Translation)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="图像到图像翻译(Image-to-Image Translation) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/05/23/image_translation.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>图像到图像翻译(Image-to-Image Translation)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-05-23"><i class="iconfont icon-date"></i>23 May 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/63970e69b1fccdcd367c01dd.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Image-to-Image Translation.</p>
</blockquote>

<p><strong>图像到图像翻译(Image-to-Image Translation)</strong>旨在学习一个映射使得图像可以从源图像域(<strong>source domain</strong>)变换到目标图像域(<strong>target domain</strong>)，同时保留图像内容(<strong>context</strong>)。</p>

<p>计算机视觉领域的图像到图像翻译问题有许多表现形式，如白天到黑夜的图像转换、黑白照到彩色照的转换、梵高风格化等；这些应用基本都是通过<a href="https://0809zheng.github.io/2022/02/01/gan.html">生成对抗网络</a>实现的。</p>

<p>根据是否提供了一对一的学习样本对，将图像到图像翻译任务划分为<strong>有配对数据(paired data)</strong>和<strong>无配对数据(unpaired data)</strong>两种情况。</p>
<ul>
  <li>有配对数据(监督图像翻译)是指在训练数据集中具有一对一的数据对；即给定联合分布$p(X,Y)$，学习条件映射$f_{x \to y}=p(Y|X)$和$f_{y \to x}=p(X|Y)$。代表方法有<strong>Pix2Pix</strong>, <strong>BicycleGAN</strong>, <strong>LPTN</strong>。</li>
  <li>无配对数据(无监督图像翻译)是指模型在多个独立的数据集之间训练，能够从多个数据集合中自动地发现集合之间的关联，从而学习出映射函数；即给定边缘分布$p(X)$和$p(Y)$，学习条件映射$f_{x \to y}=p(Y|X)$和$f_{y \to x}=p(X|Y)$。代表方法有<strong>CoGAN</strong>, <strong>PixelDA</strong>, <strong>CycleGAN</strong>, <strong>DiscoGAN</strong>, <strong>DualGAN</strong>, <strong>UNIT</strong>, <strong>MUNIT</strong>, <strong>TUNIT</strong>, <strong>StarGAN</strong>, <strong>StarGAN v2</strong>, <strong>GANILLA</strong>, <strong>NICE-GAN</strong>, <strong>CUT</strong>, <strong>SimDCL</strong>。</li>
</ul>

<h1 id="1-有配对数据的图像到图像翻译">1. 有配对数据的图像到图像翻译</h1>

<h3 id="-pix2pix">⚪ <a href="https://0809zheng.github.io/2022/03/10/p2p.html"><font color="Blue">Pix2Pix</font></a></h3>

<p><strong>Pix2Pix</strong>的生成器采用<strong>UNet</strong>网络，把一种类型的图像$x$转换为另一种类型的图像$\hat{y}=G(x)$；损失函数包括对抗损失(<strong>NSGAN</strong>)和<strong>L1</strong>重构损失。</p>

<p>判别器设计为<strong>PatchGAN</strong>，同时接收两种类型的图像$(x,y)$，判断其是否为真实图像；损失函数为对抗损失(<strong>MMGAN</strong>)。</p>

\[\begin{aligned}  \mathop{\max}_{D} &amp; \Bbb{E}_{(x,y) \text{~} (P_{data}(x),P_{data}(y))}[\log D(x,y)] + \Bbb{E}_{x \text{~} P_{data}(x)}[\log(1-D(x,G(x)))] \\ \mathop{ \min}_{G} &amp; -\Bbb{E}_{x \text{~} P_{data}(x)}[\log(D(x,G(x))] + \lambda \Bbb{E}_{(x,y) \text{~} (P_{data}(x),P_{data}(y))}[||y-G(x)||_1] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/6352430c16f2c2beb1d80d27.jpg" alt="" /></p>

<h3 id="-bicyclegan">⚪ <a href="https://0809zheng.github.io/2022/03/18/bicyclegan.html"><font color="Blue">BicycleGAN</font></a></h3>

<p><strong>BicycleGAN</strong>的训练过程采用双向的循环过程:</p>
<ul>
  <li>一种过程采用变分自编码器的形式。将图像$B$通过一个编码器$E(\cdot)$编码为隐变量$z$，与图像$A$共同输入生成器重构图像$\hat{B}$。建立图像$B$和图像$\hat{B}$之间的重构损失和判别损失，并且构造隐变量$z$的<strong>KL</strong>散度。</li>
  <li>另一种过程采用条件生成对抗网络的形式。将图像$A$和随机噪声$z$输入生成器构造图像$\hat{B}$，将其与图像$B$共同构造判别损失。并且使用编码器$E(\cdot)$将图像$\hat{B}$编码为隐变量$z$，构造其与输入隐变量之间的重构损失。</li>
</ul>

<p><img src="https://pic1.imgdb.cn/item/6353a01f16f2c2beb186f677.jpg" alt="" /></p>

<h3 id="-lptn">⚪ <a href="https://0809zheng.github.io/2022/04/27/lptn.html"><font color="Blue">LPTN</font></a></h3>

<p><strong>LPTN</strong>将图像用拉普拉斯金字塔表示，其中不同层次中的拉普拉斯图像存储了图像中的高频内容信息，因此通过轻量的卷积网络做简单处理；网络顶层的高斯图像存储图像中的低频风格信息，通过一个相对复杂的网络进行处理。整体网络仍然是轻量型的，可以实现实时图像翻译。</p>

<p><img src="https://pic.imgdb.cn/item/6398211fb1fccdcd36025ba9.jpg" alt="" /></p>

<h1 id="2-无配对数据的图像到图像翻译">2. 无配对数据的图像到图像翻译</h1>

<h3 id="-cogan">⚪ <a href="https://0809zheng.github.io/2022/03/08/cogan.html"><font color="Blue">CoGAN</font></a></h3>

<p><strong>CoGAN</strong>使用两个<strong>GAN</strong>网络，这两个网络通过权重共享机制学习通用的高级语义特征。两个网络的生成器(共享浅层网络)分别学习不同数据域中的数据分布，判别器(共享深层网络)则分别判断是否为对应数据域中的真实数据。目标函数如下：</p>

\[\begin{aligned} \mathop{ \min}_{G_1,G_2} \mathop{\max}_{D_1,D_2} &amp; \Bbb{E}_{x_1 \text{~} P_{data}(x_1)}[\log D_1(x_1)] + \Bbb{E}_{z_1 \text{~} P_{Z}(z_1)}[\log(1-D_1(G_1(z_1)))] \\ &amp; + \Bbb{E}_{x_2 \text{~} P_{data}(x_2)}[\log D_2(x_2)] + \Bbb{E}_{z_2 \text{~} P_{Z}(z_2)}[\log(1-D_2(G_2(z_2)))] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/63523d2816f2c2beb1d08242.jpg" alt="" /></p>

<h3 id="-pixelda">⚪ <a href="https://0809zheng.github.io/2022/03/15/pixelda.html"><font color="Blue">PixelDA</font></a></h3>

<p><strong>PixelDA</strong>的生成器接收源域图像和随机噪声，将其转换为目标域的图像；判别器判断输入的目标域图像是否真实；额外引入任务相关的网络(通常是分类器)辅助生成器学习。</p>

\[\begin{aligned}  \mathop{\max}_{D} &amp; \Bbb{E}_{x^t \text{~} P_{data}^t(x)}[\log D(x^t)] + \Bbb{E}_{(x^s,z) \text{~} (P_{data}^s(x),P_z(z))}[\log(1-D(G(x^s,z)))] \\ \mathop{ \min}_{G} &amp; -\Bbb{E}_{(x^s,z) \text{~} (P_{data}^s(x),P_z(z))}[\log(D(G(x^s,z))] - \Bbb{E}_{(x,y) \text{~} (P_{data}(x),P_{data}(y))}[\log T_y(x)] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/6352510b16f2c2beb1e9250c.jpg" alt="" /></p>

<h3 id="-cyclegan">⚪ <a href="https://0809zheng.github.io/2022/02/14/cyclegan.html"><font color="Blue">CycleGAN</font></a></h3>

<p><strong>CycleGAN</strong>训练了两个结构为<strong>ResNet</strong>的生成器，\(G_{X→Y}\)实现从类型$X$转换成类型$Y$，\(G_{Y→X}\)实现从类型$Y$转换成类型$X$；损失函数包括对抗损失(<strong>LSGAN</strong>)和<strong>L1</strong>循环一致性损失。</p>

<p>训练两个结构为<strong>PatchGAN</strong>的判别器，\(D_{X}\)判断图像是否属于类型$X$；\(D_{Y}\)判断图像是否属于类型$Y$；损失函数为对抗损失(<strong>LSGAN</strong>)。</p>

\[\begin{aligned}  \mathop{\min}_{D_X,D_Y} &amp; \Bbb{E}_{y \text{~} P_{data}(y)}[(D_Y(y)-1)^2] + \Bbb{E}_{x \text{~} P_{data}(x)}[(D_Y(G_{X \to Y}(x)))^2] \\ &amp;+  \Bbb{E}_{x \text{~} P_{data}(x)}[(D_X(x)-1)^2] + \Bbb{E}_{y \text{~} P_{data}(y)}[(D_X(G_{Y \to X}(y)))^2] \\ \mathop{ \min}_{G_{X \to Y},G_{Y \to X}} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[(D_Y(G_{X \to Y}(x))-1)^2]+\Bbb{E}_{y \text{~} P_{data}(y)}[(D_X(G_{Y \to X}(y))-1)^2] \\ &amp;+ \Bbb{E}_{x \text{~} P_{data}(x)}[||G_{Y \to X}(G_{X \to Y}(x))-x||_1] \\ &amp;+ \Bbb{E}_{y \text{~} P_{data}(y)}[||G_{X \to Y}(G_{Y \to X}(y))-y||_1] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/63525c6616f2c2beb1f916c7.jpg" alt="" /></p>

<h3 id="-discogan">⚪ <a href="https://0809zheng.github.io/2022/03/16/discogan.html"><font color="Blue">DiscoGAN</font></a></h3>

<p><strong>DiscoGAN</strong>训练了两个结构为<strong>UNet</strong>的生成器，\(G_{X→Y}\)实现从类型$X$转换成类型$Y$，\(G_{Y→X}\)实现从类型$Y$转换成类型$X$；损失函数包括对抗损失(<strong>NSGAN</strong>)和<strong>L2</strong>循环一致性损失。</p>

<p>训练两个结构为<strong>PatchGAN</strong>的判别器，\(D_{X}\)判断图像是否属于类型$X$；\(D_{Y}\)判断图像是否属于类型$Y$；损失函数为对抗损失(<strong>NSGAN</strong>)。</p>

\[\begin{aligned}  \mathop{\max}_{D_X,D_Y} &amp; \Bbb{E}_{y \text{~} P_{data}(y)}[\log D_Y(y)] + \Bbb{E}_{x \text{~} P_{data}(x)}[\log(1-D_Y(G_{X \to Y}(x)))] \\ &amp;+  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D_X(x)] + \Bbb{E}_{y \text{~} P_{data}(y)}[\log(1-D_X(G_{Y \to X}(y)))] \\ \mathop{ \min}_{G_{X \to Y},G_{Y \to X}} &amp;- \Bbb{E}_{x \text{~} P_{data}(x)}[\log(D_Y(G_{X \to Y}(x)))]-\Bbb{E}_{y \text{~} P_{data}(y)}[\log(D_X(G_{Y \to X}(y)))] \\ &amp;+ \Bbb{E}_{x \text{~} P_{data}(x)}[||G_{Y \to X}(G_{X \to Y}(x))-x||_2^2] \\ &amp;+ \Bbb{E}_{y \text{~} P_{data}(y)}[||G_{X \to Y}(G_{Y \to X}(y))-y||_2^2] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/635347be16f2c2beb1f58193.jpg" alt="" /></p>

<h3 id="-dualgan">⚪ <a href="https://0809zheng.github.io/2022/03/17/dualgan.html"><font color="Blue">DualGAN</font></a></h3>

<p><strong>DualGAN</strong>训练了两个结构为<strong>UNet</strong>的生成器，\(G_{X→Y}\)实现从类型$X$转换成类型$Y$，\(G_{Y→X}\)实现从类型$Y$转换成类型$X$；损失函数包括对抗损失(<strong>WGAN</strong>)和<strong>L1</strong>循环一致性损失。</p>

<p>训练两个结构为<strong>PatchGAN</strong>的判别器，\(D_{X}\)判断图像是否属于类型$X$；\(D_{Y}\)判断图像是否属于类型$Y$；损失函数为对抗损失(<strong>WGAN</strong>)。</p>

\[\begin{aligned}  \mathop{\max}_{||D_X||_L\leq 1,||D_Y||_L\leq 1} &amp; \Bbb{E}_{y \text{~} P_{data}(y)}[D_Y(y)] - \Bbb{E}_{x \text{~} P_{data}(x)}[D_Y(G_{X \to Y}(x))] \\ &amp;+  \Bbb{E}_{x \text{~} P_{data}(x)}[D_X(x)] - \Bbb{E}_{y \text{~} P_{data}(y)}[D_X(G_{Y \to X}(y))] \\ \mathop{ \min}_{G_{X \to Y},G_{Y \to X}} &amp;- \Bbb{E}_{x \text{~} P_{data}(x)}[D_Y(G_{X \to Y}(x))]-\Bbb{E}_{y \text{~} P_{data}(y)}[D_X(G_{Y \to X}(y))] \\ &amp;+ \Bbb{E}_{x \text{~} P_{data}(x)}[||G_{Y \to X}(G_{X \to Y}(x))-x||_1] \\ &amp;+ \Bbb{E}_{y \text{~} P_{data}(y)}[||G_{X \to Y}(G_{Y \to X}(y))-y||_1] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/6353529516f2c2beb104842d.jpg" alt="" /></p>

<h3 id="-unit">⚪ <a href="https://0809zheng.github.io/2022/03/21/unit.html"><font color="Blue">UNIT</font></a></h3>

<p><strong>UNIT</strong>假设不同风格的一对图像$x_1,x_2$可以在隐空间中找到同一个对应的隐变量$z$。图像集与隐空间之间的映射关系通过<strong>VAE</strong>实现，分别使用两个编码器把图像映射到隐空间，再分别使用两个生成器把隐变量重构为图像。与此同时，引入两个判别器分别判断两种类型图像的真实性。</p>

<p><strong>UNIT</strong>的目标函数包括<strong>VAE</strong>损失、<strong>GAN</strong>损失和<strong>cycle consistency</strong>损失。</p>

\[\begin{aligned} \mathop{ \min}_{G_1,G_2,E_1,E_2} \mathop{\max}_{D_1,D_2} &amp; \mathcal{L}_{\text{VAE}_1}(E_1,G_1) + \mathcal{L}_{\text{GAN}_1}(E_1,G_1,D_1) + \mathcal{L}_{\text{CC}_1}(E_1,G_1,E_2,G_2) \\ &amp; +\mathcal{L}_{\text{VAE}_2}(E_2,G_2) + \mathcal{L}_{\text{GAN}_2}(E_2,G_2,D_2) + \mathcal{L}_{\text{CC}_2}(E_2,G_2,E_1,G_1) \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/63563ebd16f2c2beb199c4fb.jpg" alt="" /></p>

<h3 id="-munit">⚪ <a href="https://0809zheng.github.io/2022/04/26/munit.html"><font color="Blue">MUNIT</font></a></h3>

<p><strong>MUNIT</strong>假设每一张图像$x$都对应在所有领域共享的内容空间中的内容编码$c$和领域特有的风格空间中的风格编码$s$。</p>

<p><strong>MUNIT</strong>的学习过程包括图像重构和编码重构两部分。图像重构是指对图像$x_1,x_2$分别编码为$(c_1,s_1),(c_2,s_2)$，再解码为重构图像\(\hat{x}_1,\hat{x}_2\)，并最终构造两者的<strong>L1</strong>重构损失。编码重构是指对图像$x_1,x_2$分别编码为$(c_1,s_1),(c_2,s_2)$，然后重组编码$(c_1,s_2),(c_2,s_1)$，并解码为迁移风格的图像\(x_{1 \to 2},x_{2 \to 1}\)，然后再将其分别编码为\((\hat{c}_1,\hat{s}_2),(\hat{c}_2,\hat{s}_1)\)，并最终构造编码的<strong>L1</strong>重构损失。此外，对图像$x_1,x_2$和迁移图像\(x_{1 \to 2},x_{2 \to 1}\)应用对抗损失。</p>

\[\begin{aligned} \mathop{ \min}_{G_1,G_2,E_1,E_2} \mathop{\max}_{D_1,D_2} &amp;\mathcal{L}_{\text{GAN}}^{x_1} + \mathcal{L}_{\text{GAN}}^{x_2} + \lambda_x(\mathcal{L}_{\text{recon}}^{x_1}+\mathcal{L}_{\text{recon}}^{x_2}) \\ &amp; + \lambda_c(\mathcal{L}_{\text{recon}}^{c_1}+\mathcal{L}_{\text{recon}}^{c_2})+ \lambda_s(\mathcal{L}_{\text{recon}}^{s_1}+\mathcal{L}_{\text{recon}}^{s_2}) \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/6396f1d2b1fccdcd364bf63d.jpg" alt="" /></p>

<h3 id="-tunit">⚪ <a href="https://0809zheng.github.io/2022/04/28/tunit.html"><font color="Blue">TUNIT</font></a></h3>

<p><strong>TUNIT</strong>实现了一个图像数据集中任意两张图像之间的翻译，该方法由编码器、生成器、判别器组成。编码器把输入图像编码为领域标签(通过<strong>IIC</strong>预训练)和风格编码(通过<strong>MoCo</strong>预训练)；生成器根据输入图像和风格编码生成图像，损失函数包括<strong>L1</strong>重构损失、对抗损失和风格对比损失；判别器接收输入图像和领域标签，判定该领域中图像的真实性，损失函数为对抗损失。</p>

\[\begin{aligned} \mathop{ \min}_{G,E} \mathop{\max}_{D} &amp;\mathcal{L}_{\text{adv}}(D,G) + \lambda_{\text{style}}^G\mathcal{L}_{\text{style}}^G(G,E) + \lambda_{\text{rec}}\mathcal{L}_{\text{rec}}(G,E) \\ &amp; - \lambda_{MI}\mathcal{L}_{MI}(E)+ \lambda_{\text{style}}^E\mathcal{L}_{\text{style}}^E(E) \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63982573b1fccdcd360b128c.jpg" alt="" /></p>

<h3 id="-stargan">⚪ <a href="https://0809zheng.github.io/2022/03/19/stargan.html"><font color="Blue">StarGAN</font></a></h3>

<p><strong>StarGAN</strong>的判别器结构为<strong>PatchGAN</strong>，用于判断图像是否为真实图像，若为真实图像则进一步预测其领域标签；目标函数包括对抗损失(<strong>NSGAN</strong>)和标签分类损失。</p>

<p>生成器结构为<strong>ResNet</strong>，接收一张图像和给定的领域标签，生成对应领域的图像；目标函数包括对抗损失(<strong>NSGAN</strong>)、标签分类损失和<strong>L1</strong>重构损失。</p>

\[\begin{aligned}  \mathop{\max}_{D} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{data}(x)}[1-\log D(G(x, y^t))] \\ &amp;+  \Bbb{E}_{(x,y) \text{~} (P_{data}(x),P_{data}(Y))}[\log D_{y}(x)] \\ \mathop{ \min}_{G} &amp;- \Bbb{E}_{x \text{~} P_{data}(x)}[D(G(x, y^t))]-\Bbb{E}_{x \text{~} P_{data}(x)}[\log D_{y^t}(G(x, y^t))] \\ &amp;+ \Bbb{E}_{x \text{~} P_{data}(x)}[||x-G(G(x, y^t),y^s)||_1] \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/6353b87d16f2c2beb1ab7c07.jpg" alt="" /></p>

<h3 id="-stargan-v2">⚪ <a href="https://0809zheng.github.io/2022/03/20/starganv2.html"><font color="Blue">StarGAN v2</font></a></h3>

<p><strong>StarGAN v2</strong>由生成器、映射网络、风格编码器和判别器构成。生成器接收输入图像和风格编码，生成对应风格的图像；映射网络输入随机噪声，生成不同图像域的不同风格编码；风格编码器提取输入图像在不同图像域中的风格编码；判别器判断图像是否为某图像域中的真实图像。</p>

<p>损失函数包括对抗损失、风格编码的<strong>L1</strong>重构损失、通过最大化两个风格编码对应的生成图像的差异构造多样性敏感损失、使用输入图像的风格编码将生成图像重新映射到原图像域构造的循环一致性损失。</p>

\[\begin{aligned} \mathop{ \min}_{F,E,G} \mathop{\max}_{D} &amp; \mathcal{L}_{\text{adv}}(F,G,D) + \lambda_{\text{sty}} \mathcal{L}_{\text{sty}}(F,E,G) \\ &amp; -\lambda_{\text{ds}}\mathcal{L}_{\text{ds}}(F,G) +  \lambda_{\text{cyc}}\mathcal{L}_{\text{cyc}}(F,E,G) \end{aligned}\]

<p><img src="https://pic1.imgdb.cn/item/6353d5c516f2c2beb1d54563.jpg" alt="" /></p>

<h3 id="-ganilla">⚪ <a href="https://0809zheng.github.io/2022/04/29/ganilla.html"><font color="Blue">GANILLA</font></a></h3>

<p><strong>GANNILLA</strong>是一种从自然图像到儿童读物插画的图像翻译模型，其主体结构与<strong>CycleGAN</strong>, <strong>DualGAN</strong>, <strong>DiscoGAN</strong>等类似，作者重新设计了生成器的网络结构，能够在保留输入图像内容的同时迁移风格。<strong>GANNILLA</strong>的生成器采用非对称结构，使用带有拼接连接的残差层，并使用上采样算子代替转置卷积层。</p>

<p><img src="https://pic.imgdb.cn/item/63988cdab1fccdcd36c12d91.jpg" alt="" /></p>

<h3 id="-nice-gan">⚪ <a href="https://0809zheng.github.io/2022/05/17/nicegan.html"><font color="Blue">NICE-GAN</font></a></h3>

<p><strong>NICE-GAN</strong>通过把判别器的一部分重用为编码器，实现紧凑的网络结构设计。具体地，把判别器$D$拆分成编码器$E^D$和分类器$C^D$；训练流程与<strong>CycleGAN</strong>类似，训练两个生成器和两个判别器；生成器损失函数包括对抗损失(<strong>LSGAN</strong>)、<strong>L1</strong>循环一致性损失和<strong>L1</strong>重构损失；判别器的损失函数为对抗损失(<strong>LSGAN</strong>)。</p>

\[\begin{aligned}  \mathop{\min}_{D_X=E_X\circ C_X,D_Y=E_Y\circ C_Y} &amp; \Bbb{E}_{y \text{~} P_{data}(y)}[(D_Y(y)-1)^2] + \Bbb{E}_{x \text{~} P_{data}(x)}[(D_Y(G_{X \to Y}(E_X(x))))^2] \\ &amp;+  \Bbb{E}_{x \text{~} P_{data}(x)}[(D_X(x)-1)^2] + \Bbb{E}_{y \text{~} P_{data}(y)}[(D_X(G_{Y \to X}(E_Y(y))))^2] \\ \mathop{ \min}_{G_{X \to Y},G_{Y \to X}} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[(D_Y(G_{X \to Y}(E_X(x)))-1)^2]+\Bbb{E}_{y \text{~} P_{data}(y)}[(D_X(G_{Y \to X}(E_Y(y)))-1)^2] \\ &amp;+ \Bbb{E}_{x \text{~} P_{data}(x)}[||G_{Y \to X}(E_Y(G_{X \to Y}(E_X(x))))-x||_1] \\ &amp;+ \Bbb{E}_{y \text{~} P_{data}(y)}[||G_{X \to Y}(E_X(G_{Y \to X}(E_Y(y))))-y||_1] \\ &amp;+ \Bbb{E}_{x \text{~} P_{data}(x)}[||G_{Y \to X}(E_X(x))-x||_1] \\ &amp;+ \Bbb{E}_{y \text{~} P_{data}(y)}[||G_{X \to Y}(E_Y(y))-y||_1] \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63998f25b1fccdcd364ecf5b.jpg" alt="" /></p>

<h3 id="-cut">⚪ <a href="https://0809zheng.github.io/2022/05/10/cut.html"><font color="Blue">CUT</font></a></h3>

<p><strong>CUT</strong>使用对比学习构造生成器的损失函数。把输入图像$x$和生成图像$\hat{y}$通过生成器的编码部分提取特征，然后使用多层感知机$H_l$对特征进行变换。此时特征的每个像素位置对应原始图像的一个图像块；则两个图像相同位置的图像块对应的特征向量为正样本对，其余位置的特征向量为负样本。基于此可以构造对比损失：</p>

\[\mathcal{L}(v,v^+,v^-) = -\log [\frac{\exp(v \cdot v^+/ \tau)}{\exp(v \cdot v^+/ \tau)+
\sum_{n=1}^N \exp(v \cdot v^-_n/ \tau)}]\]

<p><img src="https://pic.imgdb.cn/item/63e3585d4757feff33191e07.jpg" alt="" /></p>

<h3 id="-simdcl">⚪ <a href="https://0809zheng.github.io/2022/06/30/simdcl.html"><font color="Blue">SimDCL</font></a></h3>

<p><strong>SimDCL</strong>使用两个<strong>CUT</strong>构成对偶结构。损失函数包括对抗损失、对比损失、恒等损失（目标域图像$Y$经过生成器\(G_{X→Y}\)后图像风格应该保持不变）和相似度损失（同一数据模态的样本$X_1,X_2$的特征应用多层感知机$H_{X1},H_{X2}$后应相同）。</p>

<p><img src="https://pic.imgdb.cn/item/63e457cb4757feff33833266.jpg" alt="" /></p>

<h1 id="-参考文献">⚪ 参考文献</h1>

<ul>
  <li><a href="https://0809zheng.github.io/2022/03/08/cogan.html"><font color="Blue">Coupled Generative Adversarial Networks</font></a>：(arXiv1606)CoGAN：耦合生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/10/p2p.html"><font color="Blue">Image-to-Image Translation with Conditional Adversarial Networks</font></a>：(arXiv1611)Pix2Pix：通过UNet和PatchGAN实现图像转换。</li>
  <li><a href="https://0809zheng.github.io/2022/03/15/pixelda.html"><font color="Blue">Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</font></a>：(arXiv1612)PixelDA：通过GAN实现像素级领域自适应。</li>
  <li><a href="https://0809zheng.github.io/2022/02/14/cyclegan.html"><font color="Blue">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</font></a>：(arXiv1703)CycleGAN：使用循环一致损失实现无配对数据的图像转换。</li>
  <li><a href="https://0809zheng.github.io/2022/03/16/discogan.html"><font color="Blue">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</font></a>：(arXiv1703)DiscoGAN：使用GAN学习发现跨领域关系。</li>
  <li><a href="https://0809zheng.github.io/2022/03/21/unit.html"><font color="Blue">Unsupervised Image-to-Image Translation Networks</font></a>：(arXiv1703)UNIT：无监督图像到图像翻译网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/17/dualgan.html"><font color="Blue">DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</font></a>：(arXiv1704)DualGAN：图像转换的无监督对偶学习。</li>
  <li><a href="https://0809zheng.github.io/2022/03/18/bicyclegan.html"><font color="Blue">Toward Multimodal Image-to-Image Translation</font></a>：(arXiv1711)BicycleGAN：多模态图像翻译。</li>
  <li><a href="https://0809zheng.github.io/2022/03/19/stargan.html"><font color="Blue">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</font></a>：(arXiv1711)StarGAN：统一的多领域图像翻译框架。</li>
  <li><a href="https://0809zheng.github.io/2022/04/26/munit.html"><font color="Blue">Multimodal Unsupervised Image-to-Image Translation</font></a>：(arXiv1804)MUNIT：多模态无监督图像到图像翻译网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/20/starganv2.html"><font color="Blue">StarGAN v2: Diverse Image Synthesis for Multiple Domains</font></a>：(arXiv1912)StarGAN v2：多领域多样性图像合成。</li>
  <li><a href="https://0809zheng.github.io/2022/04/29/ganilla.html"><font color="Blue">GANILLA: Generative Adversarial Networks for Image to Illustration Translation</font></a>：(arXiv2002)GANILLA：把图像转换为儿童绘本风格。</li>
  <li><a href="https://0809zheng.github.io/2022/05/17/nicegan.html"><font color="Blue">Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation</font></a>：(arXiv2003)NICE-GAN: 把判别器重用为编码器的图像翻译模型。</li>
  <li><a href="https://0809zheng.github.io/2022/04/28/tunit.html"><font color="Blue">Rethinking the Truly Unsupervised Image-to-Image Translation</font></a>：(arXiv2006)TUNIT：完全无监督图像到图像翻译。</li>
  <li><a href="https://0809zheng.github.io/2022/05/10/cut.html"><font color="Blue">Contrastive Learning for Unpaired Image-to-Image Translation</font></a>：(arXiv2006)CUT：无配对数据图像到图像翻译中的对比学习。</li>
  <li><a href="https://0809zheng.github.io/2022/06/30/simdcl.html"><font color="Blue">Dual Contrastive Learning for Unsupervised Image-to-Image Translation</font></a>：(arXiv2104)SimDCL：无监督图像到图像翻译的对偶对比学习。</li>
  <li><a href="https://0809zheng.github.io/2022/04/27/lptn.html"><font color="Blue">High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network</font></a>：(arXiv2105)LPTN：高分辨率真实感实时图像翻译。</li>
</ul>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/05/29/gaussian-mixture-model.html" class="read-next-link"></a>
        <section>
          <span>Gaussian Mixture Model(GMM)：高斯混合模型</span>
          <p>  Gaussian Mixture Model.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://img.imgdb.cn/item/60763e328322e6675c8cb89f.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/05/22/transfer-learning.html" class="read-next-link"></a>
          <section>
            <span>Transfer Learning：迁移学习</span>
            <p>  Transfer Learning.  迁移学习（Transfer Learning）是指将解决某个问题获取的...</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5efc584614195aa5940f63f1.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
