<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transfer Learning：迁移学习 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Transfer Learning：迁移学习">
    <meta name="keywords"  content="机器学习">
    <!-- Open Graph -->
    <meta property="og:title" content="Transfer Learning：迁移学习 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/05/22/transfer-learning.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="post-tag">机器学习</a>
          
        
      </div>
      <h1>Transfer Learning：迁移学习</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-05-22"><i class="iconfont icon-date"></i>22 May 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5efc584614195aa5940f63f1.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Transfer Learning.</p>
</blockquote>

<ul>
  <li><strong>迁移学习（Transfer Learning）</strong>是指将解决某个问题获取的知识应用在另一个不同但相关的问题中。</li>
  <li>Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a diferent but related problem.</li>
</ul>

<p>在迁移学习中，称模型最初解决的问题领域为<strong>source domain</strong>，其中的数据集为<strong>source data</strong>；需要迁移解决的相关问题领域为<strong>target domain</strong>，其中的数据集为<strong>target data</strong>。</p>

<p>通常<strong>source data</strong>数量大，而<strong>target data</strong>数量较少，两者的数据分布是不同的。</p>

<p>迁移学习的不同分类：</p>
<ul>
  <li><strong>source data</strong>有标签，<strong>target data</strong>有标签：<strong>fine tuning</strong>、<strong>multitask learning</strong></li>
  <li><strong>source data</strong>有标签，<strong>target data</strong>无标签：<strong>Domain Adaptation</strong>、<strong>zero-shot learning</strong></li>
  <li><strong>source data</strong>无标签，<strong>target data</strong>有标签：<strong>self-taught learning</strong></li>
  <li><strong>source data</strong>无标签，<strong>target data</strong>无标签：<strong>self-taught clustering</strong></li>
</ul>

<p><img src="https://pic.downk.cc/item/5efc584614195aa5940f63f1.jpg" alt="" /></p>

<p><strong>本文目录</strong>：</p>
<ol>
  <li>Fine-tuning</li>
  <li>Multitask Learning</li>
  <li>Domain Adaptation</li>
  <li>Zero-shot learning</li>
</ol>

<h3 id="扩展阅读">⭐扩展阅读：</h3>
<ul>
  <li><a href="https://0809zheng.github.io/2020/09/14/pre-training.html"><font color="blue">Rethinking ImageNet Pre-training</font></a>：(arXiv1811)对计算机视觉任务中预训练的一些讨论。</li>
  <li><a href="https://0809zheng.github.io/2020/07/04/bit.html"><font color="blue">Big Transfer (BiT): General Visual Representation Learning</font></a>：(arXiv1912)BiT：用于迁移学习的预训练卷积神经网络模型。</li>
  <li><a href="https://0809zheng.github.io/2020/07/18/mvp.html"><font color="blue">Movement Pruning: Adaptive Sparsity by Fine-Tuning</font></a>：(arXiv2005)讨论迁移学习中的权重剪枝。</li>
  <li><a href="https://0809zheng.github.io/2020/09/15/self-training.html"><font color="blue">Rethinking Pre-training and Self-training</font></a>：(arXiv2006)对计算机视觉任务中预训练和自训练的一些讨论。</li>
</ul>

<h1 id="1-fine-tuning">1. Fine-Tuning</h1>
<p><strong>模型微调（Fine-tuning）</strong>是指用<strong>source data</strong>训练模型，再用<strong>target data</strong>微调模型。</p>

<p>当<strong>target data</strong>只有几个样本时，也叫<strong>one-shot learning</strong>。</p>

<p>缺点：<strong>target data</strong>数量少，容易过拟合。</p>

<p>一些解决措施：</p>
<ul>
  <li><strong>保守学习 Conservative Training</strong>：在微调网络时，限制网络参数和预训练参数足够接近；</li>
  <li><strong>Layer Transfer</strong>：只微调网络的一部分层，冻结其他层的参数。对于图像任务，固定浅层网络，训练深层网络；对于语音任务，固定深层网络，训练浅层网络。</li>
</ul>

<h1 id="2-multitask-learning">2. Multitask Learning</h1>
<p><strong>多任务学习（Multitask Learning）</strong>是指同时学习<strong>source domain</strong>和<strong>target domain</strong>的任务，不同的任务可以共享一部分模型结构。</p>

<p><img src="https://pic.downk.cc/item/5efc5e3014195aa594117f34.jpg" alt="" /></p>

<p>模型也可采用<strong>渐进神经网络（Progressive Neural Networks）</strong>结构，每次训练一个新的模型解决新的问题，也会用到之前训练好的模型结构：</p>

<p><img src="https://pic.downk.cc/item/5efc5ec614195aa59411b022.jpg" alt="" /></p>

<h1 id="3-domain-adaptation">3. Domain Adaptation</h1>
<p><strong>Domain Adaptation</strong>是指通过构造合适的特征提取模型，使得<strong>source domain</strong>和<strong>target domain</strong>的特征在相同的特征空间中，再用这些特征解决下游任务。</p>

<p><img src="https://pic.downk.cc/item/5efd6eec14195aa59476829f.jpg" alt="" /></p>

<p>本文主要讨论<strong>homogeneous</strong>的<strong>Domain Adaptation</strong>问题，即原问题和目标问题属于同一类问题（以图像分类为例）。</p>

<p>常用方法：</p>
<ol>
  <li>Discrepancy-based methods</li>
  <li>Adversarial-based methods</li>
  <li>Reconstruction-based methods</li>
</ol>

<h2 id="1discrepancy-based-methods">（1）Discrepancy-based methods</h2>
<p>基于差异的方法是指，通过直接训练模型使得<strong>source domain</strong>和<strong>target domain</strong>特征向量足够接近:</p>

<p><img src="https://pic.downk.cc/item/5efd6f3414195aa59476a72f.jpg" alt="" /></p>

<p>常用的方法包括：</p>
<ol>
  <li>Deep Domain Confusion (MMD)</li>
  <li>Deep Adaptation Networks</li>
  <li>CORAL, CMD</li>
</ol>

<h3 id="deep-domain-confusion">Deep Domain Confusion</h3>
<ul>
  <li>paper：Deep Domain Confusion: Maximizing for Domain Invariance</li>
</ul>

<p>通过训练，减小<strong>source data</strong>的分类误差，以及<strong>source data</strong>和<strong>target data</strong>的特征向量之间的差别：</p>

<p><img src="https://pic.downk.cc/item/5efd709c14195aa5947736c9.jpg" alt="" /></p>

<p>特征向量的差距也称为<strong>Maximum Mean Discrepancy（MMD）</strong>。</p>

<h3 id="deep-adaptation-networks">Deep Adaptation Networks</h3>
<ul>
  <li>paper：Learning Transferrable Features with Deep Adaptation Networks</li>
</ul>

<p><strong>Deep Adaptation Networks</strong>在计算特征差别时，使用了多层特征：</p>

<p><img src="https://pic.downk.cc/item/5efd712e14195aa594777261.jpg" alt="" /></p>

<h3 id="coral-cmd">CORAL, CMD</h3>
<p>之前计算特征向量的差别使用的是一阶矩（绝对值），<strong>CORAL</strong>使用二阶矩，<strong>CMD</strong>使用高阶矩。</p>

<h2 id="2adversarial-based-methods">（2）Adversarial-based methods</h2>
<p>基于对抗的方法是指，训练模型得到<strong>source domain</strong>和<strong>target domain</strong>的特征向量，再训练一个<strong>domain classifier</strong>区分特征属于哪个domain，采用对抗的方法训练整个模型：</p>

<p><img src="https://pic.downk.cc/item/5efd724914195aa59477e8fd.jpg" alt="" /></p>

<p>常用的方法包括：</p>
<ol>
  <li>Simultaneous Deep Transfer Across Domains and Tasks</li>
  <li>DANN</li>
  <li>PixelDA</li>
</ol>

<h3 id="simultaneous-deep-transfer-across-domains-and-tasks">Simultaneous Deep Transfer Across Domains and Tasks</h3>
<ul>
  <li>paper：Simultaneous Deep Transfer Across Domains and Tasks</li>
</ul>

<p><img src="https://pic.downk.cc/item/5efd732314195aa594783be6.jpg" alt="" /></p>

<p>该模型的损失函数包括：</p>
<ol>
  <li>Classification Loss：最终的分类损失；</li>
  <li>Domain Confusion Loss：交替训练，一方面希望训练一个好的domain分类器，另一方面希望特征骗过domain分类器；</li>
  <li>Label Correlation Loss：希望<strong>target domain</strong>的特征含有更多信息，采用引入温度T的soft分类。</li>
</ol>

<h3 id="dann">DANN</h3>
<ul>
  <li>paper：Domain Adversarial Training of Neural Networks</li>
</ul>

<p><img src="https://pic.downk.cc/item/5efd750814195aa59478e6a0.jpg" alt="" /></p>

<p>模型包括三部分：</p>
<ol>
  <li><strong>feature extractor</strong>：目标是最大化label分类精度，最小化domain分类精度；</li>
  <li><strong>label predictor</strong>：目标是最大化label分类精度</li>
  <li><strong>domain classifier</strong>：目标是最大化domain分类精度。</li>
</ol>

<p>模型在训练时，循环进行：</p>
<ol>
  <li>从<strong>source data</strong>中抽样，训练<strong>label predictor</strong>和<strong>domain classifier</strong>；</li>
  <li>从<strong>target data</strong>中抽样，训练<strong>domain classifier</strong>；</li>
  <li>对<strong>domain classifier</strong>的梯度进行<strong>梯度反转（gradient reversal）</strong>，更新<strong>feature extractor</strong>。</li>
</ol>

<h3 id="pixelda">PixelDA</h3>
<ul>
  <li>paper：Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</li>
</ul>

<p><strong>PixelDA</strong>的想法是先训练GAN，喂入<strong>source data</strong>生成<strong>target data</strong>，再用<strong>source data</strong>和其对应生成的<strong>target data</strong>作为同一类训练分类器。</p>

<p><img src="https://pic.downk.cc/item/5efd780214195aa5947a0498.jpg" alt="" /></p>

<h2 id="3reconstruction-based-methods">（3）Reconstruction-based methods</h2>
<p>基于重构的方法是指，要求训练模型得到<strong>source domain</strong>和<strong>target domain</strong>的特征向量足够接近，并且通过解码器能够恢复各自的图像：</p>

<p><img src="https://pic.downk.cc/item/5efd78ea14195aa5947a5779.jpg" alt="" /></p>

<p>常用的方法包括：</p>
<ol>
  <li>Domain Separation Networks</li>
</ol>

<h3 id="domain-separation-networks">Domain Separation Networks</h3>

<p><img src="https://pic.downk.cc/item/5efd793614195aa5947a72eb.jpg" alt="" /></p>

<p>模型训练两个私有的编码器和两个共享参数的编码器，<strong>Private encoder</strong>提取<strong>domain-specific</strong>特征；<strong>Shared encoder</strong>提取<strong>domain-invariant</strong>特征。</p>

<p>对于每个domain，将提取的两种特征结合起来通过解码器恢复图像；用<strong>domain-invariant</strong>特征解决下游任务。</p>

<h1 id="4-zero-shot-learning">4. Zero-shot Learning</h1>
<p>下面以图像分类任务为例介绍<strong>Zero-shot Learning</strong>方法。</p>

<h3 id="attribute-embedding">Attribute embedding</h3>

<p>人为构造一系列<strong>属性attribute</strong>，将<strong>source data</strong>的每一个类别标记为一个属性向量：</p>

<p><img src="https://pic.downk.cc/item/5efc62c614195aa594130c93.jpg" alt="" /></p>

<p>属性向量生成可以用一个神经网络实现，即把数据$x_n$的类别标签$y_n$喂入网络得到属性向量$g(y_n)$。</p>

<h3 id="zero-loss">zero loss</h3>

<p>通过训练网络把<strong>source data</strong>转化成对应的属性向量，假设网络的预测结果为$f(x_n)$，对应的属性向量为$g(y_n)$，希望样本$x_n$经过网络得到的输出和其类别对应的属性向量为$g(y_n)$足够接近，和其余类别的属性向量相差很大，相差一个超参数$k$：</p>

\[f(x_n)g(y_n) - \mathop{\max}_{m≠n} f(x_n)g(y_m) &gt; k\]

<p>由此定义<strong>zero loss</strong>：</p>

\[loss = max(0,k-f(x_n)g(y_n) + \mathop{\max}_{m≠n} f(x_n)g(y_m))\]

<p>两个网络$f$和$g$可以一起训练：</p>

\[f^*,g^* = \mathop{\arg \min}_{f,g} \sum_{n}^{} {max(0,k-f(x_n)g(y_n) + \mathop{\max}_{m≠n} f(x_n)g(y_m))}\]

<p>训练好网络，进行迁移时，将没有标签的<strong>target data</strong>喂入网络得到其对应的属性向量，与已有类别的属性向量进行比较，按照最相近的结果进行分类。</p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/05/23/image_translation.html" class="read-next-link"></a>
        <section>
          <span>图像到图像翻译(Image-to-Image Translation)</span>
          <p>  Image-to-Image Translation.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/63970e69b1fccdcd367c01dd.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/05/21/lifelong-learning.html" class="read-next-link"></a>
          <section>
            <span>Lifelong Learning：终身学习</span>
            <p>  Lifelong Learning.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5eec25c514195aa59496e904.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
