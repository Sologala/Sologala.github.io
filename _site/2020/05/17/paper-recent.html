<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recent Advances in Deep Learning for Object Detection - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Recent Advances in Deep Learning for Object Detection">
    <meta name="keywords"  content="论文阅读">
    <!-- Open Graph -->
    <meta property="og:title" content="Recent Advances in Deep Learning for Object Detection - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/05/17/paper-recent.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB" class="post-tag">论文阅读</a>
          
        
      </div>
      <h1>Recent Advances in Deep Learning for Object Detection</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-05-17"><i class="iconfont icon-date"></i>17 May 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5ec12e09c2a9a83be55f5d45.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>（1908）一篇目标检测综述.</p>
</blockquote>

<ul>
  <li>arXiv：<a href="https://arxiv.org/abs/1908.03673?context=cs.CV">https://arxiv.org/abs/1908.03673?context=cs.CV</a></li>
</ul>

<p><strong>目标检测（object detection）</strong>旨在在一张图像中寻找所有可能目标的精确位置并对每一个目标进行分类。</p>

<p>本文的三个主要部分：</p>
<ol>
  <li>检测组件 <strong>detection components</strong></li>
  <li>学习策略 <strong>learning strategy</strong></li>
  <li>应用和基准 <strong>applications &amp; benchmarks</strong></li>
</ol>

<p>本文的<strong>方法论 methodology</strong>：
<img src="https://pic.downk.cc/item/5ec12f3fc2a9a83be5608f99.jpg" alt="" /></p>

<p><strong>目录</strong>：</p>
<ol>
  <li>Introduction</li>
  <li>Problem Settings</li>
  <li>Detection Components</li>
  <li>Learning Strategy</li>
  <li>Applications</li>
  <li>Benchmarks</li>
  <li>Future</li>
</ol>

<h1 id="1-introduction">1. Introduction</h1>

<h3 id="1计算机视觉的任务">(1)计算机视觉的任务</h3>

<p>计算机视觉领域的几个<strong>基础视觉任务</strong>：</p>
<ul>
  <li>图像分类 <strong>image classification</strong></li>
  <li>目标检测 <strong>object detection</strong></li>
  <li>语义分割 <strong>semantic segmentation</strong></li>
  <li>实例分割 <strong>instance segmentation</strong></li>
</ul>

<p><img src="https://pic.downk.cc/item/5ec12f84c2a9a83be560ce0d.jpg" alt="" /></p>

<p>计算机视觉领域的几个<strong>进阶视觉任务</strong>：</p>
<ul>
  <li>人脸识别 <strong>face recognation</strong></li>
  <li>行人检测 <strong>pedestrian detection</strong></li>
  <li>视频分析 <strong>video detection</strong></li>
  <li>标志检测 <strong>logo detection</strong></li>
</ul>

<h3 id="2传统的目标检测方法">(2)传统的目标检测方法</h3>

<p>传统的目标检测<strong>流水线(pipeline)</strong>：</p>
<ul>
  <li><strong>区域生成 proposal generation</strong>：寻找图像中可能存在目标的区域（<strong>Region of Interest，RoI</strong>）；
    <ol>
      <li>Sliding windows</li>
      <li>multi-scale images</li>
      <li>multi-scale windows</li>
    </ol>
  </li>
  <li><strong>特征向量提取 feature vector extraction</strong>：使用低级的特征描述子（<strong>low-level feature descriptor</strong>）抽取特征；
    <ol>
      <li>SIFT（scale invariant feature transform）</li>
      <li>Haar</li>
      <li>HOG（histogram of gradient）</li>
      <li>SURF（speeded up robust features）</li>
    </ol>
  </li>
  <li><strong>区域分类 region classification</strong>：对每一个区域进行分类。
    <ol>
      <li>SVM</li>
      <li>bagging</li>
      <li>cascade learning</li>
      <li>adaboost</li>
    </ol>
  </li>
</ul>

<p>传统的目标检测<strong>算法</strong>：</p>
<ul>
  <li>DPM（deformable part-based machine）</li>
</ul>

<p>传统目标检测的<strong>缺陷</strong>：</p>
<ul>
  <li>区域生成时：产生大量<strong>false positive</strong>；人工选择的<strong>window size</strong>很难匹配目标；</li>
  <li>特征描述子基于低级的视觉信息，很难捕捉复杂的语义信息；</li>
  <li>检测的每一步是独立设计和优化的，不能获得整个系统的全局最优解。</li>
</ul>

<h3 id="3深度学习的目标检测方法">(3)深度学习的目标检测方法</h3>
<p>基于深度学习的目标检测发展：</p>

<p><img src="https://pic.downk.cc/item/5ec12f9bc2a9a83be560e599.jpg" alt="" /></p>

<p>上图红色表示<strong>anchor-free</strong>方法，绿色表示<strong>AutoML</strong>方法，是未来目标检测的发展方向。</p>

<p>深度学习的目标检测方法<strong>优势</strong>：</p>
<ul>
  <li>卷积神经网络强大的<strong>特征表示 feature representation</strong> 能力；</li>
  <li>可以实现<strong>端到端 end to end</strong> 的优化。</li>
</ul>

<p>目标检测方法的分类：</p>
<ul>
  <li><strong>两阶段 two-stage</strong>检测器：生成（稀疏的）感兴趣的区域并抽取特征，对每个区域进行分类；</li>
  <li><strong>单阶段 one-stage</strong>检测器：对特征映射 <strong>feature map</strong> 的每一个位置进行目标的类别预测。</li>
</ul>

<h1 id="2-problem-settings">2. Problem Settings</h1>
<p>目标检测的两个主要问题：</p>
<ul>
  <li><strong>识别 recognation</strong>：对图像中存在的目标进行分类；</li>
  <li><strong>定位 localization</strong>：确定图像中目标的具体位置。</li>
</ul>

<p>两种检测设置：</p>
<ul>
  <li>边界框 <strong>bounding box level (bbox-level)</strong></li>
  <li>像素掩膜 <strong>pixel mask level (mask-level)</strong></li>
</ul>

<p>假设数据集有$N$张图像\(\{x_1,x_2,...,x_N\}\)，对应标签\(\{y_1,y_2,...,y_N\}\)，</p>

<p>记第$i$张图像内有$M_i$个目标，分别属于$C$类中的某一类：</p>

\[y_i = \{ (c_1^i,b_1^i),(c_2^i,b_2^i),...,(c_{M_i}^i,b_{M_i}^i) \}\]

<p>其中\(c_j^i \in C\)表示类别，\(b_j^i\)表示边界框或像素掩膜参数。</p>

<p>设检测系统$f$的参数为$θ$，检测结果如下：</p>

\[y_{pred}^i = \{ (c_{pred1}^i,b_{pred1}^i),(c_{pred2}^i,b_{pred2}^i),... \}\]

<p>目标检测问题的损失函数可以写作：</p>

\[l(x,θ) = \frac{1}{N} \sum_{i=1}^{N} {l(y_{pred}^i,y_i,x_i;θ)} + \frac{λ}{2} \mid\mid θ \mid\mid^2\]

<p>评估时，定义<strong>交并比（Intersection of Union，IoU）</strong>衡量预测边界框或像素掩膜与 $ground$ $truth$ 之间的差异：</p>

\[IoU(b_{pred},b_{gt}) = \frac{Area(b_{pred} ∩ b_{gt})}{Area(b_{pred} ∪ b_{gt})}\]

<p>最终预测只有类别预测正确并且交并比超过阈值$\Omega$才认为是正确的结果：</p>

\[Prediction = \begin{cases} Positive, &amp; c_{pred}=c_{gt} \text{ and } IoU(b_{pred},b_{gt}) &gt; \Omega \\ Negative, &amp; otherwise  \end{cases}\]

<h1 id="3-detection-components">3. Detection Components</h1>

<h2 id="31-detection-settings">3.1. Detection Settings</h2>
<p>目标检测的两种检测<strong>设置</strong>：</p>
<ol>
  <li>传统的目标检测：边界框定位（<strong>bbox-level</strong>）</li>
  <li>实例分割：像素掩膜定位（<strong>pixel-level &amp; mask-level</strong>）</li>
</ol>

<h2 id="32-detection-paradigms">3.2. Detection Paradigms</h2>
<p>目标检测的两种<strong>范式</strong>：</p>
<ol>
  <li><strong>two-stage detector</strong>：首先生成稀疏的候选区域，再对区域进行分类预测；</li>
  <li><strong>one-stage detector</strong>：把图像的任何区域都看作候选区域，区分其为目标或背景。</li>
</ol>

<h3 id="321-two-stage-detectors">3.2.1 Two-stage Detectors</h3>
<p>两阶段的目标检测把检测任务分成两步：</p>
<ol>
  <li>区域生成（<strong>proposal generation</strong>）；</li>
  <li>对生成的区域进行预测（<strong>proposal prediction</strong>）。</li>
</ol>

<p>典型的两阶段目标检测器：</p>
<ul>
  <li><strong>R-CNN</strong>
    <ol>
      <li>proposal generation：通过Selective Search生成稀疏的proposal集（2000个左右）；</li>
      <li>feature extraction：使用卷积网络对resized的proposal提取4096维特征；</li>
      <li>region classification：用one-vs-all SVM进行区域分类；</li>
      <li>bbox regression：用提取的特征进行边界框的回归。</li>
      <li>缺点：卷积网络重复计算耗时，各步骤独立优化，RoI选取仅使用低级图像特征。</li>
    </ol>
  </li>
  <li><strong>SPP-net</strong>
    <ol>
      <li>先通过一次卷积网络生成特征映射，在特征映射上寻找对应的proposal；</li>
      <li>把proposal的cropping改为空间金字塔池化。</li>
      <li>缺点：训练是多阶段的（需要额外的缓存），不能end to end，SPP层不能反向传播。</li>
    </ol>
  </li>
  <li><strong>Fast R-CNN</strong>
    <ol>
      <li>使用RoI Pooling提取区域特征（相当于单层SPP）；</li>
      <li>特征提取、区域分类和边界框回归end to end训练，多任务学习multi-task learning；</li>
    </ol>
  </li>
  <li><strong>Faster R-CNN</strong>
    <ol>
      <li>提出新的区域生成方法：Region Proposal Network（RPN），在特征映射上滑动窗口，在每个位置生成特征向量；</li>
      <li>完全实现数据驱动的end to end学习。</li>
    </ol>
  </li>
  <li><strong>R-FCN</strong>
    <ol>
      <li>对特征映射用1×1卷积生成位置敏感得分映射position sensitive score map；</li>
      <li>使用位置敏感的RoI Pooling提取特征。</li>
    </ol>
  </li>
  <li><strong>FPN</strong>
    <ol>
      <li>浅层特征映射空间信息强，语义信息弱；深层特征则相反；</li>
      <li>提出了特征金字塔网络，使用多层次的特征映射。</li>
    </ol>
  </li>
</ul>

<p><img src="https://pic.downk.cc/item/5ebb92cdc2a9a83be5028232.jpg" alt="" /></p>

<h3 id="322-one-stage-detectors">3.2.2 One-stage Detectors</h3>
<p>单阶段的目标检测把图像中的每个区域看作潜在的RoI，识别其为背景还是物体。</p>

<p>典型的单阶段目标检测器：</p>
<ul>
  <li><strong>OverFeat</strong>
    <ol>
      <li>用卷积网络实现滑动窗口；</li>
      <li>使用多尺度的输入图像检测多尺度目标。</li>
      <li>缺点：分类和回归的训练是分开的。</li>
    </ol>
  </li>
  <li><strong>YOLO</strong>：
    <ol>
      <li>把图像变成7×7的特征映射，每个栅格grid cell存储是否存在目标、边界框信息和目标的类别；</li>
      <li>end to end的单阶段网络，速度快；</li>
      <li>缺点：每个栅格只能检测两个目标，只使用了最后一层特征映射，无法检测多尺度目标。</li>
    </ol>
  </li>
  <li><strong>SSD</strong>
    <ol>
      <li>引入了anchor；</li>
      <li>在多个特征映射上预测目标；</li>
      <li>使用hard negative mining避免过多negative样本。</li>
    </ol>
  </li>
  <li><strong>RetinaNet</strong>
    <ol>
      <li>引入focal loss解决类别不平衡问题.</li>
    </ol>
  </li>
  <li><strong>YOLOv2</strong>
    <ol>
      <li>用k-means预设anchor大小；</li>
      <li>多尺度的训练技巧。</li>
    </ol>
  </li>
  <li><strong>CornerNet</strong>
    <ol>
      <li>anchor-free的方法</li>
      <li>class heatmap：计算该像素属于corner的概率；</li>
      <li>corner offset：计算corner的偏置；</li>
      <li>pair embedding：把同一个物体的corner配对。</li>
    </ol>
  </li>
</ul>

<p><img src="https://pic.downk.cc/item/5ebb98b7c2a9a83be5125a86.jpg" alt="" /></p>

<h2 id="33-backbone-architecture">3.3. Backbone Architecture</h2>
<p>在图像分类任务上预训练的网络能够提供更丰富的语义信息，有利于检测任务。</p>

<p>常用的卷积神经网络结构：</p>
<ul>
  <li><strong>VGG16</strong>：五组卷积层（2+2+3+3+3）和三层全连接层；</li>
  <li><strong>ResNet</strong>：引入<strong>shortcut connection</strong>，把浅层特征加到深层特征上；</li>
  <li><strong>DenseNet</strong>：把所有浅层特征连接到深层特征上；</li>
  <li><strong>Dual Path Network(DPN)</strong>：结合ResNet和Densenet，一部分特征相加，一部分特征连接；</li>
  <li><strong>ResNeXt</strong>：引入组卷积，若干个通道分成一组；</li>
  <li><strong>MobileNet</strong>：每个通道为一组的组卷积，适用于移动端；</li>
  <li><strong>GoogLeNet</strong>：引入<strong>inception</strong>，增加模型宽度。</li>
</ul>

<p>直接把分类任务训练的网络迁移到检测任务，会有一些问题：</p>
<ol>
  <li>分类任务得到的特征映射感受野较大，分辨率低；而检测任务要求相反；</li>
  <li>分类任务使用单个特征映射，检测任务可以使用多个。</li>
</ol>

<p>为检测设计的卷积神经网络：</p>
<ul>
  <li><strong>DetNet</strong>：使用空洞卷积增加感受野的同时保持较高的分辨率，同时使用多尺度的特征映射。</li>
  <li><strong>Hourglass Network</strong>：最初为人体姿态估计设计。</li>
</ul>

<h2 id="34-proposal-generation">3.4. Proposal Generation</h2>
<p><strong>Proposal</strong>是指图像中可能含有目标的区域，被用于下一步的分类和重定位。</p>
<ul>
  <li>两阶段的方法生成的proposal是稀疏的，仅仅包含前景或背景信息；</li>
  <li>单阶段的方法生成的proposal是稠密的，把图像中每一个可能的位置看作候选区域。</li>
</ul>

<p>区域生成的方法主要有以下四种：</p>
<ol>
  <li>传统的计算机视觉方法</li>
  <li>基于anchor的方法</li>
  <li>基于关键点的方法</li>
  <li>其他方法</li>
</ol>

<h3 id="1traditional-cv-methods">(1)traditional CV methods</h3>
<p>传统计算机视觉的方法借助<strong>low-level</strong>的特征生成区域，如边缘、角点或颜色。</p>

<p>主要有三种实现策略：</p>
<ul>
  <li>计算边界框的<strong>目标得分 objectness score</strong>：对每一个可能的边界框计算是否含有目标的得分。</li>
  <li><strong>超像素融合 superpixels merging</strong>：先对图像进行分割，然后对分割区域进行融合，如<strong>Selective Search</strong>算法；</li>
  <li>生成前景和背景的<strong>分割 seed segmentation</strong>，如<strong>CPMC</strong>算法。</li>
</ul>

<p><strong>优点</strong>：</p>
<ul>
  <li>算法简单</li>
  <li>召回率 recall 高</li>
</ul>

<p><strong>缺点</strong>：</p>
<ul>
  <li>主要借助low-level特征</li>
  <li>不能和整个检测模型一起优化</li>
</ul>

<h3 id="2anchor-based-methods">(2)anchor-based methods</h3>
<p>基于<strong>anchor</strong>的方法，引入了基于预定义的<strong>anchor</strong>作为先验生成候选区域，从而不需要直接预测边界框，而是预测边界框的偏差。</p>

<p><strong>i. Region Proposal Network(RPN)</strong></p>

<ol>
  <li>在特征映射上使用3×3的卷积核滑动窗口；</li>
  <li>在每个位置，使用k个anchor（不同的size和aspect ratio）；</li>
  <li>每一个anchor提取256维特征向量，喂入两个分支：分类层和回归层；</li>
  <li>分类层给出是否为目标的得分，用来判断anchor是目标还是背景；</li>
  <li>回归层给出边界框位置的修正。</li>
</ol>

<p><img src="https://pic.downk.cc/item/5ebe4cc0c2a9a83be5923e6d.jpg" alt="" /></p>

<p><strong>ii. SSD</strong></p>

<p>与RPN不同，前者分类时只判断anchor是否含有目标，具体是哪类目标需要依靠后续网络。</p>

<p>SSD的每个anchor区分具体是哪个目标，给出所有类别的置信概率。</p>

<p><strong>iii. RefineDet</strong></p>

<p>RefineDet把anchor的边界框回归分成两步：</p>

<p>第一步，使用手工设计的anchor，学习边界框的偏移；</p>

<p>第二步，使用上一步修正的anchor，学习边界框的偏移。</p>

<p><strong>iv. Cascade R-CNN</strong></p>

<p>使用一系列的anchor修正。</p>

<h3 id="3keypoint-based-methods">(3)keypoint-based methods</h3>
<p>基于关键点的方法又可以分为：</p>
<ul>
  <li><strong>Corner-based method</strong></li>
  <li><strong>Center-based method</strong></li>
</ul>

<p><strong>i. Denet</strong></p>

<p>Denet是一种基于corner的方法，对于特征映射上的每一点，判断其为（top-left、top-right、bottom-left、bottom-right）corner的概率。</p>

<p><strong>ii. CornerNet</strong></p>

<p>CornerNet也是基于corner的方法，提出了corner pooling预测top-left和bottom-right的corner。</p>

<p><strong>iii. CenterNet</strong></p>

<p>CenterNet结合了corner和center的方法，一方面预测corner，一方面预测center，从而减少了FP。</p>

<h3 id="4other-methods">(4)other methods</h3>

<p><strong>i. AZNet</strong></p>

<p>AZNet递归的把图像划分成一系列更小的图像区域，对每个区域，AZNet计算两个值：</p>
<ul>
  <li><strong>zoom indicator</strong>：决定是否把这个区域继续划分</li>
  <li><strong>adjacency score</strong>：计算目标得分</li>
</ul>

<h2 id="35-feature-representation-learning">3.5. Feature Representation Learning</h2>
<p><strong>特征表示学习</strong>旨在把输入图像转变为有利于后续任务的特征映射，主要包括四种方法：</p>
<ol>
  <li>multi-scale feature learning</li>
  <li>region feature encoding</li>
  <li>contexture reasoning</li>
  <li>deformable feature learning</li>
</ol>

<h3 id="1multi-scale-feature-learning">(1)multi-scale feature learning</h3>
<p>卷积神经网络的<strong>浅层</strong>特征：</p>
<ul>
  <li>丰富的空间特征 spatial-rich；</li>
  <li>更高分辨率 resolution；</li>
  <li>感受野较小；</li>
  <li>适合检测小目标。</li>
</ul>

<p>卷积神经网络的<strong>深层</strong>特征：</p>
<ul>
  <li>丰富的语义特征 semantic-rich；</li>
  <li>分辨率较低；</li>
  <li>更大感受野；</li>
  <li>适合检测大目标。</li>
</ul>

<p>多尺度特征学习主要有四种方法：</p>
<ol>
  <li>Image Pyramid</li>
  <li>Prediction Pyramid</li>
  <li>Integrated Features</li>
  <li>Feature Pyramid</li>
</ol>

<p><img src="https://pic.downk.cc/item/5ebf8c7fc2a9a83be5c9c37b.jpg" alt="" /></p>

<p><strong>1. Image Pyramid</strong></p>

<p>把图像缩放成不同的尺度，每个尺度训练一个检测器，最后将结果融合。</p>

<p>代表模型：Scale Normalization for Image Pyramid（SNIP）</p>

<p><strong>2. Prediction Pyramid</strong></p>

<p>使用多层的特征映射分别检测不同尺度的目标，代表模型：</p>
<ul>
  <li>SSD</li>
  <li>Multi-scale Deep Convolutional Neural Network（MSCNN）：使用转置卷积对深层特征进行上采样；</li>
  <li>Receptive Field Block Net（RFBNet）：使用RBF block。</li>
</ul>

<p><strong>3. Integrated Features</strong></p>

<p>结合网络不同层的特征映射构建一个新的特征映射，代表模型：</p>
<ul>
  <li>Inside-Outside Network（ION）：用RoI Pooling裁剪不同的特征，然后结合起来；</li>
  <li>HyperNet：使用转置卷积对深层特征进行上采样；</li>
  <li>Multi-scale Location-aware Kernel Representation（MLKP）：捕捉各个特征的高阶统计量。</li>
</ul>

<p><strong>4. Feature Pyramid</strong></p>

<p>结合特征融合与预测金字塔，代表模型：RPN</p>

<h3 id="2region-feature-encoding">(2)region feature encoding</h3>
<p>对于两阶段的检测器，区域特征编码将提取到的proposal编码为固定尺寸的特征向量。</p>
<ol>
  <li>RoI Pooling：把proposal分成$n×n$（$n$默认取$7$）的子区域，分别进行最大池化；</li>
  <li>RoI Warping：使用双线性插值调整proposal尺寸；</li>
  <li>RoI Align：使用双线性插值考虑misalignment</li>
  <li>Precies RoI Pooling</li>
  <li>Position Sensitive RoI Pooling（PSRoI Pooling）：见R-FCN网络</li>
  <li>CoupleNet：结合RoI Pooling和PSRoI Pooling；</li>
  <li>Deformable RoI Pooling：学习每一个栅格的offset。</li>
</ol>

<h3 id="3contexture-reasoning">(3)contexture reasoning</h3>
<p>有时图像中的语境信息对目标检测也很重要。<strong>语境推理</strong>主要有两种：</p>
<ol>
  <li>global context reasoning</li>
  <li>region context reasoning</li>
</ol>

<p><strong>1. global context reasoning</strong></p>
<ul>
  <li>ION：用RNN编码上下文信息；</li>
  <li>Detection with Enriched Semantics（DES）：用一个分割的mask预测上下文信息。</li>
</ul>

<p><strong>2. region context reasoning</strong></p>
<ul>
  <li>Spatial Memory Network（SMN）：引入空间记忆力模块；</li>
  <li>Structure Inference Network（SIN）：把目标检测看作图推理任务。</li>
</ul>

<h3 id="4deformable-feature-learning">(4)deformable feature learning</h3>
<ul>
  <li>deformable-aware pooling</li>
  <li>deformable convolutional layer</li>
</ul>

<h1 id="4-learning-strategy">4. Learning Strategy</h1>

<h2 id="41-训练阶段">4.1. 训练阶段</h2>

<h3 id="1data-augmentation">(1)Data Augmentation</h3>
<ul>
  <li>两阶段：horizontal flip</li>
  <li>单阶段：rotation、random crop、expand、color jittering</li>
</ul>

<h3 id="2imbalance-sampling">(2)Imbalance Sampling</h3>
<p>目标检测中的<strong>正负样本不平衡</strong>问题非常显著。意味着大部分样本都是负样本（背景）。</p>

<ul>
  <li><strong>hard negative sampling</strong>：固定正负样本的比例，负样本使用分类损失高的hard样本。</li>
  <li><strong>focal loss</strong>：越容易正确分类的样本权重越小，从而抑制其梯度：</li>
</ul>

\[L_{FL} = α (1-p_{gt})^γ log(p_{gt})\]

<ul>
  <li><strong>gradient harmonizing mechanism(GHM)</strong>：抑制容易识别的样本，避免outlier</li>
  <li><strong>online hard negative mining</strong>：只考虑分类的困难度，忽略类别信息</li>
</ul>

<h3 id="3localization-refinement">(3)Localization Refinement</h3>
<p>边界框回归能够提高检测精度。</p>

<ul>
  <li><strong>smooth L1 regressor</strong>：</li>
</ul>

\[L_{reg}(pred,label) = \sum_{i \in \{ x,y,h,w \}}^{} {SmoothL1(pred_i-label_i)}\]

\[SmoothL1(x) = \begin{cases} 0.5x^2, &amp; \mid x \mid &lt; 1 \\ \mid x \mid -0.5, &amp; otherwise \end{cases}\]

<ul>
  <li><strong>LocNet</strong>：建模每个边界框的分布</li>
  <li><strong>Multi-Path Network</strong>：使用多个IoU阈值不同的分类器</li>
  <li><strong>Fitness-NMS</strong>：为IoU更大的区域设置更大的权重</li>
</ul>

<h3 id="4cascade-learning">(4)Cascade Learning</h3>
<ul>
  <li><strong>cascade region-proposal-network and fast-rcnn（CRAFT）</strong>：使用串联RPN</li>
  <li><strong>RefineDet &amp; Cascade R-CNN</strong>：使用串联边界框回归</li>
</ul>

<h3 id="5others">(5)Others</h3>
<ul>
  <li><strong>adversarial learning</strong>：使用GAN、对抗样本帮助学习</li>
  <li><strong>training from scratch</strong>：不适用分类的预训练模型</li>
  <li><strong>knowledge distillation</strong>：使用学生网络拟合教师网络</li>
</ul>

<h2 id="42-测试阶段">4.2. 测试阶段</h2>

<h3 id="1duplicate-removal">(1)Duplicate Removal</h3>
<p>预测结果中存在大量重复边界框或负样本。</p>

<ul>
  <li><strong>non maximum supression（NMS）</strong>：非极大值抑制</li>
</ul>

<p>对每一个预测类别，边界框按照置信度排序，计算得分最高的边界框$M$与其他边界框的IoU；若交并比超过阈值\(\Omega _{threshold}\)，则丢弃这些边界框(得分置零)：</p>

\[Score_B = \begin{cases} Score_B, &amp; IoU(B,M) &lt; \Omega _{threshold} \\ 0, &amp; IoU(B,M) ≥ \Omega _{threshold} \end{cases}\]

<ul>
  <li><strong>Soft-NMS</strong>：</li>
</ul>

<p>对于交并比超过阈值的边界框，得分并不是直接置零，而是采用一个连续函数$F$(如线性函数、高斯函数)衰减：</p>

\[Score_B = \begin{cases} Score_B, &amp; IoU(B,M) &lt; \Omega _{threshold} \\ F(IoU(B,M)), &amp; IoU(B,M) ≥ \Omega _{threshold} \end{cases}\]

<h3 id="2model-acceleration">(2)Model Acceleration</h3>
<p>模型加速。</p>
<ul>
  <li><strong>Light Head R-CNN</strong>：最后一层通道从1024调整为16</li>
  <li>使用更高效的backbone：mobileNet</li>
</ul>

<h3 id="3others">(3)Others</h3>
<ul>
  <li>图像金字塔 Image Pyramid</li>
  <li>Horizontal Flipping</li>
</ul>

<h1 id="5-applications">5. Applications</h1>

<h2 id="51-face-detection">5.1. Face Detection</h2>
<p>人脸检测旨在从图像中检测出人脸。</p>

<p>人脸检测与普通目标检测的区别：</p>
<ul>
  <li>人脸检测中目标的尺度范围更大；</li>
  <li>目标容易受到遮挡和干扰；</li>
  <li>人脸检测包含很强的结构信息；</li>
  <li>人脸检测通常只有一个目标类别。</li>
</ul>

<p>人脸检测的几个关注点：</p>
<ul>
  <li><strong>muti-scale feature learning</strong>：S3FD、SSH、MTCNN</li>
  <li><strong>contextual information</strong>：FDNet、CMS-RCNN、PyramidBox</li>
  <li><strong>loss function design</strong>：Face R-FCN</li>
</ul>

<h2 id="52-pedestrian-detection">5.2. Pedestrian Detection</h2>
<p>行人检测与普通目标检测的区别：</p>
<ul>
  <li>行人检测的边界框aspect radio大约是1.5，但是尺度范围变化大；</li>
  <li>目标容易受到拥挤、遮挡和干扰；</li>
  <li>存在更多hard negative example</li>
</ul>

<h1 id="6-benchmarks">6. Benchmarks</h1>
<p>介绍一些benchmarks和评估指标 evaluation metrics 。</p>

<h2 id="61-generic-detection-benchmarks">6.1. Generic Detection Benchmarks</h2>
<p>用于通用目标检测的benchmarks包括：</p>
<ul>
  <li>Pascal VOC2007：20类，训练集：验证集：测试集 = 2501：2510：5011</li>
  <li>Pascal VOC2012：20类，训练集：验证集：测试集 = 5717：5823：10991</li>
  <li>MSCOCO：80类，训练集：验证集：测试集 = 118287：5000：40670</li>
  <li>Open Image：共有600类、190万张图像，其中500类用于检测</li>
  <li>LVIS：1000类，164000张</li>
  <li>ImageNet：200类，不常用于目标检测</li>
</ul>

<p>通用目标检测的评估指标：
<img src="https://pic.downk.cc/item/5ec10128c2a9a83be5335834.jpg" alt="" /></p>

<h2 id="62-face-detection-benchmarks">6.2. Face Detection Benchmarks</h2>
<p>用于人脸检测的benchmarks包括：</p>
<ul>
  <li>WIDER FACE：32203张，训练集：验证集：测试集 = 4：1：5，有easy、medium、hard三种</li>
  <li>FDDB：2845张，经常被用作测试集</li>
  <li>PASCAL FACE：851张，经常被用作测试集</li>
</ul>

<p>人脸检测的评估指标：
<img src="https://pic.downk.cc/item/5ec1041ac2a9a83be535e998.jpg" alt="" /></p>

<h2 id="63-pedestrian-detection-benchmarks">6.3. Pedestrian Detection Benchmarks</h2>
<p>用于行人检测的benchmarks包括：</p>
<ul>
  <li>CityPersons：5000张</li>
  <li>Caltech：训练集：测试集 = 42782：4024</li>
  <li>ETH：1804张，经常被用作测试集</li>
  <li>INRIA：2120张，训练集：测试集 = 1832：288</li>
  <li>KITTI：训练集：测试集 = 7481：7518，包括两个子类：pedestrian和cyclist，有easy、moderate、hard三种</li>
</ul>

<p>行人检测的评估指标：
<img src="https://pic.downk.cc/item/5ec10549c2a9a83be5373928.jpg" alt="" /></p>

<h1 id="7-future">7. Future</h1>
<p>目标检测未来的发展趋势：</p>
<ol>
  <li>基于anchor的方法引入了很强的先验知识，anchor-free的方法需要找到有效的区域生成策略 proposal generation；</li>
  <li>如何使用更多的语境信息 contextural information；</li>
  <li>基于AutoML的检测，找到低消耗的方法；</li>
  <li>寻找更大的benchmarks</li>
  <li>low-shot目标检测，即样本很少的情况下如何检测；</li>
  <li>设计检测的backbone网络</li>
  <li>如何增加检测训练时的batch size；</li>
  <li>增量学习 incremental learning。</li>
</ol>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/05/19/anomaly-detection.html" class="read-next-link"></a>
        <section>
          <span>Anomaly Detection：异常检测</span>
          <p>  Anomaly Detection.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.downk.cc/item/5eb51281c2a9a83be56f3c55.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/05/16/reading-comprehension.html" class="read-next-link"></a>
          <section>
            <span>阅读理解</span>
            <p>  Reading Comprehension.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.downk.cc/item/5ee5dfe1c2a9a83be540edac.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
