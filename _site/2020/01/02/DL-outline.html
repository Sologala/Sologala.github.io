<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习(Deep Learning)概述 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="深度学习(Deep Learning)概述">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="深度学习(Deep Learning)概述 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/01/02/DL-outline.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>深度学习(Deep Learning)概述</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-01-02"><i class="iconfont icon-date"></i>02 Jan 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Outlines about Deep Learning.</p>
</blockquote>

<ul>
  <li>提示：请点击任意<a href="https://0809zheng.github.io/2020/01/02/DL-outline.html"><font color="Blue">高亮位置</font></a>以发现更多细节！</li>
</ul>

<p><strong>深度学习</strong>(<strong>Deep Learning</strong>)是一种以深度神经网络为工具的机器学习方法。
本文首先介绍深度神经网络的<strong>类型</strong>，其次介绍深度学习的<strong>基本组件</strong>和<strong>方法技巧</strong>，最后介绍深度学习在计算机视觉和自然语言处理等领域的<strong>应用</strong>。</p>

<p>本文目录：</p>
<ul>
  <li><strong>深度神经网络的类型</strong>
    <ol>
      <li><strong>卷积神经网络</strong>：卷积神经网络的基本概念、卷积神经网络中的池化层、卷积神经网络中的注意力机制、轻量级卷积神经网络</li>
      <li><strong>循环神经网络</strong>：循环神经网络的基本概念、序列到序列模型、序列到序列模型中的注意力机制</li>
      <li><strong>自注意力网络</strong>：自注意力机制、<strong>Transformer</strong>、<strong>Transformer</strong>中的位置编码、降低<strong>Transformer</strong>的计算复杂度、预训练语言模型、</li>
      <li><strong>深度生成模型</strong>：生成对抗网络、变分自编码器、流模型</li>
      <li><strong>其他类型的网络</strong>：递归神经网络、记忆增强神经网络、图神经网络</li>
    </ol>
  </li>
  <li><strong>深度学习的基本组件和方法技巧</strong>
    <ol>
      <li><strong>深度学习的基本组件</strong>：激活函数、优化方法、正则化方法、归一化方法、参数初始化方法</li>
      <li><strong>深度学习的方法</strong>：半监督学习、自监督学习、度量学习、多任务学习、主动学习、迁移学习</li>
      <li><strong>深度学习的技巧</strong>：长尾分布、对抗训练、大模型的参数高效微调</li>
    </ol>
  </li>
  <li><strong>深度学习的应用</strong>
    <ol>
      <li><strong>计算机视觉</strong>：图像识别、目标检测、开放集合目标检测、图像分割、图像超分辨率、图像到图像翻译、时空动作检测、人脸检测, 识别与验证、行人检测与属性识别、点云分类、目标计数</li>
      <li><strong>自然语言处理</strong>：</li>
      <li><strong>AI for Science</strong></li>
    </ol>
  </li>
</ul>

<h1 id="1-深度神经网络的类型">1. 深度神经网络的类型</h1>

<h2 id="1-卷积神经网络">(1) 卷积神经网络</h2>

<h3 id="-卷积神经网络convolutional-neural-network的基本概念">⚪ <a href="https://0809zheng.github.io/2020/03/06/CNN.html"><font color="Blue">卷积神经网络(Convolutional Neural Network)的基本概念</font></a></h3>

<p><strong>卷积神经网络</strong>是由卷积层、激活函数和池化层堆叠构成的深度神经网络，可以从图像数据中自适应的提取特征。</p>

<p><img src="https://pic.downk.cc/item/5ea54956c2a9a83be5d81c10.jpg" alt="" /></p>

<p>卷积层是一种局部的互相关操作，使用卷积核在输入图像或特征上按照光栅扫描顺序滑动，并通过局部仿射变换构造输出特征；具有局部连接、参数共享和平移等变性等特点。</p>

<p>卷积神经网络中的卷积层包括标准卷积, 转置卷积, 扩张卷积(<strong>Dilated Conv</strong>, <strong>IC-Conv</strong>), 可分离卷积(空间可分离卷积, 深度可分离卷积, 平展卷积), 组卷积, 可变形卷积, 差分卷积(中心差分卷积, 交叉中心差分卷积, 像素差分卷积), 动态卷积(<strong>CondConv</strong>, <strong>DynamicConv</strong>, <strong>DyNet</strong>, <strong>ODConv</strong>, <strong>DRConv</strong>), <strong>Involution</strong>, 圆形卷积, 八度卷积, 稀疏卷积(空间稀疏卷积, 子流形稀疏卷积), <strong>CoordConv</strong>。</p>

<h3 id="-卷积神经网络中的池化pooling层">⚪ <a href="https://0809zheng.github.io/2021/07/02/pool.html"><font color="Blue">卷积神经网络中的池化(Pooling)层</font></a></h3>

<p><strong>池化层</strong>可以对特征图进行降采样，从而减小网络的计算成本，降低过拟合的风险。卷积神经网络中的池化方法包括：</p>
<ul>
  <li>通用的池化方法：最大池化, 平均池化, 混合池化, 分数最大池化, 幂平均池化, 随机池化, 随机空间采样池化(<strong>S3Pool</strong>), 细节保留池化(<strong>DPP</strong>), 局部重要性池化(<strong>LIP</strong>), 软池化, 动态优化池化(<strong>DynOPool</strong>)</li>
  <li>为下游任务设计的池化方法：全局平均池化(<strong>GAP</strong>), 协方差池化, 空间金字塔池化(<strong>SPP</strong>), 感兴趣区域池化(<strong>RoI Pooling</strong>), 双线性池化</li>
</ul>

<h3 id="-卷积神经网络中的注意力机制attention-mechanism">⚪ <a href="https://0809zheng.github.io/2020/11/18/AinCNN.html"><font color="Blue">卷积神经网络中的注意力机制(Attention Mechanism)</font></a></h3>

<p>卷积神经网络中的<strong>注意力机制</strong>表现为在特征的某个维度上计算相应<strong>统计量</strong>，并根据所计算的统计量对该维度上的每一个元素赋予不同的权重，用以增强网络的特征表达能力。</p>

<p>卷积层的特征维度包括通道维度和空间维度，因此注意力机制可以应用在不同维度上：</p>
<ul>
  <li><strong>通道注意力(Channel Attention)</strong>：<strong>SENet</strong>, <strong>CMPT-SE</strong>, <strong>GENet</strong>, <strong>GSoP</strong>, <strong>SRM</strong>, <strong>SKNet</strong>, <strong>DIA</strong>, <strong>ECA-Net</strong>, <strong>SPANet</strong>, <strong>FcaNet</strong>, <strong>EPSA</strong>, <strong>TSE</strong>, <strong>NAM</strong></li>
  <li><strong>空间注意力(Spatial Attention)</strong>：<strong>Residual Attention Network</strong>, <strong>SGE</strong>, <strong>ULSAM</strong></li>
  <li>通道+空间：(<strong>并联</strong>)<strong>scSE</strong>, <strong>BAM</strong>, <strong>SA-Net</strong>, <strong>Triplet Attention</strong>; (<strong>串联</strong>)<strong>CBAM</strong>; (<strong>融合</strong>)<strong>SCNet</strong>, <strong>Coordinate Attention</strong>, <strong>SimAM</strong></li>
  <li>其他注意力：<strong>DCANet</strong>, <strong>WE</strong>, <strong>ATAC</strong>, <strong>AFF</strong>, <strong>AW-Convolution</strong>, <strong>BA^2M</strong>, <strong>Interflow</strong>, <strong>CSRA</strong></li>
</ul>

<h3 id="-卷积神经网络的可视化">⚪ <a href="https://0809zheng.github.io/2020/12/16/custom.html">卷积神经网络的可视化</a></h3>

<h3 id="-轻量级lightweight卷积神经网络">⚪ <a href="https://0809zheng.github.io/2021/09/10/lightweight.html"><font color="Blue">轻量级(LightWeight)卷积神经网络</font></a></h3>

<p><strong>轻量级</strong>网络设计旨在设计计算复杂度更低的卷积网络结构。</p>
<ul>
  <li>从<strong>结构</strong>的角度考虑，卷积层提取的特征存在冗余，可以设计特殊的卷积操作，减少卷积操作的冗余，从而减少计算量。如<strong>SqueezeNet</strong>, <strong>SqueezeNext</strong>, <strong>MobileNet V1,2,3</strong>, <strong>ShuffleNet V1,2</strong>, <strong>IGCNet V1,2</strong>, <strong>ChannelNet</strong>, <strong>EfficientNet V1,2</strong>, <strong>GhostNet</strong>, <strong>MicroNet</strong>, <strong>CompConv</strong>。</li>
  <li>从<strong>计算</strong>的角度，模型推理过程中存在大量乘法运算，而乘法操作(相比于加法)对于目前的硬件设备不友好，可以对乘法运算进行优化，也可以减少计算量。如<strong>AdderNet</strong>使用<strong>L1</strong>距离代替卷积乘法；使用<strong>Mitchell</strong>近似代替卷积乘法。</li>
</ul>

<h2 id="2-循环神经网络">(2) 循环神经网络</h2>

<h3 id="-循环神经网络recurrent-neural-network的基本概念">⚪ <a href="https://0809zheng.github.io/2020/03/07/RNN.html"><font color="Blue">循环神经网络(Recurrent Neural Network)的基本概念</font></a></h3>

<p><strong>循环神经网络(RNN)</strong>可以处理输入长度不固定的文本等时间序列数据。<strong>RNN</strong>每一时刻的隐状态$h_t$不仅和当前时刻的输入$x_t$相关，也和上一时刻的隐状态$h_{t-1}$相关。<strong>RNN</strong>具有通用近似性、图灵完备性等特点。</p>

<p><img src="https://pic.downk.cc/item/5e9fdc29c2a9a83be5533395.jpg" alt="" /></p>

<p><strong>RNN</strong>存在长程依赖问题：理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度消失现象，实际上只能学习到短期的依赖关系。解决措施是引入门控机制，如<strong>LSTM</strong>, <strong>GRU</strong>, <strong>QRNN</strong>, <strong>SRU</strong>, <strong>ON-LSTM</strong>。</p>

<p>也可以通过增加循环层的深度增强<strong>RNN</strong>的特征提取能力，包括<strong>Stacked RNN</strong>, <strong>Bidirectional RNN</strong>。</p>

<h3 id="-序列到序列模型-sequence-to-sequence">⚪ <a href="https://0809zheng.github.io/2020/04/21/sequence-2-sequence.html"><font color="Blue">序列到序列模型 (Sequence to Sequence)</font></a></h3>

<p><strong>序列到序列(Seq2Seq)模型</strong>是一种序列生成模型，能够根据一个随机长度的输入序列生成另一个随机长度的序列。<strong>Seq2Seq</strong>模型通常采用编码器-解码器结构：</p>

<p><img src="https://pic.imgdb.cn/item/63b431e6be43e0d30e71b68d.jpg" alt="" /></p>

<p><strong>Seq2Seq</strong>模型在生成序列时可以通过贪婪搜索或束搜索实现。序列生成时存在曝光偏差问题，可以通过计划采样缓解。</p>

<p>典型的<strong>Seq2Seq</strong>模型包括条件<strong>Seq2Seq</strong>模型、指针网络。</p>

<h3 id="-序列到序列模型中的注意力机制-attention-mechanism">⚪ <a href="https://0809zheng.github.io/2020/04/22/attention.html"><font color="Blue">序列到序列模型中的注意力机制 (Attention Mechanism)</font></a></h3>

<p>在<strong>Seq2Seq</strong>模型中，将输入序列通过编码器转换为一个上下文向量$c$，再喂入解码器。注意力机制是指在解码器的每一步中，通过输入序列的所有隐状态$h_{1:T}$构造注意力分布$(α_1,…,α_t,…,α_T)$，然后构造当前步的上下文向量$c= \sum_{t=1}^{T} {α_th_t}$。</p>

<h2 id="3-自注意力网络">(3) 自注意力网络</h2>

<h3 id="-自注意力机制-self-attention-mechanism">⚪ <a href="https://0809zheng.github.io/2020/04/24/self-attention.html"><font color="Blue">自注意力机制 (Self-Attention Mechanism)</font></a></h3>

<p><strong>自注意力机制</strong>用于捕捉单个序列$X$的内部关系。把输入序列$X$映射为查询矩阵$Q$, 键矩阵$K$和值矩阵$V$；根据查询矩阵$Q$和键矩阵$K$生成注意力图，并作用于值矩阵$V$获得自注意力的输出$H$。</p>

<p><img src="https://pic.downk.cc/item/5ea28825c2a9a83be5477d93.jpg" alt="" /></p>

<h3 id="-transformer">⚪ <a href="https://0809zheng.github.io/2020/04/25/transformer.html"><font color="Blue">Transformer</font></a></h3>

<p><strong>Transformer</strong>是一个基于多头自注意力机制的深度网络模型，网络结构包括编码器和解码器。编码器生成基于注意力的特征表示，该表示具有从全局上下文中定位特定信息的能力；解码器从特征表示中进行检索。</p>

<p><img src="https://pic.imgdb.cn/item/618b94ea2ab3f51d91f6d24e.jpg" alt="" /></p>

<h3 id="-transformer中的位置编码-position-encoding">⚪ <a href="https://0809zheng.github.io/2021/07/12/efficienttransformer.html"><font color="Blue">Transformer中的位置编码 (Position Encoding)</font></a></h3>

<p><strong>Transformer</strong>中的自注意力机制具有置换不变性(<strong>permutation invariant</strong>)，导致打乱输入序列的顺序对输出结果不会产生任何影响。通过<strong>位置编码</strong>把位置信息引入输入序列中，以打破模型的全对称性。</p>
<ul>
  <li><strong>绝对位置编码</strong>：只依赖于单一位置，将绝对位置信息加入到输入序列中，相当于引入索引的嵌入。比如<strong>Sinusoidal</strong>, <strong>Learnable</strong>, <strong>FLOATER</strong>, <strong>Complex-order</strong>, <strong>RoPE</strong></li>
  <li><strong>相对位置编码</strong>：不同位置的交互项，通过微调自注意力运算过程使其能分辨不同<strong>token</strong>之间的相对位置。比如<strong>XLNet</strong>, <strong>T5</strong>, <strong>DeBERTa</strong>, <strong>URPE</strong></li>
</ul>

<h3 id="-降低transformer的计算复杂度">⚪ <a href="https://0809zheng.github.io/2021/07/12/efficienttransformer.html"><font color="Blue">降低Transformer的计算复杂度</font></a></h3>

<p>自注意力运算中<strong>计算注意力矩阵</strong>以及<strong>加权求和计算输出</strong>这两个步骤引入了$O(N^2)$的计算复杂度。因此可以改进这两个步骤，从而降低计算复杂度。</p>
<ul>
  <li>改进注意力矩阵的计算: 这类方法的改进思路是使得注意力矩阵的计算<strong>稀疏化</strong>，即对输入序列中的每一个位置只计算其与一部分位置(而不是全部位置)之间的相关性，表现为注意力矩阵是稀疏的。如<strong>Sparse Transformer</strong>, <strong>Reformer</strong>, <strong>Longformer</strong>, <strong>Big Bird</strong>。</li>
  <li>改进输出的加权求和: 这类方法的改进思路是使得自注意力的计算<strong>线性化</strong>。如<strong>Efficient Attention</strong>, <strong>Synthesizer</strong>, <strong>Linformer</strong>, <strong>Linear Transformer</strong>, <strong>Performer</strong>, <strong>Nyströmformer</strong>, <strong>External Attention</strong>, <strong>FLASH</strong>。</li>
</ul>

<h3 id="-预训练语言模型-pretrained-language-model">⚪ <a href="https://0809zheng.github.io/2020/04/27/elmo-bert-gpt.html"><font color="Blue">预训练语言模型 (Pretrained Language Model)</font></a></h3>

<p>预训练语言模型是一种从大量无标签的语料库中学习通用的自然语言特征表示的方法。使用预训练语言模型的步骤如下：1. 在大量无标签的语料库上进行特定任务的<strong>预训练</strong>；2. 在下游任务的语料库上进行<strong>微调</strong>。</p>

<p>根据预训练的任务不同，预训练语言模型可以划分为以下几类：</p>
<ul>
  <li><strong>词嵌入(word embedding)</strong>：上下文无关的嵌入</li>
  <li><strong>概率语言建模 Language Modeling(LM)</strong>：自回归或单向语言建模，即给定前面所有词预测下一个词。如<strong>ELMo</strong>, <strong>GPT 1,2,3</strong>。</li>
  <li><strong>掩码语言建模 Masked Language Modeling(MLM)</strong>：从输入序列中遮盖一些<strong>token</strong>，然后训练模型通过其余的<strong>token</strong>预测<strong>masked token</strong>。如<strong>BERT</strong>, <strong>ALBERT</strong>, <strong>ELECTRA</strong>, <strong>REALM</strong>。</li>
  <li><strong>序列到序列的掩码语言建模 Seq2Seq Masked Language Modeling(Seq2Seq MLM)</strong>：采用编码器-解码器结构，将<strong>masked</strong>序列输入编码器，解码器以自回归的方式顺序生成<strong>masked token</strong>。如<strong>MASS</strong>, <strong>UniLM</strong>, <strong>T5</strong>, <strong>T5.1.1</strong>, <strong>mT5</strong>。</li>
  <li><strong>增强掩码语言建模 Enhanced Masked Language Modeling(E-MLM)</strong>：在掩码语言建模的过程中使用了一些增强方法。如<strong>RoBERTa</strong>, <strong>DeBERTa</strong>。</li>
  <li><strong>排列语言建模 Permuted Language Modeling(PLM)</strong>：在输入序列的随机排列上进行语言建模。如<strong>XLNet</strong>。</li>
</ul>

<h2 id="4-深度生成模型">(4) 深度生成模型</h2>

<p><strong>生成模型</strong>(<strong>generative model</strong>)是指使用带参数$\theta$的概率分布$p_{\theta}(x)$拟合已有数据样本集\(\{x\}\)。由于概率分布$p_{\theta}(x)$的形式通常是未知的，可以将其假设为离散型或连续型分布；若进一步引入<strong>隐变量(latent variable)</strong> $z$，则可以间接地构造概率分布$p_{\theta}(x)$：</p>

\[p_{\theta}(x) = \int p_{\theta}(x,z) dz = \int p_{\theta}(x|z)p(z) dz\]

<p>参数$\theta$的求解可以通过极大似然估计。若记真实数据分布为$\tilde{p}(x)$，则优化目标为最大化对数似然\(\Bbb{E}_{x\text{~}\tilde{p}(x)}[\log p_{\theta}(x)]\)。由于该算式包含积分运算，直接求解比较困难；不同的生成模型通过不同的求解技巧避开这个困难。</p>

<h3 id="-自回归模型-auto-regressive">⚪ 自回归模型 (Auto-Regressive)</h3>

<p>从最严格的角度来看，图像应该是一个离散的分布，因为它是由有限个像素组成的，而每个像素的取值也是离散的、有限的，因此可以通过离散分布来描述。这个思路的成果就是PixelRNN一类的模型了，我们称之为“自回归流”，其特点就是无法并行，所以计算量特别大。所以，我们更希望用连续分布来描述图像。当然，图像只是一个场景，其他场景下我们也有很多连续型的数据，所以连续型的分布的研究是很有必要的。</p>

<p>的本质，就是希望用一个我们知道的概率模型来拟合所给的数据样本，也就是说，我们得写出一个带参数θ的分布qθ(x)。然而，我们的神经网络只是“万能函数拟合器”，却不是“万能分布拟合器”，也就是它原则上能拟合任意函数，但不能随意拟合一个概率分布，因为概率分布有“非负”和“归一化”的要求。这样一来，我们能直接写出来的只有离散型的分布，或者是连续型的高斯分布。</p>

<h3 id="-生成对抗网络-generative-adversarial-network">⚪ <a href="https://0809zheng.github.io/2022/02/01/gan.html"><font color="Blue">生成对抗网络 (Generative Adversarial Network)</font></a></h3>

<p><strong>生成对抗网络</strong>通过交替优化的对抗训练绕开了似然的直接求解，使用生成器$G$构造真实分布的近似分布\(P_G(x)\)，并使用判别器衡量生成分布和真实分布之间的差异。</p>

\[\begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P(z)}[\log(1-D(G(z)))] \end{aligned}\]

<p>生成对抗网络的设计是集目标函数、网络结构、优化过程于一体的，<strong>GAN</strong>的各种变体也是基于对这些方面的改进：</p>
<ul>
  <li>改进目标函数：基于分布散度(如<strong>f-GAN</strong>, <strong>BGAN</strong>, <strong>Softmax GAN</strong>, <strong>RGAN</strong>, <strong>LSGAN</strong>, <strong>WGAN-div</strong>, <strong>GAN-QP</strong>, <strong>Designing GAN</strong>)、基于积分概率度量(如<strong>WGAN</strong>, <strong>WGAN-GP</strong>, <strong>DRAGAN</strong>, <strong>SN-GAN</strong>, <strong>GN-GAN</strong>, <strong>GraN-GAN</strong>, <strong>c-transform</strong>, <strong>McGAN</strong>, <strong>MMD GAN</strong>, <strong>Fisher GAN</strong>)</li>
  <li>改进网络结构：调整神经网络(如<strong>DCGAN</strong>, <strong>SAGAN</strong>, <strong>BigGAN</strong>, <strong>Self-Modulation</strong>, <strong>StyleGAN1,2,3</strong>, <strong>TransGAN</strong>)、引入编码器(如<strong>VAE-GAN</strong>, <strong>BiGAN</strong>, <strong>VQGAN</strong>)、使用能量模型(如<strong>EBGAN</strong>, <strong>LSGAN</strong>, <strong>BEGAN</strong>, <strong>MAGAN</strong>, <strong>MEG</strong>)、由粗到细的生成(如<strong>LAPGAN</strong>, <strong>StackGAN</strong>, <strong>PGGAN</strong>, <strong>SinGAN</strong>)</li>
  <li>改进优化过程：<strong>TTUR</strong>, <strong>Dirac-GAN</strong>, <strong>VDB</strong>, <strong>Cascading Rejection</strong>, <strong>ADA</strong>, <strong>Hubness Prior</strong></li>
  <li>其他应用：条件生成(如<strong>CGAN</strong>, <strong>InfoGAN</strong>, <strong>ACGAN</strong>, <strong>Projection Discriminator</strong>)、<a href="https://0809zheng.github.io/2020/05/23/image_translation.html"><font color="Blue">图像到图像翻译</font></a>(有配对数据, 如<strong>Pix2Pix</strong>, <strong>BicycleGAN</strong>, <strong>LPTN</strong>; 无配对数据, 如<strong>CoGAN</strong>, <strong>PixelDA</strong>, <strong>CycleGAN</strong>, <strong>DiscoGAN</strong>, <strong>DualGAN</strong>, <strong>UNIT</strong>, <strong>MUNIT</strong>, <strong>TUNIT</strong>, <strong>StarGAN</strong>, <strong>StarGAN v2</strong>, <strong>GANILLA</strong>, <strong>NICE-GAN</strong>, <strong>CUT</strong>, <strong>SimDCL</strong>)、超分辨率(如<strong>SRGAN</strong>, <strong>ESRGAN</strong>)、图像修补(如<strong>Context Encoder</strong>, <strong>CCGAN</strong>, <strong>SPADE</strong>)、机器学习应用(如<strong>Semi-Supervised GAN</strong>, <strong>AnoGAN</strong>, <strong>ClusterGAN</strong>)</li>
</ul>

<h3 id="-变分自编码器-variational-autoencoder">⚪ <a href="https://0809zheng.github.io/2022/04/01/vae.html"><font color="Blue">变分自编码器 (Variational Autoencoder)</font></a></h3>

<p><strong>变分自编码器</strong>的优化目标不是对数似然，而是对数似然的变分下界：</p>

\[\log p_{\theta}(x)  \geq \mathbb{E}_{z \text{~} q_{\phi}(z|x)} [\log p_{\theta}(x | z)] - KL[q_{\phi}(z|x)||p(z)]\]

<p><strong>VAE</strong>的优化目标共涉及三个不同的概率分布：由概率编码器表示的后验分布$q_{\phi}(z|x)$、隐变量的先验分布$p(z)$以及由概率解码器表示的生成分布$p_{\theta}(x|z)$。对<strong>VAE</strong>的各种改进可以落脚于对这些概率分布的改进：</p>

<ul>
  <li>后验分布$q(z|x)$：后验分布为模型引入了正则化；一种改进思路是通过调整后验分布的正则化项增强模型的解耦能力(如<strong>β-VAE</strong>, <strong>Disentangled β-VAE</strong>, <strong>InfoVAE</strong>, <strong>DIP-VAE</strong>, <strong>FactorVAE</strong>, <strong>β-TCVAE</strong>, <strong>HFVAE</strong>)。</li>
  <li>先验分布$p(z)$：先验分布描绘了隐变量分布的隐空间；一种改进思路是通过引入标签实现半监督学习(如<strong>CVAE</strong>, <strong>CMMA</strong>)；一种改进思路是通过对隐变量离散化实现聚类或分层特征表示(如<strong>Categorical VAE</strong>, <strong>Joint VAE</strong>, <strong>VQ-VAE</strong>, <strong>VQ-VAE-2</strong>, <strong>FSQ</strong>)；一种改进思路是更换隐变量的概率分布形式(如<strong>Hyperspherical VAE</strong>, <strong>TD-VAE</strong>, <strong>f-VAE</strong>, <strong>NVAE</strong>)。</li>
  <li>生成分布$p(x|z)$：生成分布代表模型的数据重构能力；一种改进思路是将均方误差损失替换为其他损失(如<strong>EL-VAE</strong>, <strong>DFCVAE</strong>, <strong>LogCosh VAE</strong>)。</li>
  <li>改进整体损失函数：也有方法通过调整整体损失改进模型，如紧凑变分下界(如<strong>IWAE</strong>, <strong>MIWAE</strong>)或引入<strong>Wasserstein</strong>距离(如<strong>WAE</strong>, <strong>SWAE</strong>)。</li>
  <li>改进模型结构：如<strong>BN-VAE</strong>通过引入<strong>BatchNorm</strong>缓解<strong>KL</strong>散度消失问题；引入对抗训练(如<strong>AAE</strong>, <strong>VAE-GAN</strong>)。</li>
</ul>

<h3 id="-流模型-flow-based-model">⚪ <a href="https://0809zheng.github.io/2022/05/01/flow.html"><font color="Blue">流模型 (Flow-based Model)</font></a></h3>

<p><strong>流模型</strong>通过一系列可逆变换(双射函数$f$)建立较为简单的先验分布$p(z)$与较为复杂的实际数据分布$p(x)$之间的映射关系：</p>

\[\begin{aligned} x&amp;=f_K \circ \cdots \circ f_1(z) \\ p(x) &amp;= p(z)\cdot |\prod_{k=1}^{K} \det J_{f_k}(z_{k-1})|^{-1} \end{aligned}\]

<p>由于流模型给出了概率分布$p(x)$的显式表达式，可直接最大化对数似然：</p>

\[\begin{aligned}  \log p(x)  = \log  p(z) - \sum_{k=1}^{K}\log  | \det J_{f_k}(z_{k-1})| \end{aligned}\]

<p>从优化目标中可以看出，流模型是由先验分布$p(z)$和双射函数$x=f(z)$唯一确定的。根据双射函数的不同设计思路，流模型分为以下两类：</p>
<ul>
  <li><strong>标准化流</strong>(<strong>Normalizing Flow</strong>)：通过数学定理与性质设计<strong>Jacobian</strong>行列式$\det J_{f}(z)$容易计算的双射函数$x=f(z)$。标准化流是最基础的流模型，事实上其他类别的流模型可以看作标准化流的延申。这类模型包括<strong>Normalizing Flow</strong>, <strong>iResNet</strong>等。</li>
  <li><strong>自回归流</strong>(<strong>Autoregressive Flow</strong>)：把双射函数$x=f(z)$建模为自回归模型，即$x$的第$i$个维度$x_i$的生成只依赖于前面的维度$x_{1:i-1}$(自回归流)或$z_{1:i-1}$(逆自回归流)，此时<strong>Jacobian</strong>矩阵$J_{f}(z)$为三角矩阵，行列式容易计算。这类模型包括<strong>IAF</strong>, <strong>MAF</strong>, <strong>NICE</strong>, <strong>Real NVP</strong>, <strong>Glow</strong>, <strong>Flow++</strong>等。</li>
</ul>

<h3 id="-扩散模型">⚪ <a href="">扩散模型</a></h3>

<h3 id="-其他生成网络">⚪ 其他生成网络</h3>

<p><a href="https://0809zheng.github.io/2022/03/27/gmmn.html">Generative Moment Matching Network</a></p>

<h2 id="4-其他类型的神经网络">(4) 其他类型的神经网络</h2>

<h3 id="-递归神经网络-recursive-neural-network">⚪ <a href="https://0809zheng.github.io/2020/03/08/recursive-neural-network.html"><font color="Blue">递归神经网络 (Recursive Neural Network)</font></a></h3>

<p><strong>递归神经网络</strong>是循环神经网络在有向无环图上的扩展，主要用来建模自然语言句子的语义。给定一个句子的语法结构（一般为树状结构），可以使用递归神经网络来按照句法的组合关系来合成一个句子的语义。句子中每个短语成分又可以分成一些子成分，即每个短语的语义都可以由它的子成分语义组合而来，并进而合成整句的语义。</p>

<p><img src="https://pic.downk.cc/item/5ea14499c2a9a83be5c09f98.jpg" alt="" /></p>

<p>典型的递归神经网络包括递归神经张量网络、矩阵-向量递归网络、<strong>Tree LSTM</strong>。</p>

<h3 id="-记忆增强神经网络-memory-augmented-neural-network">⚪ <a href="https://0809zheng.github.io/2020/04/23/memory-network.html"><font color="Blue">记忆增强神经网络 (Memory Augmented Neural Network)</font></a></h3>

<p><strong>记忆增强神经网络</strong>在神经网络中引入外部记忆单元来提高网络容量。记忆网络的模块包括：主网络$C$负责信息处理以及与外界的交互；外部记忆单元$M$用来存储信息；读取模块$R$根据主网络生成的查询向量从外部记忆单元读取信息；写入模块$W$根据主网络生成的查询向量和要写入的信息更新外部记忆单元。读取或写入操作通常使用注意力机制实现。</p>

<p><img src="https://pic.downk.cc/item/5ea1641fc2a9a83be5e6cfc3.jpg" alt="" /></p>

<p>典型的记忆增强神经网络包括端到端记忆网络、神经图灵机。</p>

<h3 id="-图神经网络-graph-neural-network">⚪ <a href="https://0809zheng.github.io/2020/03/09/graph-neural-network.html"><font color="Blue">图神经网络 (Graph Neural Network)</font></a></h3>

<p><strong>图神经网络</strong>是用于处理图结构的神经网络，其核心思想是学习一个函数映射$f(\cdot)$，图中的节点$v_i$通过该映射可以聚合它自己的特征$x_i$与它的邻居特征$x_{j \in N(v_i)}$来生成节点$v_i$的新表示。</p>

<p><img src="https://pic.downk.cc/item/5ea59bbdc2a9a83be5281d20.jpg" alt="" /></p>

<p><strong>GNN</strong>可以分为两大类，基于空间（<strong>spatial-based</strong>）和基于谱（<strong>spectral-based</strong>）。</p>
<ul>
  <li>基于空间的<strong>GNN</strong>直接根据邻域聚合特征信息，把图粗化为高级子结构，可用于提取图的各级表示和执行下游任务。如<strong>NN4G</strong>, <strong>DCNN</strong>, <strong>DGC</strong>, <strong>MoNET</strong>, <strong>GraphSAGE</strong>, <strong>GAT</strong>, <strong>GIN</strong>。</li>
  <li>基于谱的<strong>GNN</strong>把图网络通过傅里叶变换转换到谱域，引入滤波器处理图谱后通过逆变换还原到顶点域。如<strong>ChebNet</strong>, <strong>GCN</strong>, <strong>DropEdge</strong>。</li>
</ul>

<h3 id="-胶囊网络">⚪ <a href="https://0809zheng.github.io/2020/04/20/Capsule-Network.html">胶囊网络</a></h3>

<h1 id="2-深度学习的基本组件和方法技巧">2. 深度学习的基本组件和方法技巧</h1>

<h2 id="1-深度学习的基本组件">(1) 深度学习的基本组件</h2>
<h3 id="-激活函数-activation-function">⚪ <a href="https://0809zheng.github.io/2020/03/01/activation.html"><font color="Blue">激活函数 (Activation Function)</font></a></h3>
<p><strong>激活函数</strong>能为神经网络引入非线性，在设计激活函数时可以考虑的性质包括：连续可导、计算量小、没有饱和区、没有偏置偏移、具有生物可解释性、提取上下文信息、通用近似性。</p>

<p>常见的激活函数根据设计思路分类如下：</p>
<ul>
  <li><strong>S</strong>型激活函数：形如<strong>S</strong>型曲线的激活函数。包括<strong>Step</strong>，<strong>Sigmoid</strong>，<strong>HardSigmoid</strong>，<strong>Tanh</strong>，<strong>HardTanh</strong></li>
  <li><strong>ReLU</strong>族激活函数：形如<strong>ReLU</strong>的激活函数。包括<strong>ReLU</strong>，<strong>Softplus</strong>, <strong>Squareplus</strong>，<strong>ReLU6</strong>，<strong>LeakyReLU</strong>，<strong>PReLU</strong>，<strong>RReLU</strong>，<strong>ELU</strong>，<strong>GELU</strong>，<strong>CELU</strong>，<strong>SELU</strong></li>
  <li>自动搜索激活函数：通过自动搜索解空间得到的激活函数。包括<strong>Swish</strong>，<strong>HardSwish</strong>，<strong>Elish</strong>，<strong>HardElish</strong>，<strong>Mish</strong></li>
  <li>基于梯度的激活函数：通过梯度下降为每个神经元学习独立函数。包括<strong>APL</strong>，<strong>PAU</strong>，<strong>ACON</strong>，<strong>PWLU</strong>，<strong>OPAU</strong>，<strong>SAU</strong>，<strong>SMU</strong></li>
  <li>基于上下文的激活函数：多输入单输出函数，输入上下文信息。包括<strong>maxout</strong>，<strong>Dynamic ReLU</strong>，<strong>Dynamic Shift-Max</strong>，<strong>FReLU</strong></li>
</ul>

<h3 id="-优化方法-optimization">⚪ <a href="https://0809zheng.github.io/2020/03/02/optimization.html"><font color="Blue">优化方法 (Optimization)</font></a></h3>

<p>深度学习中的<strong>优化</strong>问题是指在已有的数据集上实现最小的训练误差$l(\theta)$，通常用基于梯度的数值方法求解。在实际应用梯度方法时，可以根据截止到当前步$t$的历史梯度信息\(\{g_{1},...,g_{t}\}\)计算修正的参数更新量$h_t$（比如累积动量、累积二阶矩校正学习率等）。指定每次计算梯度所使用数据批量 $\mathcal{B}$ 和学习率 $\gamma$，则第$t$次参数更新为：</p>

\[\begin{aligned} g_t&amp;=\frac{1}{\|\mathcal{B}\|}\sum_{x \in \mathcal{B}}^{}\nabla_{\theta} l(θ_{t-1}) \\ h_t &amp;= f(g_{1},...,g_{t}) \\ θ_t&amp;=θ_{t-1}-\gamma h_t \end{aligned}\]

<p>基于梯度的方法存在一些缺陷，不同的改进思路如下：</p>
<ul>
  <li>更新过程中容易陷入局部极小值或鞍点；常见解决措施是在梯度更新中引入<strong>动量</strong>(如<strong>momentum</strong>, <strong>NAG</strong>, <strong>Funnelled SGDM</strong>)。</li>
  <li>参数的不同维度的梯度大小不同，导致参数更新时在梯度大的方向震荡，在梯度小的方向收敛较慢；常见解决措施是为每个特征设置<strong>自适应</strong>学习率(如<strong>RProp</strong>, <strong>AdaGrad</strong>, <strong>RMSprop</strong>, <strong>AdaDelta</strong>)。</li>
  <li>可以结合基于动量的方法和基于自适应学习率的方法，如<strong>Adam</strong>, <strong>AdamW</strong>, <strong>Adamax</strong>, <strong>Nadam</strong>, <strong>AMSGRad</strong>, <strong>Radam</strong>, <strong>AdaX</strong>, <strong>Amos</strong>, <strong>Lion</strong>。这类方法需要同时存储与模型参数具有相同尺寸的动量和方差，通常会占用较多内存，一些减少内存占用的优化算法包括<strong>Adafactor</strong>, <strong>SM3</strong>。</li>
  <li>在分布式训练大规模神经网络时，整体批量通常较大，权重更新的次数减少，常见解决措施是通过<strong>层级自适应</strong>实现每一层的梯度归一化(如<strong>LARS</strong>, <strong>LAMB</strong>, <strong>NovoGrad</strong>)。</li>
  <li>其他优化方法：随机权重平均、零阶优化、使用前向梯度代替反向传播梯度、<strong>Lookahead</strong>、<strong>Data Echoing</strong>。</li>
</ul>

<h3 id="-正则化方法-regularization">⚪ <a href="https://0809zheng.github.io/2020/03/03/regularization.html"><font color="Blue">正则化方法 (Regularization)</font></a></h3>

<p><strong>正则化</strong>指的是通过<strong>引入噪声</strong>或限制模型的<strong>复杂度</strong>，降低模型对输入或者参数的敏感性，避免过拟合，提高模型的泛化能力。常用的正则化方法包括：</p>
<ul>
  <li>约束<strong>目标函数</strong>：在目标函数中增加模型参数的正则化项，包括<strong>L2</strong>正则化, <strong>L1</strong>正则化, <strong>L0</strong>正则化, 弹性网络正则化, 谱正则化, 自正交性正则化, <strong>WEISSI</strong>正则化, 梯度惩罚</li>
  <li>约束<strong>网络结构</strong>：在网络结构中添加噪声，包括随机深度, <strong>Dropout</strong>及其系列方法,</li>
  <li>约束<strong>优化过程</strong>：在优化过程中施加额外步骤，包括数据增强, 梯度裁剪, <strong>Early Stop</strong>, 标签平滑, 变分信息瓶颈, 虚拟对抗训练, <strong>Flooding</strong></li>
</ul>

<h3 id="-归一化方法-normalization">⚪ <a href="https://0809zheng.github.io/2020/03/04/normalization.html"><font color="Blue">归一化方法 (Normalization)</font></a></h3>

<p>输入数据的特征通常具有不同的量纲和取值范围，使得不同特征的<strong>尺度</strong>差异很大。<strong>归一化</strong>泛指把数据特征的不同维度转换到相同尺度的方法。深度学习中常用的归一化方法包括：</p>
<ol>
  <li>基础归一化方法：最小-最大值归一化、标准化、白化、逐层归一化</li>
  <li>深度学习中的特征归一化：局部响应归一化<strong>LRN</strong>、批归一化<strong>BN</strong>、层归一化<strong>LN</strong>、实例归一化<strong>IN</strong>、组归一化<strong>GN</strong>、切换归一化<strong>SN</strong></li>
  <li>改进特征归一化：（改进<strong>BN</strong>）<strong>Batch Renormalization</strong>, <strong>AdaBN</strong>, <strong>L1-Norm BN</strong>, <strong>GBN</strong>, <strong>SPADE</strong>；（改进<strong>LN</strong>）<strong>RMS Norm</strong>；（改进<strong>IN</strong>）<strong>FRN</strong>, <strong>AdaIN</strong></li>
  <li>深度学习中的参数归一化：权重归一化<strong>WN</strong>、余弦归一化<strong>CN</strong>、谱归一化<strong>SN</strong></li>
</ol>

<h3 id="-参数初始化方法-parameter-initialization">⚪ <a href="https://0809zheng.github.io/2020/03/05/initialization.html"><font color="Blue">参数初始化方法 (Parameter Initialization)</font></a></h3>

<p>对神经网络进行训练时，需要对神经网络的参数进行初始化。糟糕的初始化不仅会使模型效果变差，还有可能使得模型根本训练不动或者不收敛。</p>

<p>常见的初始化方法包括零初始化、随机初始化、稀疏初始化、<strong>Xavier</strong>初始化、<strong>Kaiming</strong>初始化、正交初始化、恒等初始化、<strong>ZerO</strong>初始化、模仿初始化。</p>

<h2 id="2-深度学习的方法">(2) 深度学习的方法</h2>

<h3 id="-半监督学习-semi-supervised-learning">⚪ <a href="https://0809zheng.github.io/2022/09/01/semi.html"><font color="Blue">半监督学习 (Semi-Supervised Learning)</font></a></h3>

<p><strong>半监督学习</strong>是指同时从有标签数据和无标签数据中进行学习。半监督学习的假设包括平滑性假设、聚类假设、低密度分离假设和流形假设。</p>

<p>常用的半监督学习方法包括：</p>
<ul>
  <li><strong>一致性正则化</strong>：假设神经网络的随机性或数据增强不会改变输入样本的真实标签，如$\Pi$<strong>-Model</strong>, <strong>Temporal Ensembling</strong>, <strong>Mean Teacher</strong>, <strong>VAT</strong>, <strong>ICT</strong>, <strong>UDA</strong>。</li>
  <li><strong>伪标签</strong>：根据当前模型的最大预测概率为无标签样本指定假标签，如<strong>Label Propagation</strong>, <strong>Confirmation Bias</strong>, <strong>Noisy Student</strong>, <strong>Meta Pseudo Label</strong>。</li>
  <li><strong>一致性正则化+伪标签</strong>：既构造无标签样本的伪标签，又同时建立监督损失和无监督损失，如<strong>MixMatch</strong>, <strong>ReMixMatch</strong>, <strong>FixMatch</strong>, <strong>DivideMix</strong>。</li>
</ul>

<h3 id="-自监督学习-self-supervised-learning">⚪ <a href="https://0809zheng.github.io/2022/10/01/self.html"><font color="Blue">自监督学习 (Self-Supervised Learning)</font></a></h3>

<p><strong>自监督学习</strong>是一种无监督表示学习方法，旨在根据无标签数据集中的一部分信息预测剩余的信息，并以有监督的方式来训练该数据集。</p>

<p>适用于图像数据集的自监督任务包括：</p>
<ul>
  <li><strong>前置任务(pretext task)</strong>：通过从数据集中自动构造伪标签而设计的对目标任务有帮助的辅助任务，如<strong>Exemplar-CNN</strong>, <strong>Context Prediction</strong>, <strong>Jigsaw Puzzle</strong>, <strong>Image Colorization</strong>, <strong>Learning to Count</strong>, <strong>Image Rotation</strong>, <strong>Jigsaw Clustering</strong>, <strong>Evolving Loss</strong>, <strong>PIC</strong>, <strong>MP3</strong>。</li>
  <li><strong>对比学习(contrastive learning)</strong>：学习一个特征嵌入空间使得正样本对彼此靠近、负样本对相互远离。(对比损失函数) <strong>NCE</strong>, <strong>CPC</strong>, <strong>CPC v2</strong>, <strong>Alignment and Uniformity</strong>, <strong>Debiased Contrastive Loss</strong>, <strong>Hard Negative Samples</strong>, <strong>FlatNCE</strong>; (并行数据增强) <strong>InvaSpread</strong>, <strong>SimCLR</strong>, <strong>SimCLRv2</strong>, <strong>BYOL</strong>, <strong>SimSiam</strong>, <strong>DINO</strong>, <strong>SwAV</strong>, <strong>PixContrast</strong>, <strong>Barlow Twins</strong>; (存储体) <strong>InstDisc</strong>, <strong>MoCo</strong>, <strong>MoCo v2</strong>, <strong>MoCo v3</strong>; (多模态) <strong>CMC</strong>, <strong>CLIP</strong>; (应用) <strong>CURL</strong>, <strong>CUT</strong>, <strong>Background Augmentation</strong>, <strong>FD</strong>。</li>
  <li><strong>掩码图像建模(masked image modeling)</strong>：随机遮挡图像中的部分<strong>patch</strong>，并以自编码器的形式重构这部分<strong>patch</strong>，如<strong>BEiT</strong>, <strong>MAE</strong>, <strong>SimMIM</strong>, <strong>iBOT</strong>, <strong>ConvMAE</strong>, <strong>QB-Heat</strong>, <strong>LocalMIM</strong>, <strong>DeepMIM</strong>。</li>
</ul>

<h3 id="-度量学习-metric-learning">⚪ <a href="https://0809zheng.github.io/2022/11/01/metric.html"><font color="Blue">度量学习 (Metric Learning)</font></a></h3>

<p><strong>深度度量学习</strong>通过共享权重的<strong>Siamese</strong>网络把原始样本映射到低维特征空间，并设计合理的度量损失使得同类样本在特征空间上的距离比较近，不同类样本之间的距离比较远。</p>

<p>度量学习的目标在于最小化相似样本(正样本对)之间的距离，最大化不相似样本(负样本对)之间的距离。深度度量损失包括：</p>
<ul>
  <li>基于<strong>对(pair)</strong>的度量损失：考虑一个批次样本中样本对之间的关系，最小化正样本对$(x,x^+)$之间的距离，最大化负样本对$(x,x^-)$之间的距离。如<strong>Contrastive Loss</strong>, <strong>Binomial Deviance Loss</strong>, <strong>Triplet Loss</strong>, <strong>Improved Triplet Loss</strong>, <strong>Batch Hard Triplet Loss</strong>, <strong>Hierarchical Triplet Loss</strong>, <strong>Angular Loss</strong>, <strong>Quadruplet Loss</strong>, <strong>N-pair Loss</strong>, <strong>Lift Structured Loss</strong>, <strong>Histogram Loss</strong>, <strong>Ranked List Loss</strong>, <strong>Soft Nearest Neighbor Loss</strong>, <strong>Multi-Similarity Loss</strong>, <strong>Circle Loss</strong>。</li>
  <li>基于<strong>代理(proxy)</strong>的度量损失：为每个类别赋予一个代理样本，拉近每个类别的样本和该类别对应的代理样本之间的距离，拉远与其他类别对应的代理样本之间的距离。如<strong>Magnet Loss</strong>, <strong>Clustering Loss</strong>, <strong>Proxy-NCA</strong>, <strong>ProxyNCA++</strong>, <strong>Proxy-Anchor</strong>。</li>
</ul>

<h3 id="-多任务学习-multi-task-learning">⚪ <a href="https://0809zheng.github.io/2021/08/28/MTL.html"><font color="Blue">多任务学习 (Multi-Task Learning)</font></a></h3>

<p><strong>多任务学习</strong>是指同时学习多个属于不同领域的任务，并通过特定任务的领域信息提高泛化能力。多任务学习的方法设计可以分别从<strong>网络结构</strong>与<strong>损失函数</strong>两个角度出发。</p>

<p>一个高效的多任务网络，应同时兼顾特征共享部分和任务特定部分。根据模型在处理不同任务时网络参数的共享程度，多任务学习方法的网络结构可分为：</p>
<ul>
  <li><strong>硬参数共享 (Hard Parameter Sharing)</strong>：模型的主体部分共享参数，输出结构任务独立。如<strong>Multilinear Relationship Network</strong>, <strong>Fully-adaptive Feature Sharing</strong>。</li>
  <li><strong>软参数共享 (Soft Parameter Sharing)</strong>：不同任务采用独立模型，模型参数彼此约束。如<strong>Cross-Stitch Network</strong>, <strong>Sluice Network</strong>, <strong>Multi-Task Attention Network</strong>。</li>
</ul>

<p>多任务学习将多个相关的任务共同训练，其总损失函数是每个任务的损失函数的加权求和式：\(\mathcal{L}_{total} = \sum_{k}^{} w_k\mathcal{L}_k\)。多任务学习的目的是寻找模型参数的<strong>帕累托最优解</strong>，因此需要设置合适的任务权重。一些权重自动设置方法包括<strong>Uncertainty</strong>, <strong>Gradient Normalization</strong>, <strong>Dynamic Weight Average</strong>, <strong>Multi-Objective Optimization</strong>, <strong>Dynamic Task Prioritization</strong>, <strong>Loss-Balanced Task Weighting</strong></p>

<h3 id="-主动学习-active-learning">⚪ <a href="https://0809zheng.github.io/2022/08/01/activelearning.html"><font color="Blue">主动学习 (Active Learning)</font></a></h3>

<p><strong>主动学习</strong>是指从未标注数据中只选择一小部分样本进行标注和训练来降低标注成本。深度主动学习最常见的场景是基于<strong>池</strong>(<strong>pool-based</strong>)的主动学习，即从大量未标注的数据样本中迭代地选择最“有价值”的数据，直到性能达到指定要求或标注预算耗尽；选择最“有价值”的数据的过程被称为<strong>采样策略</strong>。</p>

<p>深度主动学习方法可以根据不同的<strong>采样策略</strong>进行分类：</p>
<ul>
  <li><strong>不确定性采样 (uncertainty sampling)</strong>：选择使得模型预测的不确定性最大的样本。不确定性的衡量可以通过机器学习方法(如<strong>entropy</strong>)、<strong>QBC</strong>方法(如<strong>voter entropy</strong>, <strong>consensus entropy</strong>)、贝叶斯神经网络(如<strong>BALD</strong>, <strong>bayes-by-backprop</strong>)、对抗生成(如<strong>GAAL</strong>, <strong>BGADL</strong>)、对抗攻击(如<strong>DFAL</strong>)、损失预测(如<strong>LPL</strong>)、标签预测(如<strong>forgetable event</strong>, <strong>CEAL</strong>)</li>
  <li><strong>多样性采样 (diversity sampling)</strong>：选择更能代表整个数据集分布的样本。多样性的衡量可以通过聚类(如<strong>core-set</strong>, <strong>Cluster-Margin</strong>)、判别学习(如<strong>VAAL</strong>, <strong>CAL</strong>, <strong>DAL</strong>)</li>
  <li><strong>混合策略 (hybrid strategy)</strong>：选择既具有不确定性又具有代表性的样本。样本的不确定性和代表性既可以同时估计(如<strong>exploration-exploitation</strong>, <strong>BatchBALD</strong>, <strong>BADGS</strong>, <strong>Active DPP</strong>, <strong>VAAL</strong>, <strong>MAL</strong>)，也可以分两阶段估计(如<strong>Suggestive Annotation</strong>, <strong>DBAL</strong>)。</li>
</ul>

<h3 id="-迁移学习-transfer-learning">⚪ <a href="https://0809zheng.github.io/2020/05/22/transfer-learning.html"><font color="Blue">迁移学习 (Transfer Learning)</font></a></h3>

<p><strong>迁移学习</strong>是指将解决某个问题时获取的知识应用在另一个不同但相关的问题中。根据源域数据和目标域数据的标签存在情况，迁移学习可以细分为：</p>
<ul>
  <li>源域数据有标签，目标域数据有标签：微调(<strong>Fine Tuning</strong>)</li>
  <li>源域数据有标签，目标域数据无标签：领域自适应(<strong>Domain Adaptation</strong>)、零样本学习(<strong>Zero-Shot Learning</strong>)</li>
  <li>源域数据无标签，目标域数据有标签：<strong>self-taught learning</strong></li>
  <li>源域数据无标签，目标域数据无标签：<strong>self-taught clustering</strong></li>
</ul>

<p>模型<strong>微调</strong>是指用带有标签的源域数据预训练模型后，再用带有标签的目标域数据微调模型。</p>

<p><strong>领域自适应</strong>是指通过构造合适的特征提取模型，使得源域数据和目标域数据的特征落入相同或相似的特征空间中，再用这些特征解决下游任务。常用的领域自适应方法包括：</p>
<ul>
  <li>基于差异的方法：直接计算和减小源域和目标域数据特征向量的差异，如<strong>Deep Domain Confusion</strong>, <strong>Deep Adaptation Network</strong>, <strong>CORAL</strong>, <strong>CMD</strong>。</li>
  <li>基于对抗的方法：引入域判别器并进行对抗训练，如<strong>DANN</strong>, <strong>SDT</strong>, <strong>PixelDA</strong>。</li>
  <li>基于重构的方法：引入解码器重构输入样本，如<strong>Domain Separation Network</strong>。</li>
</ul>

<h2 id="3-深度学习的技巧">(3) 深度学习的技巧</h2>

<h3 id="-长尾分布-long-tailed">⚪ <a href="https://0809zheng.github.io/2020/03/02/optimization.html"><font color="Blue">长尾分布 (Long-Tailed)</font></a></h3>

<p>实际应用中的数据集大多服从<strong>长尾分布</strong>，即少数类别(<strong>head class</strong>)占据绝大多数样本，多数类别(<strong>tail class</strong>)仅有少量样本。解决长尾分布问题的方法包括：</p>
<ul>
  <li>重采样 <strong>Re-sampling</strong>：通过对<strong>head class</strong>进行欠采样或对<strong>tail class</strong>进行过采样，人为地构造类别均衡的数据集。包括<strong>Random under/over-sampling</strong>, <strong>Class-balanced sampling</strong>, <strong>Meta Sampler</strong>等。</li>
  <li>重加权 <strong>Re-weighting</strong>：在损失函数中对不同类别样本的损失设置不同的权重，通常是对<strong>tail class</strong>对应的损失设置更大的权重。其中在$\log$运算之外调整损失函数的本质是在调节样本权重或者类别权重(如<strong>Inverse Class Frequency Weighting</strong>, <strong>Cost-Sensitive Cross-Entropy Loss</strong>, <strong>Focal Loss</strong>, <strong>Class-Balanced Loss</strong>)。在$\log$运算之内调整损失函数的本质是调整<strong>logits</strong>得分$z$，从而缓解对<strong>tail</strong>类别的负梯度(如<strong>Equalization Loss</strong>, <strong>Equalization Loss v2</strong>, <strong>Logit Adjustment Loss</strong>, <strong>Balanced Softmax Loss</strong>, <strong>Seesaw Loss</strong>)。</li>
  <li>其他方法：一些方法将长尾分布问题解耦为特征的表示学习和特征的分类。一些方法按照不同类别的样本数量级对类别进行分组(如<strong>BAGS</strong>)。</li>
</ul>

<h3 id="-对抗训练-adversarial-training">⚪ <a href="https://0809zheng.github.io/2020/07/26/adversirial_attack_in_classification.html"><font color="Blue">对抗训练 (Adversarial Training)</font></a></h3>

<p><strong>对抗训练</strong>是指通过构造对抗样本，对模型进行对抗攻击和防御来增强模型的稳健性。对抗训练的一般形式如下：</p>

\[\mathcal{\min}_{\theta} \mathbb{E}_{(x,y)\sim \mathcal{D}} \left[ \mathcal{\max}_{\Delta x \in \Omega}  \mathcal{L}(x+\Delta x,y;\theta) \right]\]

<ul>
  <li>对抗攻击是指想办法造出更多的对抗样本；常用的对抗攻击方法包括：<strong>FGSM</strong>, <strong>I-FGSM</strong>, <strong>MI-FGSM</strong>, <strong>NI-FGSM</strong>, <strong>DIM</strong>, <strong>TIM</strong>, <strong>One Pixel Attack</strong>, <strong>Black-box Attack</strong>。</li>
  <li>对抗防御是指想办法让模型能正确识别更多的对抗样本；常用的对抗防御方法包括<strong>Smoothing</strong>, <strong>Feature Squeezing</strong>, <strong>Randomization</strong>, <strong>Proactive defense</strong>。</li>
</ul>

<h3 id="-大模型的参数高效微调-parameter-efficient-fine-tuning">⚪ <a href="https://0809zheng.github.io/2023/02/02/peft.html"><font color="Blue">大模型的参数高效微调 (Parameter-Efficient Fine-Tuning)</font></a></h3>

<p>将预训练好的大型模型在下游任务上进行微调已成为处理不同任务的通用范式；但是随着模型越来越大，对模型进行全部参数的微调（<strong>full fine-tuning</strong>）变得非常昂贵。<strong>参数高效微调</strong>是指冻结预训练模型的大部分参数，仅微调少量或额外的模型参数。</p>

<p>参数高效微调方法有以下几种形式：</p>
<ul>
  <li>增加额外参数(<strong>addition</strong>)：在原始模型中引入额外的可训练参数，如<strong>Adapter</strong>, <strong>AdapterFusion</strong>, <strong>AdapterDrop</strong>, <strong>P-Tuning</strong>, <strong>Prompt Tuning</strong>, <strong>Prefix-Tuning</strong>, <strong>P-Tuning v2</strong>, <strong>Ladder Side-Tuning</strong></li>
  <li>选取部分参数(<strong>specification</strong>)：指定原始模型中的部分参数可训练，如<strong>BitFit</strong>, <strong>Child-Tuning</strong></li>
  <li>重参数化(<strong>reparameterization</strong>)：将微调过程重参数化为低维子空间的优化，如<strong>Diff Pruning</strong>, <strong>LoRA</strong>, <strong>AdaLoRA</strong>, <strong>QLoRA</strong>, <strong>GLoRA</strong></li>
  <li>
    <p>混合方法：如<strong>MAM Adapter</strong>, <strong>UniPELT</strong></p>
  </li>
  <li><a href="https://0809zheng.github.io/2020/04/28/explainable-DL.html">深度学习的可解释性</a></li>
</ul>

<h2 id="-网络压缩">() 网络压缩</h2>
<p>网络压缩旨在平衡网络的准确性和运算效率。
压缩预训练的网络 设计新的网络结构</p>
<ul>
  <li><a href="https://0809zheng.github.io/2020/05/01/network-compression.html">网络压缩</a>：网络剪枝、知识蒸馏、结构设计、模型量化</li>
</ul>

<h1 id="3-深度学习的应用">3. 深度学习的应用</h1>

<h2 id="1-计算机视觉">(1) 计算机视觉</h2>

<h3 id="-图像识别-image-recognition">⚪ <a href="https://0809zheng.github.io/2020/05/06/image-classification.html"><font color="Blue">图像识别 (Image Recognition)</font></a></h3>

<p><strong>图像识别</strong>是计算机视觉的基本任务，旨在对每张图像内出现的物体进行类别区分。基于深度学习的图像识别方法不需要手工提取特征，而是使用卷积神经网络自动提取特征并进行分类。应用于图像识别任务的卷积神经网络的结构发展包括：</p>
<ol>
  <li>早期探索：奠定“卷积层-下采样层-全连接层”的拓扑结构。如<strong>LeNet5</strong>, <strong>AlexNet</strong>, <strong>ZFNet</strong>, <strong>NIN</strong>, <strong>SPP-net</strong>, <strong>VGGNet</strong></li>
  <li>深度化：增加堆叠卷积层的数量。如<strong>Highway Network</strong>, <strong>ResNet</strong>, <strong>Stochastic Depth</strong>, <strong>DenseNet</strong>, <strong>Pyramidal ResNet</strong></li>
  <li>模块化：设计用于堆叠的网络模块。如<strong>Inception v1-4</strong>, <strong>WideResNet</strong>, <strong>Xception</strong>, <strong>ResNeXt</strong>, <strong>NASNet</strong>, <strong>ResNeSt</strong>, <strong>ConvNeXt v1-2</strong></li>
  <li>轻量化：设计轻量级卷积层，可参考<a href="https://0809zheng.github.io/2021/09/10/lightweight.html"><font color="Blue">轻量级卷积神经网络</font></a>。</li>
  <li>其他结构：<strong>Noisy Student</strong>, <strong>SCAN</strong>, <strong>NFNet</strong>, <strong>ResNet-RS</strong></li>
</ol>

<h3 id="-图像分割-image-segmentation">⚪ <a href="https://0809zheng.github.io/2020/05/07/semantic-segmentation.html"><font color="Blue">图像分割 (Image Segmentation)</font></a></h3>

<p><strong>图像分割</strong>是对图像中的每个像素进行分类，可以细分为：</p>
<ul>
  <li><strong>语义分割</strong>：注重类别之间的区分，而不区分同一类别的不同个体；</li>
  <li><strong>实例分割</strong>：注重类别以及同一类别的不同个体之间的区分；</li>
  <li><strong>全景分割</strong>：对于可数的对象实例(如行人、汽车)做实例分割，对于不可数的语义区域(如天空、地面)做语义分割。</li>
</ul>

<p>图像分割模型通常采用<strong>编码器-解码器</strong>结构。编码器从预处理的图像数据中提取特征，解码器把特征解码为分割掩码。图像分割模型的发展趋势可以大致总结为：</p>
<ul>
  <li>全卷积网络：<strong>FCN</strong>, <strong>SegNet</strong>, <strong>RefineNet</strong>, <strong>U-Net</strong>, <strong>V-Net</strong>, <strong>M-Net</strong>, <strong>W-Net</strong>, <strong>Y-Net</strong>, <strong>UNet++</strong>, <strong>Attention U-Net</strong>, <strong>GRUU-Net</strong>, <strong>BiSeNet V1,2</strong>, <strong>DFANet</strong>, <strong>SegNeXt</strong></li>
  <li>上下文模块：<strong>DeepLab v1,2,3,3+</strong>, <strong>PSPNet</strong>, <strong>FPN</strong>, <strong>UPerNet</strong>, <strong>EncNet</strong>, <strong>PSANet</strong>, <strong>APCNet</strong>, <strong>DMNet</strong>, <strong>OCRNet</strong>, <strong>PointRend</strong>, <strong>K-Net</strong></li>
  <li>基于<strong>Transformer</strong>：<strong>SETR</strong>, <strong>TransUNet</strong>, <strong>SegFormer</strong>, <strong>Segmenter</strong>, <strong>MaskFormer</strong>, <strong>SAM</strong></li>
  <li>通用技巧：<strong>Deep Supervision</strong>, <strong>Self-Correction</strong></li>
</ul>

<p>图像分割中常用的评估指标包括：<strong>PA</strong>, <strong>CPA</strong>, <strong>MPA</strong>, <strong>IoU</strong>, <strong>MIoU</strong>, <strong>FWIoU</strong>, <strong>Dice Coefficient</strong>。</p>

<p>图像分割的损失函数用于衡量预测分割结果和真实标签之间的差异。根据损失函数的推导方式不同，图像分割任务中常用的损失函数可以划分为：</p>
<ul>
  <li>基于分布的损失：<strong>Cross-Entropy Loss</strong>, <strong>Weighted Cross-Entropy Loss</strong>, <strong>TopK Loss</strong>, <strong>Focal Loss</strong>, <strong>Distance Map Penalized CE Loss</strong></li>
  <li>基于区域的损失：<strong>Sensitivity-Specifity Loss</strong>, <strong>IoU Loss</strong>, <strong>Lovász Loss</strong>, <strong>Dice Loss</strong>, <strong>Tversky Loss</strong>, <strong>Focal Tversky Loss</strong>, <strong>Asymmetric Similarity Loss</strong>, <strong>Generalized Dice Loss</strong>, <strong>Penalty Loss</strong></li>
  <li>基于边界的损失：<strong>Boundary Loss</strong>, <strong>Hausdorff Distance Loss</strong></li>
</ul>

<h3 id="-目标检测-object-detection">⚪ <a href="https://0809zheng.github.io/2020/05/08/object-detection.html"><font color="blue">目标检测 (Object Detection)</font></a></h3>

<p><strong>目标检测</strong>任务是指在图像中检测出可能存在的目标；包括<strong>定位</strong>和<strong>分类</strong>两个子任务：其中定位是指确定目标在图像中的具体位置，分类是确定目标的具体类别。</p>

<p>传统的目标检测算法首先在图像中生成候选区域，然后对每个候选区域提取特征向量，最后对每个候选区域提取的特征进行分类。常用的候选区域生成方法包括滑动窗口、<strong>Felzenszwalb</strong>算法、选择搜索算法。常用的特征描述子包括图像梯度向量、方向梯度直方图<strong>HOG</strong>、尺度不变特征变换<strong>SIFT</strong>、可变形部位模型<strong>DPM</strong>。</p>

<p>基于深度学习的目标检测模型包括：</p>
<ul>
  <li><strong>两阶段</strong>的目标检测模型：首先在图像中生成可能存在目标的候选区域，然后对这些候选区域进行预测。如<strong>R-CNN</strong>, <strong>Fast RCNN</strong>, <strong>Faster RCNN</strong>, <strong>SPP-Net</strong>, <strong>FPN</strong>, <strong>Libra RCNN</strong>, <strong>Cascade RCNN</strong>, <strong>Sparse RCNN</strong></li>
  <li><strong>单阶段</strong>的目标检测模型：把图像中的每一个位置看作潜在的候选区域，直接进行预测。如<strong>OverFeat</strong>, <strong>YOLOv1-3</strong>, <strong>SSD</strong>, <strong>RetinaNet</strong>, <strong>Guided Anchoring</strong>, <strong>ASFF</strong>, <strong>EfficientDet</strong>, <strong>YOLT</strong>, <strong>Poly-YOLO</strong>, <strong>YOLOv4</strong>, <strong>YOLOv5</strong>, <strong>RTMDet</strong></li>
  <li><strong>Anchor-Free</strong>的目标检测模型：把目标检测任务视作关键点检测等其它形式的任务，直接对目标的位置进行预测。(<strong>anchor-point</strong>方法) <strong>FCOS</strong>, <strong>YOLOX</strong>, <strong>YOLOv6</strong>, <strong>YOLOv7</strong>, <strong>YOLOv8</strong>, <strong>YOLOv9</strong>, <strong>YOLOv10</strong>; (<strong>key-point</strong>方法) <strong>CornerNet</strong>, <strong>CenterNet</strong>, <strong>RepPoints</strong></li>
  <li>基于<strong>Transformer</strong>的目标检测模型：<strong>DETR</strong>, <strong>Deformable DETR</strong></li>
</ul>

<p>目标检测的常用评估指标包括准确率、召回率、<strong>F-score</strong>、<strong>P-R</strong>曲线、平均准确率<strong>AP</strong>、类别平均准确率<strong>mAP</strong>。</p>

<p><strong>非极大值抑制</strong>算法是目标检测等任务中常用的后处理方法，能够过滤掉多余的检测边界框。提高<strong>NMS</strong>算法精度的方法包括<strong>Soft-NMS</strong>, <strong>IoU-Guided NMS</strong>, <strong>Weighted NMS</strong>, <strong>Softer-NMS</strong>, <strong>Adaptive NMS</strong>, <strong>DIoU-NMS</strong>。提高<strong>NMS</strong>算法效率的方法包括<strong>CUDA NMS</strong>, <strong>Fast NMS</strong>, <strong>Cluster NMS</strong>, <strong>Matrix NMS</strong>。</p>

<p>目标检测中的损失函数包括边界框的<strong>分类</strong>损失和<strong>回归</strong>损失。其中分类损失用于区分边界框的类别，即边界框内目标的类别，对于两阶段的检测方法还包含边界框的正负类别；常用的分类损失函数包括<strong>Cross-Entropy loss</strong>, <strong>Focal loss</strong>, <strong>Generalized Focal Loss</strong>, <strong>Varifocal Loss</strong>, <strong>GHM</strong>, <strong>Poly loss</strong>。</p>

<p>而回归损失衡量预测边界框坐标$x_{pred}$和<strong>GT</strong>边界框坐标$x_{gt}$之间的差异，常用的回归损失函数包括<strong>L1 / L2 loss</strong>, <strong>Smooth L1 loss</strong>, <strong>Dynamic SmoothL1 Loss</strong>, <strong>Balanced L1 loss</strong>, <strong>IoU loss</strong>, <strong>GIoU loss</strong>, <strong>DIoU loss</strong>, <strong>CIoU loss</strong>, <strong>EIoU loss</strong>, <strong>SIoU loss</strong>, <strong>MPDIoU loss</strong>。</p>

<p><strong>标签分配</strong>策略是指在训练目标检测器时，为特征图不同位置的预测样本分配合适的标签（即区分<strong>anchor</strong>是正样本还是负样本），用于计算损失。标签分配根据非负即正划分为<strong>硬标签分配(hard LA)</strong>和<strong>软标签分配(soft LA)</strong>。</p>
<ul>
  <li>硬标签分配策略是指根据阈值把样本划分为正样本或者负样本。依据在训练阶段是否动态调整阈值，硬标签分配策略又可以细分为静态和动态两种：
    <ol>
      <li><strong>静态分配</strong>策略主要依据于模型的先验知识（例如距离阈值和<strong>iou</strong>阈值等）来选取不同的正负样本；</li>
      <li><strong>动态分配</strong>策略依据在训练阶段采用不同的统计量来动态地设置阈值，并划分正负样本；如<strong>DLA</strong>, <strong>MCA</strong>, <strong>HAMBox</strong>, <strong>ATSS</strong>, <strong>SimOTA</strong>, <strong>DSLA</strong>。</li>
    </ol>
  </li>
  <li>软标签分配策略则会根据预测结果与<strong>GT</strong>计算正负权重，在候选正样本(中心点落在<strong>GT</strong>框内)的基础上依据正负样本权重分配正负样本，且在训练的过程中动态调整分配权重。常见的软标签分配策略包括<strong>Noisy Anchor</strong>, <strong>AutoAssign</strong>, <strong>SAPD</strong>, <strong>TOOD</strong>。</li>
</ul>

<h3 id="-开放集合目标检测-open-set-object-detection">⚪ <a href="https://0809zheng.github.io/2023/11/01/opensetdet.html"><font color="blue">开放集合目标检测 (Open-Set Object Detection)</font></a></h3>

<p><strong>开集目标检测</strong>是指在可见类的数据上进行训练，然后完成对不可见类数据的定位与识别。一些常见的开集目标检测方法包括：</p>
<ul>
  <li>基于无监督学习的开集检测器：通过聚类、弱监督等手段实现开集检测，如<strong>OSODD</strong>, <strong>Detic</strong>, <strong>VLDet</strong></li>
  <li>基于多模态学习的开集检测器：
    <ol>
      <li>基于<strong>Referring</strong>的开集检测器：借助多模态视觉-语言模型实现检测，如<strong>ViLD</strong>, <strong>RegionCLIP</strong>, <strong>VL-PLM</strong>, <strong>Grad-OVD</strong></li>
      <li>基于<strong>Grounding</strong>的开集检测器：把开集检测任务建模为边界框提取+短语定位任务，如<strong>OVR-CNN</strong>, <strong>MDETR</strong>, <strong>GLIP</strong>, <strong>DetCLIP</strong>, <strong>DetCLIPv2</strong>, <strong>Grounding DINO</strong></li>
    </ol>
  </li>
</ul>

<h3 id="-人体姿态估计">⚪ <a href="https://0809zheng.github.io/2020/05/08/pose-estimation.html">人体姿态估计</a></h3>

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/07/3dhuman.html">三维人体模型</a></li>
  <li><a href="https://0809zheng.github.io/2020/11/26/eval-pose-estimate.html">人体姿态估计的评估指标</a></li>
</ul>

<h3 id="-图像超分辨率-super-resolution">⚪ <a href="https://0809zheng.github.io/2020/08/27/SR.html"><font color="blue">图像超分辨率 (Super Resolution)</font></a></h3>

<p>图像<strong>超分辨率</strong>旨在将低分辨率图像<strong>LR</strong>放大为对应的高分辨率图像<strong>HR</strong>，从而使图像更清晰。图像超分辨率的传统方法主要是基于插值的方法，如最邻近插值、双线性插值、双三次插值；而基于深度学习的图像超分辨率方法，可以根据<strong>上采样的位置</strong>不同进行分类：</p>

<ul>
  <li><strong>预定义上采样(Predefined upsampling)</strong>：首先对图像应用预定义的插值方法进行上采样，再通过卷积网络增加细节，如<strong>SRCNN</strong>, <strong>VDSR</strong>。</li>
  <li><strong>单次上采样(Single upsampling)</strong>：先通过卷积网络提取丰富的特征，再通过预定义或可学习的单次上采样增加分辨率，如<strong>FSRCNN</strong>, <strong>ESPCN</strong>, <strong>EDSR</strong>, <strong>RCAN</strong>, <strong>SAN</strong>。</li>
  <li><strong>渐进上采样(Progressive upsampling)</strong>：通过多次上采样逐渐增加分辨率，如<strong>LapSRN</strong>。</li>
  <li><strong>循环采样(Iterative up and downsampling)</strong>：循环地进行上采样和下采样，增加丰富的特征信息，如<strong>DBPN</strong>, <strong>DRN</strong>。</li>
  <li>其他结构：如<strong>SRGAN</strong>, <strong>ESRGAN</strong>引入生成对抗网络；<strong>LIIF</strong>学习二维图像的连续表达形式。</li>
</ul>

<p>图像超分辨率的评估指标主要包括峰值信噪比<strong>PSNR</strong>和结构相似度<strong>SSIM</strong>。</p>

<h3 id="-图像到图像翻译-image-to-image-translation">⚪ <a href="https://0809zheng.github.io/2020/05/23/image_translation.html"><font color="blue">图像到图像翻译 (Image-to-Image Translation)</font></a></h3>

<p><strong>图像到图像翻译</strong>旨在学习一个映射使得图像可以从源图像域变换到目标图像域，同时保留图像内容。根据是否提供了一对一的学习样本对，将图像到图像翻译任务划分为<strong>有配对数据(paired data)</strong>和<strong>无配对数据(unpaired data)</strong>两种情况。</p>
<ul>
  <li>有配对数据(监督图像翻译)是指在训练数据集中具有一对一的数据对；即给定联合分布$p(X,Y)$，学习条件映射$f_{x \to y}=p(Y|X)$和$f_{y \to x}=p(X|Y)$。代表方法有<strong>Pix2Pix</strong>, <strong>BicycleGAN</strong>, <strong>LPTN</strong>。</li>
  <li>无配对数据(无监督图像翻译)是指模型在多个独立的数据集之间训练，能够从多个数据集合中自动地发现集合之间的关联，从而学习出映射函数；即给定边缘分布$p(X)$和$p(Y)$，学习条件映射$f_{x \to y}=p(Y|X)$和$f_{y \to x}=p(X|Y)$。代表方法有<strong>CoGAN</strong>, <strong>PixelDA</strong>, <strong>CycleGAN</strong>, <strong>DiscoGAN</strong>, <strong>DualGAN</strong>, <strong>UNIT</strong>, <strong>MUNIT</strong>, <strong>TUNIT</strong>, <strong>StarGAN</strong>, <strong>StarGAN v2</strong>, <strong>GANILLA</strong>, <strong>NICE-GAN</strong>, <strong>CUT</strong>, <strong>SimDCL</strong>。</li>
</ul>

<h3 id="-时空动作检测-spatio-temporal-action-detection">⚪ <a href="https://0809zheng.github.io/2021/07/15/stad.html"><font color="blue">时空动作检测 (Spatio-Temporal Action Detection)</font></a></h3>

<p><strong>时空动作检测</strong>旨在识别视频中目标动作出现的区间和对应的类别，并在空间范围内用一个包围框标记出人物的空间位置。按照处理方式不同，时空动作检测可方法以分为：</p>

<ul>
  <li><strong>帧级的检测器(frame-level detector)</strong>：每次检测时输入单帧图像，得到单帧图像上的检测结果；之后把检测结果沿时间维度进行连接，得到视频检测结果。如<strong>T-CNN</strong>。</li>
  <li><strong>管级的检测器(tubelet-level detector)</strong>：每次检测时输入多帧连续视频帧，对每帧上预定义的检测框进行修正，并对不同输入的结果在时序上进行连接。如<strong>ACT-detector</strong>, <strong>MOC-detector</strong>。</li>
</ul>

<h3 id="-人脸检测-识别与验证-face-detection-recognition-and-verification">⚪ <a href="https://0809zheng.github.io/2020/05/10/face-recognition.html"><font color="blue">人脸检测, 识别与验证 (Face Detection, Recognition, and Verification)</font></a></h3>

<p><strong>人脸检测</strong>是指检测任意一幅给定的图像中是否含有人脸，如果是则返回人脸的位置、大小和姿态，是人脸验证与识别的关键步骤。常用的人脸检测方法包括<strong>Eigenface</strong>, <strong>SSH</strong>。</p>

<p><strong>人脸识别</strong>是指判断给定的人脸图像属于用户数据库中的哪个人（或没有匹配），是一种多分类问题。常用的人脸识别方法包括<strong>DeepFace</strong>。</p>

<p><strong>人脸验证</strong>是指判断给定的人脸图像和用户<strong>ID</strong>是否匹配，是一种二分类问题。常用的人脸识别方法包括<strong>DeepID</strong>, <strong>DeepID2</strong>。</p>

<h3 id="-行人检测与属性识别-pedestrian-detection-and-attribute-recognition">⚪ <a href="https://0809zheng.github.io/2020/05/12/pedestrian-attribute-recognition.html"><font color="blue">行人检测与属性识别 (Pedestrian Detection and Attribute Recognition)</font></a></h3>

<p><strong>行人检测</strong>是指找出图像或视频帧中所有的行人，包括位置和大小；常用的行人检测方法包括<strong>DeepParts</strong>。</p>

<p><strong>行人属性识别</strong>是指从行人图像中挖掘具有高级语义的属性信息；常用的行人属性识别方法包括<strong>DeepSAR</strong>, <strong>DeepMAR</strong>, <strong>HydraPlus-Net</strong>。</p>

<h3 id="-文本检测与识别-text-detection-and-recognition">⚪ <a href="https://0809zheng.github.io/2020/05/15/text-detection-recognition.html"><font color="blue">文本检测与识别 (Text Detection and Recognition)</font></a></h3>

<p><strong>文本检测</strong>是指找出图像中的文字区域；文本识别是指对定位好的文字区域进行识别，将图像中的文字区域进转化为字符信息。常用的文本检测与识别方法包括<strong>EAST</strong>, <strong>CRNN</strong>, <strong>Mask TextSpotter</strong>。</p>

<h3 id="-点云分类-point-cloud-classification">⚪ <a href="https://0809zheng.github.io/2023/04/01/pointcloud.html"><font color="blue">点云分类 (Point Cloud Classification)</font></a></h3>

<p><strong>点云分类</strong>即点云形状分类，是一种重要的点云理解任务。该任务的方法通常首先学习每个点的嵌入，然后使用聚合方法从整个点云中提取全局形状嵌入，并通过分类器进行分类。根据神经网络输入的数据格式，三维点云分类方法可分为：</p>
<ul>
  <li>基于多视图(<strong>Multi-view based</strong>)的方法：将点云投影为多个二维图像，如<strong>MVCNN</strong>, <strong>MHBN</strong>。</li>
  <li>基于体素(<strong>Voxel-based</strong>)的方法：将点云转换为三维体素表示，如<strong>VoxNet</strong>, <strong>OctNet</strong>。</li>
  <li>基于点(<strong>Point-based</strong>)的方法：直接处理原始点云，如<strong>PointNet</strong>, <strong>PointNet++</strong>, <strong>PointCNN</strong>, <strong>DGCNN</strong>, <strong>PCT</strong>。</li>
</ul>

<h3 id="-目标计数-object-counting">⚪ <a href="https://0809zheng.github.io/2023/05/01/counting.html"><font color="blue">目标计数 (Object Counting)</font></a></h3>

<p><strong>目标计数</strong>任务旨在从图像或视频中统计特定目标实例的数量。本文主要讨论基于<strong>回归</strong>的计数方式，即直接学习图像到目标数量或目标密度图的映射关系，通用的目标技术方案包括：</p>
<ol>
  <li><strong>少样本计数 (Few-Shot Counting)</strong>：提供目标样本<strong>exemplar</strong>在查询图像中进行匹配，如<strong>GMN</strong>, <strong>FamNet</strong>, <strong>LaoNet</strong>, <strong>CFOCNet</strong>, <strong>SAFECount</strong>, <strong>BMNet+</strong>, <strong>Counting-DETR</strong>, <strong>CounTR</strong>, <strong>LOCA</strong>, <strong>SPDCN</strong>, <strong>VCN</strong>, <strong>SAM Counting</strong>, <strong>CACViT</strong>, <strong>DAVE</strong>, <strong>SSD</strong>。</li>
  <li><strong>无参考计数 (Reference-less Counting)</strong>：自动挖掘和计数所有显著性目标，如<strong>LC</strong>, <strong>RLC</strong>, <strong>MoVie</strong>, <strong>DSAA</strong>, <strong>CaOC</strong>, <strong>RepRPN-Counter</strong>, <strong>RCC</strong>, <strong>GCNet</strong>, <strong>ZSC</strong>, <strong>ABC123</strong>, <strong>OmniCount</strong>。</li>
  <li><strong>文本引导计数 (Text-Guided Counting)</strong>：通过预训练视觉语言模型进行目标计数，如<strong>CountCLIP</strong>, <strong>CLIP-Count</strong>, <strong>CounTX</strong>, <strong>VLCounter</strong>, <strong>CLIP Counting</strong>, <strong>ExpressCount</strong>。</li>
</ol>

<h2 id="2-自然语言处理">(2) 自然语言处理</h2>

<h3 id="-1">⚪ <a href="https://0809zheng.github.io/2020/08/27/SR.html">1</a></h3>

<ul>
  <li><a href="https://0809zheng.github.io/2020/04/29/word-embedding.html">词嵌入</a></li>
  <li><a href="https://0809zheng.github.io/2020/05/13/text-summary.html">文本摘要</a></li>
  <li><a href="https://0809zheng.github.io/2020/05/14/image-caption.html">图像描述</a></li>
  <li><a href="https://0809zheng.github.io/2020/06/11/ctc.html">连接时序分类</a></li>
  <li><a href="https://0809zheng.github.io/2020/10/26/musicgen.html">音乐生成</a></li>
</ul>

<h2 id="3-ai-for-science">(3) AI for Science</h2>

<ul>
  <li><a href="https://0809zheng.github.io/2021/06/28/fno.html">Fourier Neural Operator for Parametric Partial Differential Equations</a>：(arXiv2010)为偏微分方程设计的傅里叶神经算子。</li>
  <li><a href="https://0809zheng.github.io/2022/01/08/mathai.html">Advancing mathematics by guiding human intuition with AI</a>：(Nature 2021.12)用人工智能引导人类直觉推进数学发展。</li>
  <li><a href="https://0809zheng.github.io/2022/06/19/noether.html">Noether Networks: Meta-Learning Useful Conserved Quantities</a>：(arXiv2112)Noether网络：通过元学习学习有用的守恒量。</li>
  <li><a href="https://0809zheng.github.io/2022/03/13/alphacode.html">Competition-Level Code Generation with AlphaCode</a>：(arXiv2203)AlphaCode: 竞赛级别的代码生成。</li>
  <li><a href="https://0809zheng.github.io/2022/11/21/alphatensor.html">Discovering faster matrix multiplication algorithms with reinforcement learning</a>：(Nature 2022.10)AlphaTensor：通过强化学习发现更快的矩阵乘法算法。</li>
  <li><a href="https://0809zheng.github.io/2023/06/07/alphadev.html">Faster sorting algorithms discovered using deep reinforcement learning</a>：(Nature 2023.06)AlphaDev：通过深度强化学习发现更快的排序算法。</li>
</ul>

<h1 id="4-参考文献与扩展阅读">4. 参考文献与扩展阅读</h1>

<h3 id="-深度学习的相关课程">⚪ 深度学习的相关课程</h3>
<ul>
  <li><a href="https://www.coursera.org/specializations/deep-learning">Deep Learning | Coursera （Andrew Ng）</a></li>
  <li><a href="https://www.bilibili.com/video/BV1zE411T7nb?from=search&amp;seid=890015452850895449">吴恩达Tensorflow2.0实践系列课程</a></li>
  <li><a href="http://cs231n.stanford.edu/syllabus.html">CS231n：计算机视觉（李飞飞）</a></li>
  <li><a href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">CS294-158：深度无监督学习</a></li>
  <li><a href="https://www.bilibili.com/video/BV1E7411t7ay">旷视x北大《深度学习实践》</a></li>
  <li><strong>YouTuber</strong>：<a href="https://www.youtube.com/@HungyiLeeNTU">李宏毅</a>、<a href="https://www.youtube.com/@YannicKilcher">Yannic Kilcher</a></li>
</ul>

<h3 id="-深度学习的相关书籍">⚪ 深度学习的相关书籍</h3>
<ul>
  <li><a href="https://book.douban.com/subject/27087503/">Deep Learning（花书）</a></li>
  <li><a href="https://nndl.github.io/">《神经网络与深度学习》（邱锡鹏）</a></li>
  <li><a href="http://zh.d2l.ai/">《动手学深度学习》（李沐等）</a></li>
</ul>

<h3 id="-深度学习的相关博客">⚪ 深度学习的相关博客</h3>
<ul>
  <li>企业博客：<a href="https://openai.com/blog/">OpenAI</a>、<a href="https://www.deepmind.com/blog">DeepMind</a>、<a href="https://www.deepmind.com/blog">DeepLearning.AI</a></li>
  <li>个人博客：<a href="https://lilianweng.github.io/">Lil’Log</a>、<a href="https://spaces.ac.cn/">科学空间</a></li>
</ul>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/01/10/statistics.html" class="read-next-link"></a>
        <section>
          <span>数理统计学(Mathematic Statistics)的基本概念</span>
          <p>  Mathematic Statistics.</p>
        </section>
        
        <div class="filter"></div>
        <img src="" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/01/01/resume.html" class="read-next-link"></a>
          <section>
            <span>Publications of Z.Zheng</span>
            <p>  My Publications.</p>
          </section>
          
          <div class="filter"></div>
          <img src="" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
