<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications of Z.Zheng - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Publications of Z.Zheng">
    <meta name="keywords"  content="">
    <!-- Open Graph -->
    <meta property="og:title" content="Publications of Z.Zheng - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2020/01/01/resume.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
      </div>
      <h1>Publications of Z.Zheng</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="20-01-01"><i class="iconfont icon-date"></i>01 Jan 2020</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>My Publications.</p>
</blockquote>

<h3 id="-upsampling-attention-network-for-single-image-super-resolution">⚪ <a href="https://ieeexplore.ieee.org/document/9420808"><font color="blue">Upsampling Attention Network for Single Image Super-resolution</font></a></h3>

<p><img src="https://pic.imgdb.cn/item/623d9eca27f86abb2ac2c1b7.jpg" alt="" />
<img src="https://pic.imgdb.cn/item/623d9ef327f86abb2ac3ff2f.jpg" alt="" /></p>

<details><summary>Abstract：Click to Read</summary>Recently, convolutional neural network (CNN) has been widely used in single image super-resolution (SISR) and made significant advances. However, most of the existing CNN-based SISR models ignore fully utilization of the extracted features during upsampling, causing information bottlenecks, hence hindering the expressive ability of networks. To resolve these problems, we propose an upsampling attention network (UAN) for richer feature extraction and reconstruction. Specifically, we present a residual attention groups (RAGs) based structure to extract structural and frequency information, which is composed of several residual feature attention blocks (RFABs) with a non-local skip connection. Each RFAB adaptively rescales spatial- and channel-wise features by paying attention to correlations among them. Furthermore, we propose an upsampling attention block (UAB), which not only applies parallel upsampling processes to obtain richer feature representations, but also combines them to obt ain better reconstruction results. Experiments on standard benchmarks show the advantage of our UAN over state-of-the-art methods both in objective metrics and visual qualities.</details>

<ul>
  <li><em>Authors</em>：<strong>Z.Zheng</strong>, Y.Jiao and G.Fang</li>
  <li><em>Conference</em>：International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2021</li>
</ul>

<h3 id="-human-posture-reconstruction-for-through-the-wall-radar-imaging-using-convolutional-neural-networks">⚪ <a href="https://ieeexplore.ieee.org/document/9420808"><font color="blue">Human Posture Reconstruction for Through-the-Wall Radar Imaging Using Convolutional Neural Networks</font></a></h3>

<p><img src="https://pic.imgdb.cn/item/623d9f9f27f86abb2ac8c836.jpg" alt="" />
<img src="https://pic.imgdb.cn/item/623d9fb927f86abb2ac9633b.jpg" alt="" /></p>

<details><summary>Abstract：Click to Read</summary>Low imaging spatial resolution hinders through-the-wall radar imaging (TWRI) from reconstructing complete human postures. This letter mainly discusses a convolutional neural network (CNN)-based human posture reconstruction method for TWRI. The training process follows a supervision-prediction learning pipeline inspired by the cross-modal learning technique. Specifically, optical images and TWRI signals are collected simultaneously using a self-develop radar containing an optical camera. Then, the optical images are processed with a computer-vision-based supervision network to generate ground-truth human skeletons. Next, the same type of skeleton is predicted from corresponding TWRI signals using a prediction network. After training, the model shows complete predictions in wall-occlusive scenarios solely using TWRI signals. Experiments show comparable quantitative results with the state-of-the-art vision-based methods in nonwall-occlusive scenarios and accurate qualitative results with wall occlusion.</details>

<ul>
  <li><em>Authors</em>：<strong>Z.Zheng</strong>, J.Pan, Z.Ni, C.Shi, S.Ye and G.Fang</li>
  <li><em>Journal</em>：IEEE Geoscience and Remote Sensing Letters, 2021</li>
</ul>

<h3 id="-a-method-for-reducing-timing-jitters-impact-in-through-wall-human-detection-by-ultra-wideband-impulse-radar">⚪ <a href="https://www.mdpi.com/2072-4292/13/18/3577"><font color="blue">A Method for Reducing Timing Jitter’s Impact in Through-Wall Human Detection by Ultra-Wideband Impulse Radar</font></a></h3>

<p><img src="https://pic.imgdb.cn/item/623da17a27f86abb2ad51388.jpg" alt="" />
<img src="https://pic.imgdb.cn/item/623da1af27f86abb2ad69070.jpg" alt="" /></p>

<details><summary>Abstract：Click to Read</summary>Ultra-wideband (UWB) impulse radar is widely used for through-wall human respiration detection due to its high range resolution and high penetration capability. UWB impulse radar emits very narrow time pulses, which can directly obtain the impulse response of the target. However, the time interval between successive pulses emitted is not ideally fixed because of timing jitter. This results in the impulse response position of the same target not being fixed, but it is related to slow-time. The clutter scattered by the stationary target becomes non-stationary clutter, which affects the accurate extraction of the human respiration signal. In this paper, we propose a method for reducing timing jitter’s impact in through-wall human detection by UWB impulse radar. After the received signal is processed by the Fast Fourier transform (FFT) in slow-time, we model the range-frequency matrix in the frequency domain as a superposition of the low-rank representation of jitter-induced clutter data and the sparse representation of human respiratory data. By only extracting the sparse component, the impact of timing jitter in human respiration detection can be reduced. Both numerical simulated data and experimental data demonstrate that our proposed method can effectively remove non-stationary clutter induced by timing jitter and improve the accuracy of the human target signal extraction.</details>

<ul>
  <li><em>Authors</em>：C.Shi, Z.Ni, J.Pan, <strong>Z.Zheng</strong>, S.Ye and G.Fang</li>
  <li><em>Journal</em>：Remote Sensing, 2021</li>
</ul>

<h3 id="-motion-compensation-method-based-on-mfdf-of-moving-target-for-uwb-mimo-through-wall-radar-system">⚪ <a href="https://ieeexplore.ieee.org/document/9562984"><font color="blue">Motion Compensation Method Based on MFDF of Moving Target for UWB MIMO Through-Wall Radar System</font></a></h3>

<p><img src="https://pic.imgdb.cn/item/623da21a27f86abb2ad9bfd6.jpg" alt="" />
<img src="https://pic.imgdb.cn/item/623da2b527f86abb2addba55.jpg" alt="" /></p>

<details><summary>Abstract：Click to Read</summary>Ultrawideband (UWB) multiple-input–multiple-output (MIMO) radar is widely used for through-wall imaging (TWI) due to its excellent penetrability and large aperture. Multichannels in the MIMO radar system are usually time-division multiplexing based on microwave switches to reduce the complexity of the system in engineering. The switching process of the channel will bring time delay, which cannot be ignored in the TWI of the moving target. The switching time delay will cause the defocus and position shift of the TWI of the moving target. This letter proposes a motion compensation method based on multiframe data fusion (MFDF) used for correcting the echo of the through-wall moving target. A geometric model is established in the proposed method through the echo of the current frame and the next frame, and the compensated signal is obtained through the geometric solution. The proposed method is compared with before compensation and the traditional single-channel motion compensation algorithm (SCMCA) through simulation and experimental data verification. The visual images and quantitative results show that the proposed motion compensation method can obtain a good focus image of the through-wall moving target and reduce the positioning error.</details>

<ul>
  <li><em>Authors</em>：J.Pan, Z.Ni, C.Shi, <strong>Z.Zheng</strong>, S.Ye and G.Fang</li>
  <li><em>Journal</em>：IEEE Geoscience and Remote Sensing Letters, 2021</li>
</ul>

<h3 id="-structure-information-is-the-key-self-attention-roi-feature-extractor-in-3d-object-detection">⚪ <a href="https://arxiv.org/abs/2111.00931"><font color="blue">Structure Information is the Key: Self-Attention RoI Feature Extractor in 3D Object Detection</font></a></h3>

<p><img src="https://pic.imgdb.cn/item/623da33427f86abb2ae19b07.jpg" alt="" />
<img src="https://pic.imgdb.cn/item/623da35227f86abb2ae28cb9.jpg" alt="" /></p>

<details><summary>Abstract：Click to Read</summary>Unlike 2D object detection where all RoI features come from grid pixels, the RoI feature extraction of 3D point cloud object detection is more diverse. In this paper, we first compare and analyze the differences in structure and performance between the two state-of-the-art models PV-RCNN and Voxel-RCNN. Then, we find that the performance gap between the two models does not come from point information, but structural information. The voxel features contain more structural information because they do quantization instead of downsampling to point cloud so that they can contain basically the complete information of the whole point cloud. The stronger structural information in voxel features makes the detector have higher performance in our experiments even if the voxel features don't have accurate location information. Then, we propose that structural information is the key to 3D object detection. Based on the above conclusion, we propose a Self-Attention RoI Feature Extractor (SARFE) to enhance structural information of the feature extracted from 3D proposals. SARFE is a plug-and-play module that can be easily used on existing 3D detectors. Our SARFE is evaluated on both KITTI dataset and Waymo Open dataset. With the newly introduced SARFE, we improve the performance of the state-of-the-art 3D detectors by a large margin in cyclist on KITTI dataset while keeping real-time capability.</details>

<ul>
  <li><em>Authors</em>：D.Zhang, <strong>Z.Zheng</strong>, X.Bi and X.Liu</li>
  <li><em>Preprint</em>：arXiv:2111.00931</li>
</ul>

<h3 id="-declutter-gan-gpr-b-scan-data-clutter-removal-using-conditional-generative-adversarial-nets">⚪ <a href="https://ieeexplore.ieee.org/document/9736999"><font color="blue">Declutter-GAN: GPR B-scan Data Clutter Removal Using Conditional Generative Adversarial Nets</font></a></h3>

<p><img src="https://pic.imgdb.cn/item/623da45c27f86abb2ae78427.jpg" alt="" />
<img src="https://pic.imgdb.cn/item/623da4db27f86abb2aeb396e.jpg" alt="" /></p>

<details><summary>Abstract：Click to Read</summary>Clutter removal in ground-penetrating radar (GPR) B-scan data has been widely studied in recent years. In this letter, we propose a novel data-driven clutter suppression method in GPR data based on conditional generative adversarial nets (cGANs). The proposed method learns a function that maps the cluttered data to the clutter-free data from the training set. The training set consists of pairs of cluttered data and corresponding clutter-free data. Different from the traditional method that only uses the simulation training set, we simulate the clutter-free data and add the real collected non-target data to the simulated clutter-free data as cluttered data, so that the trained network can generalize well to the real GPR data. The proposed method is compared with the subspace method, sparse representation-based method, and low-rank and sparse matrix decomposition methods (LRSD) on both simulation data and real collected data. The results show that the proposed method has higher performance in terms of computational complexity, clutter suppression results, and applicability than those state-of-the-art methods.</details>

<ul>
  <li><em>Authors</em>：Z.Ni, C.Shi, J.Pan, <strong>Z.Zheng</strong>, S.Ye and G.Fang</li>
  <li><em>Journal</em>：IEEE Geoscience and Remote Sensing Letters, 2022</li>
</ul>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2020/01/02/DL-outline.html" class="read-next-link"></a>
        <section>
          <span>深度学习(Deep Learning)概述</span>
          <p>  Outlines about Deep Learning.</p>
        </section>
        
        <div class="filter"></div>
        <img src="" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2020/01/01/ML-outline.html" class="read-next-link"></a>
          <section>
            <span>机器学习(Machine Learning)概述</span>
            <p>  Outlines about Machine Learning.</p>
          </section>
          
          <div class="filter"></div>
          <img src="" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
