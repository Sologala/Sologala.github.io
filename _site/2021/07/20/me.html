<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Maximum Entropy：最大熵模型 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Maximum Entropy：最大熵模型">
    <meta name="keywords"  content="机器学习">
    <!-- Open Graph -->
    <meta property="og:title" content="Maximum Entropy：最大熵模型 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2021/07/20/me.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="post-tag">机器学习</a>
          
        
      </div>
      <h1>Maximum Entropy：最大熵模型</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="21-07-20"><i class="iconfont icon-date"></i>20 Jul 2021</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic1.imgdb.cn/item/634e4a9f16f2c2beb14bec87.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Maximum Entropy.</p>
</blockquote>

<h1 id="1-熵">1. 熵</h1>
<p><strong>熵(entropy)</strong>既是<strong>不确定性</strong>的度量，又是<strong>信息</strong>的度量。对于某个事件，其熵越大，事件的不确定性越大，我们能从中获得的信息量也越大。若用概率分布$p(x)$描述事件，则离散形式和连续形式的熵分别计算如下：</p>

\[\begin{aligned} S &amp; = -\sum_{x}^{}p(x) \log  p(x) \\ S &amp; = -\int_{x}^{}p(x) \log  p(x)dx \end{aligned}\]

<p>上式中的$\log $通常采用自然对数，也可以用$2$为底。一般地，任何大于$1$的底都是成立的，相当于改变了信息的“单位”。</p>

<p>在定义熵时，我们希望能够构造出一个表示信息量的公式。上述公式并不是灵光一现得到的，而是通过人为定义衡量信息量的“熵”应该有的性质，从而推导出来的。下面以离散形式为例介绍这个推导过程。</p>

<p>“熵”既然用来表示一个概率分布$p(x)$的信息量，则它应该有以下性质：</p>

<p><strong>① 熵是概率分布$p(x)$的函数，并且是光滑函数。</strong></p>

<p><strong>② 熵具有可加性：</strong></p>

\[S[p(x)] = \sum_{x}^{} f(p(x))\]

<p><strong>③ 当X和Y是独立随机变量时，有：</strong></p>

\[S[p(x)p(y)] = S[p(x)] + S[p(y)]\]

<p>由上述三条性质，即可确定熵的表达式。假设X和Y是二元随机变量，X的概率分布为$p,1-p$，Y的概率分布为$q,1-q$，XY的联合概率分布为$pq,(1-p)q,p(1-q),(1-p)(1-q)$，则按照上述性质，应有：</p>

\[\begin{aligned}&amp; f(pq)+f((1-p)q)+f(p(1-q))+f((1-p)(1-q)) \\&amp;= f(p)+f(1-p)+f(q)+f(1-q) \end{aligned}\]

<p>等式左端是自变量的积的形式，右端是单个自变量的形式，不妨通过取对数运算把乘积变成求和。设$f(x)=h(x) \log  x$，则有：</p>

\[\begin{aligned} &amp;h(pq) \log  (pq)+h((1-p)q) \log  ((1-p)q)\\+&amp;h(p(1-q)) \log  (p(1-q))+h((1-p)(1-q)) \log  ((1-p)(1-q))  \\=&amp; h(p) \log  (p)+h(1-p) \log  (1-p)+h(q) \log  (q)+h(1-q) \log  (1-q) \end{aligned}\]

<p>上式整理得：</p>

\[\begin{aligned} &amp;[h(pq)+h(p(1-q))-h(p)] \log  (p)\\+&amp;[h(pq)+h((1-p)q)-h(q)] \log  (q) \\+ &amp;[h((1-p)q)+h((1-p)(1-q))-h(1-p)] \log  (1-p) \\+&amp; [h(p(1-q))+h((1-p)(1-q))-h(1-q)] \log  (1-q) \\=&amp;0 \end{aligned}\]

<p>注意到若$h(\cdot)$取线性函数，则上式每一项的系数为$0$，等式自然满足。因此不妨取$h(x)=\alpha x$，则$f(x)=\alpha x \log  x$，对应熵的表达式：</p>

\[S[p(x)] = \sum_{x}^{} \alpha p(x) \log  p(x)\]

<p>上式中的$\alpha$数值并不重要，只影响信息量的单位，其符号比较重要，因此增加性质：</p>

<p><strong>④ 信息量越大，熵越大。</strong></p>

<p>根据定义④，确定事件的熵应该比不确定事件的熵小。对于确定事件，其熵$S = \sum_{x}^{} \alpha  \log  1 = 0$；则对于不确定事件，应满足：</p>

\[S[p(x)] = \sum_{x}^{} \alpha p(x) \log  p(x) &gt; 0\]

<p>注意到$p(x) \in [0,1]$，因此$\alpha&lt;0$，不妨取$-1$，此即熵的计算公式。</p>

<p>一些特殊的熵：</p>
<ul>
  <li><strong>联合熵</strong>：多元分布的熵(衡量多元分布的信息量)</li>
</ul>

\[S[p(x,y)] = -\sum_{x}^{} \sum_{y}^{} p(x,y) \log  p(x,y)\]

<ul>
  <li><strong>条件熵</strong>：条件分布的熵(衡量已知边缘分布的条件下联合分布的信息量)</li>
</ul>

\[\begin{aligned} S(Y|X) &amp;= S[p(x,y)]-S[p(x)] \text{ ( 定义① ) } \\&amp;= -\sum_{x}^{} \sum_{y}^{} p(x,y) \log  p(x,y)+\sum_{x}^{}  p(x) \log  p(x) \\ &amp;= -\sum_{x}^{} \sum_{y}^{} p(x)p(y|x) \log  p(x)p(y|x)+\sum_{x}^{}  p(x) \log  p(x)  \\ &amp;= -\sum_{x}^{} \sum_{y}^{} p(x)p(y|x) \log  p(x) - \sum_{x}^{} \sum_{y}^{} p(x)p(y|x) \log  p(y|x)+\sum_{x}^{}  p(x) \log  p(x) \\ &amp;= - \sum_{x}^{} p(x)\sum_{y}^{} p(y|x) \log  p(y|x) = \sum_{x}^{} p(x)S(Y|X=x) \text{ ( 定义② )} \\ &amp;= - \sum_{x}^{} \sum_{y}^{} p(x,y) \log  p(y|x)  \text{ ( 定义③ )} \end{aligned}\]

<h1 id="2-最大熵原理-the-maximum-entropy-principle">2. 最大熵原理 The Maximum Entropy Principle</h1>
<p><strong>最大熵原理(Maximum Entropy Principle)</strong>指出对于一个未知的概率分布，在只掌握其部分知识的前提下，应选取符合这部分知识的同时使得<strong>熵最大</strong>的概率分布形式。</p>

<h2 id="1-离散形式的最大熵原理">(1) 离散形式的最大熵原理</h2>
<p>对于某个概率分布$p(x)$，通过生活经验或先验信息可能会获得该分布的部分知识，将其以期望形式给出约束：</p>

\[E[f(x)] = \sum_{x}^{} p(x)f(x) = \tau\]

<p>注意到$f(x)=1,\tau=1$时该约束是概率之和为$1$，是一个平凡的约束。一般地，假设有$k$个约束，则最大熵原理等价于一个带有约束的极值问题，采用拉格朗日乘子法求解：</p>

\[\begin{aligned} L(p(x),\lambda) = &amp;-\sum_{x}^{}p(x)\log p(x)-\lambda_0(\sum_{x}^{} p(x)-1)-\lambda_1(\sum_{x}^{} p(x)f_1(x)  -\tau_1)\\&amp;- \cdots -\lambda_k(\sum_{x}^{} p(x)f_k(x)  -\tau_k) \end{aligned}\]

<p>对上式求偏导令其为零$\frac{\partial L}{\partial p(x)}=0$，可得：</p>

\[- \log p(x) -1-\lambda_0-\lambda_1f_1(x)- \cdots -\lambda_kf_k(x)=0\]

<p>解得：</p>

\[p(x) = e^{-1-\lambda_0-\sum_{i=1}^{k}\lambda_if_i(x)}\]

<p>注意到$\sum_{x}^{} p(x)=1$，因此：</p>

\[\sum_{x}^{} p(x)=\sum_{x}^{} e^{-1-\lambda_0-\sum_{i=1}^{k}\lambda_if_i(x)} = \sum_{x}^{}e^{-1-\lambda_0}e^{-\sum_{i=1}^{k}\lambda_if_i(x)}=1\]

<p>因此$e^{-1-\lambda_0}=\frac{1}{\sum_{x}^{}e^{-\sum_{i=1}^{k}\lambda_if_i(x)}}$，将其分母记为归一化因子$Z$，代回原式可得：</p>

\[p(x) = \frac{e^{-\sum_{i=1}^{k}\lambda_if_i(x)}}{\sum_{x}^{}e^{-\sum_{i=1}^{k}\lambda_if_i(x)}} = \frac{1}{Z}e^{-\sum_{i=1}^{k}\lambda_if_i(x)}\]

<p>将$p(x)$的表达式代入：</p>

\[\sum_{x}^{} p(x)f_i(x) - \tau_i = 0, \quad (i=1,2,...,k)\]

<p>即可解出未知的$\lambda_i$。然而对于一般的$f_i(x)$，上式并没有解析解，甚至数值求解都比较困难。</p>

<h2 id="2-连续形式的最大熵原理">(2) 连续形式的最大熵原理</h2>
<p>对于连续形式的最大熵原理，其求解过程和结果与上一节类似，即求解如下约束最优化问题：</p>

\[\begin{aligned} L(p(x),\lambda) = &amp;-\int_{x}^{}p(x)\log p(x)dx-\lambda_0(\int_{x}^{} p(x)dx-1)-\lambda_1(\int_{x}^{} p(x)f_1(x)dx-  \tau_1)\\&amp;-...-\lambda_k(\int_{x}^{} p(x)f_k(x)dx-  \tau_k) \end{aligned}\]

<p>求解结果如下：</p>

\[p(x) = \frac{e^{-\sum_{i=1}^{k}\lambda_if_i(x)}}{\int_{x}^{}e^{-\sum_{i=1}^{k}\lambda_if_i(x)}dx} = \frac{1}{Z}e^{-\sum_{i=1}^{k}\lambda_if_i(x)}\]

<p>求解未知的$\lambda_i$仍然需要将$p(x)$的表达式代入：</p>

\[\int_{x}^{} p(x)f_i(x)dx - \tau_i = 0, \quad (i=1,2,...,k)\]

<p>一些特殊形式的连续型最大熵原理可以求得解析解，下面介绍两种形式。</p>

<h3 id="-已知变量x的均值-fxx">⚪ 已知变量x的均值 $f(x)=x$</h3>
<p>($x$的取值为$[0,+∞)$)此时解的形式为：</p>

\[p(x) = \frac{1}{Z}e^{-\lambda x}\]

<p>先求归一化因子$Z$：</p>

\[Z = \int_{0}^{+∞} e^{-\lambda x} dx = \frac{1}{\lambda}\]

<p>则概率分布可以表示成：</p>

\[p(x) = \lambda e^{-\lambda x}\]

<p>引入均值$\tau$约束：</p>

\[\tau = \int_{x}^{} p(x)xdx = \int_{0}^{+∞} \lambda e^{-\lambda x}xdx = \frac{1}{\lambda} \int_{0}^{+∞} e^{-t}tdt = \frac{1}{\lambda}\]

<p>故所求概率分布可以表示成：</p>

\[p(x) = \frac{1}{\tau} e^{-\frac{x}{\tau}}\]

<h3 id="-已知变量x的均值和方差-f_1xxf_2xx2">⚪ 已知变量x的均值和方差 $f_1(x)=x,f_2(x)=x^2$</h3>
<p>此时解的形式为：</p>

\[p(x) = \frac{1}{Z}e^{-\lambda_1x-\lambda_2x^2}\]

<p>先求归一化因子$Z$：</p>

\[\begin{aligned} Z &amp;= \int_{-∞}^{+∞} e^{-\lambda_1x-\lambda_2x^2} dx = \int_{-∞}^{+∞} e^{-\lambda_2(x+\frac{\lambda_1}{2\lambda_2})^2+\frac{\lambda_1^2}{4\lambda_2}} dx \\ &amp;= e^{\frac{\lambda_1^2}{4\lambda_2}} \int_{-∞}^{+∞} e^{-(\sqrt{\lambda_2}t)^2} \frac{dt}{\sqrt{\lambda_2}} \\ &amp;(\text{由 } \int_{-∞}^{+∞} e^{-x^2}dx=\sqrt{\pi}) \\ &amp;= \sqrt{\frac{\pi}{\lambda_2}}e^{\frac{\lambda_1^2}{4\lambda_2}} \end{aligned}\]

<p>则概率分布可以表示成：</p>

\[p(x) = \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}}e^{-\lambda_1x-\lambda_2x^2}\]

<p>引入均值$\tau_1$和方差$\tau_2$约束：</p>

\[\begin{aligned} \tau_1 &amp;= \int_{x}^{} p(x)xdx \\ &amp;= \int_{-∞}^{+∞} \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}}e^{-\lambda_1x-\lambda_2x^2}xdx \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}} \int_{-∞}^{+∞} e^{-\lambda_2(x+\frac{\lambda_1}{2\lambda_2})^2}e^{\frac{\lambda_1^2}{4\lambda_2}}xdx \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2(x+\frac{\lambda_1}{2\lambda_2})^2} (x+\frac{\lambda_1}{2\lambda_2}-\frac{\lambda_1}{2\lambda_2})d(x+\frac{\lambda_1}{2\lambda_2}) \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2t^2} tdt + \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2t^2} (-\frac{\lambda_1}{2\lambda_2})dt \\ &amp;= 0+\sqrt{\frac{\lambda_2}{\pi}} (-\frac{\lambda_1}{2\lambda_2}) \sqrt{\frac{\pi}{\lambda_2}}  = -\frac{\lambda_1}{2\lambda_2} \end{aligned}\]

\[\begin{aligned} \tau_2 &amp;= \int_{x}^{} p(x)x^2dx \\ &amp;= \int_{-∞}^{+∞} \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}}e^{-\lambda_1x-\lambda_2x^2}x^2dx \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}}e^{-\frac{\lambda_1^2}{4\lambda_2}} \int_{-∞}^{+∞} e^{-\lambda_2(x+\frac{\lambda_1}{2\lambda_2})^2}e^{\frac{\lambda_1^2}{4\lambda_2}}x^2dx \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2(x+\frac{\lambda_1}{2\lambda_2})^2}[(x+\frac{\lambda_1}{2\lambda_2})^2-\frac{\lambda_1}{2\lambda_2}(x+\frac{\lambda_1}{2\lambda_2})+\frac{\lambda_1^2}{4\lambda_2^2}]d(x+\frac{\lambda_1}{2\lambda_2}) \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2t^2}t^2dt + \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2t^2}(-\frac{\lambda_1}{2\lambda_2}t)dt + \sqrt{\frac{\lambda_2}{\pi}} \int_{-∞}^{+∞} e^{-\lambda_2t^2}(\frac{\lambda_1^2}{4\lambda_2^2})dt \\ &amp;= \sqrt{\frac{\lambda_2}{\pi}}\frac{1}{\lambda_2\sqrt{\lambda_2}}(0+\frac{\sqrt{\pi}}{2})+0+\sqrt{\frac{\lambda_2}{\pi}}\frac{\lambda_1^2}{4\lambda_2^2}\sqrt{\frac{\pi}{\lambda_2}} = -\frac{\lambda_1^2+2\lambda_2}{4\lambda_2^2} \end{aligned}\]

<p>联立上式可求得$\lambda_1=-\frac{\tau_1}{(\tau_2-\tau_1^2)}$,$\lambda_2=\frac{1}{2(\tau_2-\tau_1^2)}$，故所求概率分布可以表示成：</p>

\[p(x) = \sqrt{\frac{1}{2\pi (\tau_2-\tau_1^2)}}e^{-\frac{(x-\tau_1)^2}{2(\tau_2-\tau_1^2)}}\]

<p>上式即均值为$\tau_1$,方差为$\tau_2-\tau_1^2$的<strong>正态分布</strong>。</p>

<h1 id="3-最大熵模型">3. 最大熵模型</h1>
<p>最大熵模型将<strong>分类</strong>问题建模为<strong>条件分布</strong>，将模型的输入和输出都看作随机变量，即求给定输入$x$的条件下输出$y$的概率分布$p(Y|X)$，求解过程依据最大熵原理，即求下述<strong>条件熵</strong>的极值：</p>

\[S(Y|X) = S[p(x,y)] - S[p(x)]\]

<p>输入样本的分布$p(x)$是未知的，但是可以通过对已有样本的大量统计得出其经验分布。因此求条件熵的最大值等价于求$S[p(x,y)]$的最大值。</p>

<p>人为定义输入样本的<strong>特征</strong>(通常把输入的每个维度看作一个特征)，若输入样本的第$d$维特征满足某一条件(如大于某数值)，且该样本的分类类别为$c$，则可以构造<strong>特征函数</strong>：</p>

\[\chi(x,y) = \begin{cases} 1, \text{ 当x的第d维特征满足一定条件且y=c时} \\ 0,  \text{ 其他情况} \end{cases}\]

<p>统计满足该特征函数的样本数比例$\tau=\frac{N_{\chi}}{N}$，则可以引入约束：</p>

\[E[\chi(x,y)] = \sum_{x,y}^{} p(x,y)\chi(x,y) = \tau\]

<p>一般地，引入$k$个特征函数作为约束，上式为约束极值问题，由最大熵原理可以得到：</p>

\[p(x,y)=\frac{1}{Z}e^{-\sum_{i=1}^{k}\lambda_i \chi_i(x,y)}, \quad Z=\sum_{x,y}^{} e^{-\sum_{i=1}^{k}\lambda_i \chi_i(x,y)}\]

<p>若最终关注$p(y|x)$，结果为：</p>

\[p(y|x) = \frac{p(x,y)}{p(x)} = \frac{1}{Z \times p(x)}e^{-\sum_{i=1}^{k}\lambda_i \chi_i(x,y)}\]

<p>最大熵模型形式简单，但求解困难，一般需要通过数值方法求解，一些求解算法可参考<a href="https://blog.csdn.net/itplus/article/details/26550369">Referecne</a>。</p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2021/07/21/diversity.html" class="read-next-link"></a>
        <section>
          <span>集成学习中的多样性度量(Diversity Measure)</span>
          <p>  Diversity Measure in Ensemble Learning.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/611deab74907e2d39c3e9f9c.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2021/07/19/bbn.html" class="read-next-link"></a>
          <section>
            <span>BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</span>
            <p>  BBN：通过累积学习进行长尾分类的双分支网络.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/60f508195132923bf8403c92.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
