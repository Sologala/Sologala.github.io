<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>卷积神经网络中的池化(Pooling)层 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="卷积神经网络中的池化(Pooling)层">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="卷积神经网络中的池化(Pooling)层 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2021/07/02/pool.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>卷积神经网络中的池化(Pooling)层</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="21-07-02"><i class="iconfont icon-date"></i>02 Jul 2021</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/60dec5105132923bf86c3fca.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Pooling Layers.</p>
</blockquote>

<p><strong>池化(pooling)</strong>是卷积神经网络中的重要组成部分。通过池化可以对特征图(<strong>feature map</strong>)进行降采样，从而减小网络的模型参数量和计算成本，也在一定程度上降低过拟合的风险。池化的作用包括：</p>
<ol>
  <li>通过降采样增大网络的感受野</li>
  <li>通过信息提取抑制噪声，进行特征选择，降低信息的冗余</li>
  <li>通过减小特征图的尺寸降低模型计算量，降低网络优化难度，减少过拟合的风险</li>
  <li>使模型对输入图像中的特征位置变化(变形、扭曲、平移)更加鲁棒</li>
</ol>

<p>本文介绍卷积神经网络中的池化方法，包括：</p>
<ul>
  <li>通用的池化方法
    <ol>
      <li><strong>Max Pooling</strong> 最大池化</li>
      <li><strong>Average Pooling</strong> 平均池化</li>
      <li><strong>Mix Pooling</strong> 混合池化</li>
      <li><strong>Fractional Max-Pooling</strong> 分数最大池化</li>
      <li><strong>Power Average Pooling</strong> 幂平均池化</li>
      <li><strong>Stochastic Pooling</strong> 随机池化</li>
      <li><strong>Stochastic Spatial Sampling Pooling (S3Pool)</strong> 随机空间采样池化</li>
      <li><strong>Detail-Preserving Pooling (DPP)</strong> 细节保留池化</li>
      <li><strong>Local Importance Pooling (LIP)</strong> 局部重要性池化</li>
      <li><strong>Soft Pooling</strong> 软池化</li>
      <li><strong>Dynamically Optimized Pooling (DynOPool)</strong> 动态优化池化</li>
    </ol>
  </li>
  <li>为下游任务设计的池化方法
    <ol>
      <li><strong>Global Average Pooling</strong> 全局平均池化</li>
      <li><strong>Covariance Pooling</strong> 协方差池化</li>
      <li><strong>Spatial Pyramid Pooling (SPP)</strong> 空间金字塔池化</li>
      <li><strong>Region of Interest Pooling (RoI Pooling)</strong> 感兴趣区域池化</li>
      <li><strong>Bilinear Pooling</strong> 双线性池化</li>
    </ol>
  </li>
</ul>

<h1 id="-通用的池化方法">⚪ 通用的池化方法</h1>

<h2 id="1-最大池化-max-pooling">1. 最大池化 Max Pooling</h2>
<p><strong>最大池化(Max Pooling)</strong>是将输入的图像特征划分为若干个可重叠的子区域，对每个子区域$R$输出其最大值：</p>

\[\tilde{a} = \mathop{\max}_{i \in R} a_i\]

<p><img src="https://pic.imgdb.cn/item/63aaca9a08b6830163bfe166.jpg" alt="" /></p>

<p>最大池化只选择每个矩形子区域中的最大值，即提取特征图中响应最强烈的部分进入下一层，这种操作能够过滤网络中大量的冗余信息，但也会丢失特征图中的一些细节信息。</p>

<p>可以使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">torch.nn.MaxPool2d</a>构造最大池化层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">maxpool_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># 池化窗口的大小
</span>    <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c1"># 池化操作的步长，默认等于窗口大小
</span>    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>   <span class="c1"># 零像素的边缘填充数量
</span>    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># 扩张元素的数量
</span>    <span class="n">return_indices</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># 返回池化取值的索引，并通过nn.MaxUnpool2d()进行反池化
</span>    <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 在输出尺寸中是否使用向上取整代替向下取整
</span>    <span class="p">)</span>
</code></pre></div></div>

<h2 id="2-平均池化-average-pooling">2. 平均池化 Average Pooling</h2>
<p><strong>平均池化(Average Pooling)</strong>是将输入的图像特征划分为若干个可重叠的子区域，对每个子区域$R$输出其平均值：</p>

\[\tilde{a} = \sum_{i \in R}^{} \frac{a_i}{|R|}\]

<p><img src="https://pic.imgdb.cn/item/63aaca7108b6830163bfa128.jpg" alt="" /></p>

<p>平均池化选择每个矩形子区域中的平均值，可以提取特征图中所有特征的平均信息，从而保留更多的图像背景信息。</p>

<p>可以使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d">torch.nn.AvgPool2d</a>构造平均池化层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avgpool_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">AvgPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># 池化窗口的大小
</span>    <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c1"># 池化操作的步长，默认等于窗口大小
</span>    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>   <span class="c1"># 零像素的边缘填充数量
</span>    <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 在输出尺寸中是否使用向上取整代替向下取整
</span>    <span class="n">count_include_pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># 计算均值时是否考虑填充像素
</span>    <span class="n">divisor_override</span><span class="o">=</span><span class="bp">None</span> <span class="c1"># 若指定将用作平均操作中的除数
</span>    <span class="p">)</span>
</code></pre></div></div>

<h2 id="3-混合池化-mix-pooling">3. 混合池化 Mix Pooling</h2>
<ul>
  <li>论文：<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.678.7068&amp;rep=rep1&amp;type=pdf">Mixed Pooling for Convolutional Neural Networks</a></li>
</ul>

<p>最大池化和平均池化各有利弊，<strong>混合池化(Mix Pooling)</strong>则通过引入一个取值为$0$或$1$的随机数$\lambda$，每次池化操作随机地选用其中之一：</p>

\[\tilde{a} = \lambda \cdot \mathop{\max}_{i \in R} a_i + (1-\lambda) \cdot \sum_{i \in R}^{} \frac{a_i}{|R|}\]

<p>混合池化优于传统的最大池化和平均池化，缓解了过拟合的风险；并且该方法所需的计算开销可以忽略不计。</p>

<h2 id="4-分数最大池化-fractional-max-pooling">4. 分数最大池化 Fractional Max-Pooling</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1412.6071">Fractional Max-Pooling</a></li>
</ul>

<p><strong>分数最大池化(Fractional Max-Pooling)</strong>把输入尺寸为$(N_{in},N_{in})$的区域随机划分为与输出尺寸相同的$N_{out}\times N_{out}$块不均匀的子区域，并对每个子区域执行最大池化操作。</p>

<p>子区域的划分通过随机产生的序列构成。若设置池化窗口的大小为$r$，则随机构造两个递增整数序列\((a_i)_{i=0}^{N_{out}}, (b_j)_{j=0}^{N_{out}}\)，序列的起始值为$1$，终止值为$N_{in}$，并且相邻元素之间的递增量取值范围是$[1,…,r]$。则子区域的划分有不相交和重叠两种形式：</p>

\[P=[a_{i-1},a_i-1]\times [b_{j-1},b_j-1] \text{  or  } P_{i,j} =[a_{i-1},a_i]\times [b_{j-1},b_j]\]

<p>可以使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d">torch.nn.FractionalMaxPool2d</a>构造分数最大池化层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fmp_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">FractionalMaxPool2d</span><span class="p">(</span>
    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># 池化窗口的大小，应设置为k=N_{out}/N_{in}
</span>    <span class="n">output_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>  <span class="c1"># 输出特征的空间尺寸
</span>    <span class="n">output_ratio</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c1"># 输出尺寸与输入尺寸的比值 N_{in}/N_{out}
</span>    <span class="n">return_indices</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 返回池化取值的索引，并通过nn.MaxUnpool2d()进行反池化
</span>    <span class="p">)</span>
</code></pre></div></div>

<h2 id="5-幂平均池化-power-average-pooling">5. 幂平均池化 Power Average Pooling</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1311.4025">Signal recovery from Pooling Representations</a></li>
</ul>

<p><strong>幂平均池化(Power Average Pooling)</strong>是将输入的图像特征划分为若干个可重叠的子区域，对每个子区域$R$输出其$L_p$范数：</p>

\[\tilde{a} = \sqrt[p]{\sum_{i \in R}^{} a_i^p}\]

<p>当$p=1$时，该池化方法与平均池化仅相差一个常数；当$p→∞$时，该池化方法等价于最大池化。</p>

<p>可以使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d">torch.nn.LPPool2d</a>构造幂平均池化层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lpp_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LPPool2d</span><span class="p">(</span>
    <span class="n">norm_type</span><span class="p">,</span>   <span class="c1"># l_p范数的系数p
</span>    <span class="n">kernel_size</span><span class="p">,</span> <span class="c1"># 池化窗口的大小
</span>    <span class="n">stride</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c1"># 池化操作的步长，默认等于窗口大小
</span>    <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># 在输出尺寸中是否使用向上取整代替向下取整
</span>    <span class="p">)</span>
</code></pre></div></div>

<h2 id="6-随机池化-stochastic-pooling">6. 随机池化 Stochastic Pooling</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1301.3557v1">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks</a></li>
</ul>

<p><strong>随机池化(Stochastic Pooling)</strong>的思路是先对池化窗口中的元素进行归一化，得到概率矩阵。再按其概率按照多项分布(<strong>multinomial</strong>)采样选择一个元素作为池化输出结果:</p>

<p><img src="https://pic.imgdb.cn/item/60df081c5132923bf8b69c29.jpg" alt="" /></p>

<p>在反向传播时，只需保留前向传播被选中元素的索引，将梯度反传到该位置，其实现过程与最大池化的反向传播类似。随机池化特征图中的元素按照其概率值大小随机选择，数值大的元素被选中的概率也大，它不像最大池化只取最大元素值，因此随机池化具有更强的泛化能力。</p>

<h2 id="7-随机空间采样池化-stochastic-spatial-sampling-pooling-s3pool">7. 随机空间采样池化 Stochastic Spatial Sampling Pooling (S3Pool)</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1611.05138">S3Pool: Pooling with Stochastic Spatial Sampling</a></li>
</ul>

<p><strong>S3 (Stochastic Spatial Sampling)</strong>池化包括两个步骤。</p>
<ol>
  <li>使用给定尺寸的最大或平均池化窗口按步长为$1$沿特征图滑动，生成空间尺寸不变的新特征图，假设尺寸为$h \times w$；</li>
  <li>给定超参数<strong>栅格尺寸(grid size)</strong> $g$，将特征图划分为$\frac{h}{g} \times \frac{w}{g}$个子区域。在每个区域中随机采样$\frac{g}{s}$行$\frac{g}{s}$列，即可得到降采样后尺寸为$\frac{h}{s} \times \frac{w}{s}$的特征图。</li>
</ol>

<p><img src="https://pic.imgdb.cn/item/60dfc5ae5132923bf882c20c.jpg" alt="" /></p>

<p><strong>S3</strong>池化相比于随机池化增加了更多的随机性，相当于一个更强的正则化方法，能够提高模型的表达能力。此外<strong>S3</strong>池化可以看作一种隐式的针对特征图的数据增强方法。</p>

<h2 id="8-细节保留池化-detail-preserving-pooling-dpp">8. 细节保留池化 Detail-Preserving Pooling (DPP)</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1804.04076">Detail-Preserving Pooling in Deep Networks</a></li>
</ul>

<p><strong>细节保留池化(Detail-Preserving Pooling, DPP)</strong>认为不同空间位置的像素$q$对中心像素$p$的影响是不同的，因此引入通过可学习参数$\alpha$,$\lambda$控制的权重$w[p,q]$衡量像素$p,q$之间的相关性。对于图像$I$中像素点$p$处的池化计算，考虑其邻域$\Omega_{p}$中的像素点$q$对其影响：</p>

\[D_{\alpha,\lambda}(I)[p] =\frac{1}{\sum_{q' \in \Omega_{p}}^{} w_{\alpha,\lambda}[p,q']} \sum_{q \in \Omega_{p}}^{} w_{\alpha,\lambda}[p,q]I[q]\]

<p>权重$w$受可学习参数$\alpha$和$\lambda$控制：</p>

\[w_{\alpha,\lambda}[p,q] = \alpha + ρ_{\lambda}(I[q]-\tilde{I}[p]) , \quad \tilde{I}_F[p]=\sum_{q \in \tilde{\Omega}_p}^{}F[q]I[q]\]

<p>其中$F[\cdot]$是一个可学习的滤波器算子，$\alpha$是一个全局的奖励参数，$\lambda$用于控制一个奖励函数$ρ(\cdot)$决定像素$q$对像素$p$的影响。$ρ(\cdot)$有两种形式：</p>
<ul>
  <li>对称(<strong>symmetric</strong>)形式:
\(ρ_{\text{Sym}(x)} = (\sqrt{x^2+\epsilon^2})^{\lambda}\)</li>
  <li>非对称(<strong>asymmetric</strong>)形式:
\(ρ_{\text{Asym}(x)} = (\sqrt{\max(0,x)^2+\epsilon^2})^{\lambda}\)</li>
</ul>

<p><strong>DPP</strong>池化可以放大空间变化并保留重要的图像结构细节，通过引入可学习的参数控制细节的保存量；此外，<strong>DPP</strong>池化可以与随机池化等方法结合使用，以进一步提高准确率。</p>

<h2 id="9-局部重要性池化-local-importance-pooling-lip">9. 局部重要性池化 Local Importance Pooling (LIP)</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1908.04156">LIP: Local Importance-based Pooling</a></li>
</ul>

<p><strong>局部重要性池化(Local Importance Pooling, LIP)</strong>是一种通过学习基于输入的自适应重要性权重，从而在采样过程中自动增强特征提取能力的池化方法。具体地，通过一个子网络自动学习输入特征的重要性；这也可以看作是一种局部注意力方法，通过局部卷积产生注意力权值，并在局部归一化。</p>

<p>对于输入特征$I$，首先生成一个自适应重要性权重图$F(I)$(与输入特征尺寸相同，代表每个像素位置的重要性)。为了使重要性权重非负且易于优化，使用指数构造权重图：</p>

\[F(I)=\exp(\mathcal{G}(I))\]

<p>其中$\mathcal{G}(\cdot)$被称为<strong>对数模块(Logit Module)</strong>，因为其为重要性权重的对数。在实践中，对数模块$\mathcal{G}(\cdot)$是由一个子网络实现的，作者给出了两种实现结构：</p>

<p><img src="https://pic.imgdb.cn/item/63aaec6308b6830163fb25f6.jpg" alt="" /></p>

<p>在池化过程中，对于特征位置$(x,y)$，选取其邻域$\Omega$，对该邻域的重要性权重进行归一化后作用于原特征的局部滑动窗口，计算得到池化后的特征：</p>

\[O_{x',y'} = \frac{\sum_{(\Delta x,\Delta y) \in \Omega}^{} F(I)_{x+\Delta x,y+\Delta y}I_{x+\Delta x,y+\Delta y}}{\sum_{(\Delta x,\Delta y) \in \Omega}^{} F(I)_{x+\Delta x,y+\Delta y}}\]

<p><strong>LIP</strong>可以自适应地学习具有可判别性的特征图，汇总下采样特征同时丢弃无信息特征。这种池化机制能保留物体大部分细节，对于细节信息异常丰富的任务至关重要。</p>

<h2 id="10-软池化-soft-pooling">10. 软池化 Soft Pooling</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/2101.00440">Refining activation downsampling with SoftPool</a></li>
</ul>

<p><strong>软池化(soft pooling)</strong>可以在保持池化层功能的同时尽可能减少池化过程中带来的信息损失，更好地保留信息特征并改善网络的性能。与最大池化等不同，软池化是可微的，通过反向传播回传梯度，有助于网络进一步训练。</p>

<p>软池化的步骤如下：</p>
<ol>
  <li>通过滑动窗口在特征图上选择局部区域$R$；</li>
  <li>通过<strong>softmax</strong>计算特征区域的每个像素点权重:
\(w_i = \frac{e^{a_i}}{\sum_{j \in R}^{}e^{a_i}}\)</li>
  <li>将相应的特征数值与权重相乘后求和获得输出结果:
\(\tilde{a} = \sum_{i \in R}^{}w_i \cdot a_i\)</li>
</ol>

<p><img src="https://pic.imgdb.cn/item/63aaecd908b6830163fbfaf6.jpg" alt="" /></p>

<p>软池化使得整体的局部数值都有所贡献，并且重要的特征占有较高的权重，能够保留更多信息。</p>

<h2 id="11-动态优化池化-dynamically-optimized-pooling-dynopool">11. 动态优化池化 Dynamically Optimized Pooling (DynOPool)</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/2205.15254">Pooling Revisited: Your Receptive Field is Suboptimal</a></li>
</ul>

<p><strong>动态优化池化 (DynOPool)</strong>通过学习每一层感受野的最佳大小和形状来调整特征映射的空间尺寸。<strong>DynOPool</strong>通过参数$r=(r_h,r_w)$控制感受野的大小，把空间尺寸为$H_{in} \times W_{in}$的输入特征划分为$H_{out} \times W_{out}$个栅格：</p>

\[\begin{aligned} H_{out} &amp;= \lfloor H_{in} \cdot r_h  \rceil \\ W_{out} &amp;= \lfloor W_{in} \cdot r_w  \rceil \end{aligned}\]

<p><strong>DynOPool</strong>进一步通过参数$\delta=( \delta_h,\delta_w)$控制感受野的形状。对于某个栅格中心$p=(p_h,p_w)$，构造四个查询点$q=(p_h ± \delta_h,p_w± \delta_w)$，四个查询点的像素值通过双线性插值获得。则该栅格的输出可以通过四个查询点的聚合函数(比如最大池化)构造。</p>

<p><img src="https://pic.imgdb.cn/item/63aaf99508b68301631306b2.jpg" alt="" /></p>

<h1 id="-为下游任务设计的池化方法">⚪ 为下游任务设计的池化方法</h1>

<h2 id="1-全局平均池化-global-average-pooling-gap">1. 全局平均池化 Global Average Pooling (GAP)</h2>
<ul>
  <li>论文：<a href="https://arxiv.org/abs/1312.4400">Network In Network</a></li>
</ul>

<p>早期的卷积神经网络设计中，在卷积层和池化层后通常将特征图展开成一维向量，并设置全连接层用于降维和后续的分类等任务。由于全连接层具有较多参数量，会降低网络的训练速度并引入过拟合的风险；可以使用<strong>全局平均池化(Global Average Pooling,GAP)</strong>将整个特征图中每个通道的所有元素取平均进行输出：</p>

<p><img src="https://pic.imgdb.cn/item/63aacb6a08b6830163c16288.jpg" alt="" /></p>

<p><strong>GAP</strong>相当于为每一个分类类别指定一个通道的特征图，使用该特征图的平均值作为该类别的概率。它使得卷积神经网络对输入尺寸不再有限制(全连接层需要固定的输入尺寸)，并且没有引入额外的参数，因此没有增加过拟合的风险。<strong>GAP</strong>也可以看作一种正则化的方法，因为它显式地把最后一层特征图作为类别置信度图。</p>

<p>可以使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d">torch.nn.AdaptiveAvgPool2d</a>构造全局平均池化层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gap_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="2-协方差池化-covariance-pooling">2. 协方差池化 Covariance Pooling</h2>
<ul>
  <li>论文：<a href="https://ieeexplore.ieee.org/document/8759970">Skip-Connected Covariance Network for Remote Sensing Scene Classification</a></li>
</ul>

<p><strong>协方差池化(Covariance Pooling)</strong>是指计算特征图的协方差，从中提取高阶统计信息，以此构造特征向量：</p>

<p><img src="https://pic.imgdb.cn/item/63aae66208b6830163f02460.jpg" alt="" /></p>

<p>对于尺寸为$D \times H \times W$的特征图，首先将其变换为特征矩阵$X \in \Bbb{R}^{D \times HW}$，之后计算该矩阵的协方差矩阵：</p>

\[C = X \hat{I} X^T\]

<p>其中散点矩阵$\hat{I}=\frac{1}{HW}(1-\frac{1}{HW}11^T)$。对协方差矩阵$C$进行特征值分解$C=U \Sigma U^T$，使用矩阵对数将协方差矩阵投影到欧氏空间，获得池化后的特征：</p>

\[F = U\log(\Sigma)U^T\]

<p>由于$F$是对称矩阵，只取其右上角元素。将其展开后作为长度为长度为$\frac{D(D+1)}{2}$的输出特征向量$f$，并进一步执行后续任务。</p>

<h2 id="3-空间金字塔池化-spatial-pyramid-pooling-spp">3. 空间金字塔池化 Spatial Pyramid Pooling (SPP)</h2>
<ul>
  <li>paper：<a href="https://arxiv.org/abs/1406.4729">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></li>
</ul>

<p><strong>空间金字塔池化</strong>能够把任意不同尺寸和不同纵横比的图像特征转换为固定尺寸大小的输出特征向量。在实现时分别把特征划分成$k_i \times k_i$的栅格，然后应用最大池化操作构造长度为$\sum_i k_i^2c$的输出特征。</p>

<p><img src="https://pic.imgdb.cn/item/63abf68f08b6830163947507.jpg" alt="" /></p>

<h2 id="4-感兴趣区域池化-region-of-interest-pooling-roi-pooling">4. 感兴趣区域池化 Region of Interest Pooling (RoI Pooling)</h2>
<ul>
  <li>paper：<a href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a></li>
</ul>

<p><strong>感兴趣区域池化 (RoI Pooling)</strong>是指在目标检测过程中，根据提取的<strong>RoI</strong>坐标在特征图中选定区域，并把这部分区域转化为固定空间尺寸的输出特征图。构造过程为把特征区域划分成$k \times k$的栅格，然后对每个栅格应用最大池化操作。</p>

<p><img src="https://pic.imgdb.cn/item/63abfb8408b6830163a24ac3.jpg" alt="" /></p>

<h2 id="5-双线性池化-bilinear-pooling">5. 双线性池化 Bilinear Pooling</h2>
<ul>
  <li>paper：<a href="https://arxiv.org/abs/1504.07889">Bilinear CNN Models for Fine-grained Visual Recognition</a></li>
</ul>

<p>双线性池化主要用于特征融合。对于从同一个样本提取出来的特征$x$和特征$y$，通过双线性池化得到两个特征融合后的向量，并进一步用于下游任务。</p>

<p>对于图像\(\mathcal{I}\)在位置$l$的两个特征\(f_A(l, \mathcal{I}) \mathbb{R}^{1 \times M}\)和\(f_B(l, \mathcal{I}) \mathbb{R}^{1 \times N}\)， 对其进行双线性融合后得到矩阵$b$；对图像所有位置的矩阵$b$求和后展开为向量$x$，对$x$进行矩归一化和<strong>L2</strong>归一化后得到融合特征$z$。</p>

\[\begin{aligned}
b\left(l, \mathcal{I}, f_A, f_B\right) &amp; =f_A^T(l, \mathcal{I}) f_B(l, \mathcal{I}) &amp; &amp; \in \mathbb{R}^{M \times N} \\
\xi(\mathcal{I}) &amp; =\sum_l b\left(l, \mathcal{I}, f_A, f_B\right) &amp; &amp; \in \mathbb{R}^{M \times N} \\
x &amp; =\operatorname{vec}(\xi(\mathcal{I})) &amp; &amp; \in \mathbb{R}^{M N \times 1} \\
y &amp; =\operatorname{sign}(x) \sqrt{|x|} &amp; &amp; \in \mathbb{R}^{M N \times 1} \\
z &amp; =y /\|y\|_2 &amp; &amp; \in \mathbb{R}^{M N \times 1}
\end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/649cf15e1ddac507ccc512c4.jpg" alt="" /></p>

<p>如果特征$x$和特征$y$来自两个特征提取器，则被称为多模双线性池化（<strong>MBP，Multimodal Bilinear Pooling</strong>）；如果特征$x=y$，则被称为同源双线性池化（<strong>HBP，Homogeneous Bilinear Pooling</strong>）或者<a href="http://videolectures.net/site/normal_dl/tag=725540/eccv2012_carreira_semantic_01.pdf">二阶池化（<strong>Second-order Pooling</strong>）</a>。</p>

<p>原始的双线性池化存在融合后的特征维数过高的问题，融合后的特征维数等于特征$x$和特征$y$的维数之积。一些降低融合特征维数的方法可参考：<a href="https://arxiv.org/abs/1511.06062">Compact Bilinear Pooling (CBP)</a>, <a href="https://arxiv.org/abs/1606.01847">Multimodal Compact Bilinear Pooling (MCBP)</a>, <a href="https://arxiv.org/abs/1611.05109">Low-rank Bilinear Pooling (LBP)</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-030-01219-9_22">Grassmann Pooling</a>。</p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2021/07/07/feedback.html" class="read-next-link"></a>
        <section>
          <span>Addressing Some Limitations of Transformers with Feedback Memory</span>
          <p>  Feedback Transformer：改进Transformer的序列信息提取能力.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/60e52ec95132923bf80b16ff.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2021/07/01/mama.html" class="read-next-link"></a>
          <section>
            <span>Language Models are Open Knowledge Graphs</span>
            <p>  使用预训练的语言模型生成知识图谱.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/60dd22da5132923bf889139c.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
