<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>图像长尾分布(Long-Tail Distribution)问题 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="图像长尾分布(Long-Tail Distribution)问题">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="图像长尾分布(Long-Tail Distribution)问题 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2021/05/20/long-tail.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>图像长尾分布(Long-Tail Distribution)问题</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="21-05-20"><i class="iconfont icon-date"></i>20 May 2021</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/625fe9c2239250f7c51abe59.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Long-tail distribution problem in image datasets.</p>
</blockquote>

<p>在<strong>ImageNet</strong>、<strong>COCO</strong>等常用视觉数据集中，由于经过人工预筛选，图像中的不同目标类别的数量是接近的。而在实际的视觉应用中，数据集大多服从<strong>长尾分布(long-tail distribution)</strong>，即少数类别(称为<strong>head class</strong>)占据绝大多数样本，多数类别(称为<strong>tail class</strong>)仅有少量样本。一个典型的长尾分布数据集(<strong>Open Brands</strong>商标数据集)如下图所示。</p>

<p><img src="https://pic.imgdb.cn/item/60a4730d6ae4f77d353fd1fe.jpg" alt="" /></p>

<p>定义数据集的<strong>不平衡率(imbalance ratio)</strong>为类别的最大数量和最小数量之比。目前常用的一些长尾分布数据集如下：</p>
<ul>
  <li><strong>CIFAR100-LT</strong>：对<strong>CIFAR100</strong>的每类训练样本下采样得到的，不平衡率可以取$10,50,100$；测试集保持不变。</li>
  <li><strong>ImageNet-LT</strong>：从<strong>ImageNet</strong>中按照<strong>Pareto</strong>分布采样得到，包含$1000$类别的$19$<strong>k</strong>张图像，最多的类别具有$1280$张图像，而最少的仅有$5$张图像，不平衡率为$256$。</li>
  <li><strong>iNaturalist 2018</strong>：自然物种分类数据集，包含$8142$类别的$437513$张图像，包含长尾分布和细粒度检测问题，不平衡率为$500$。</li>
  <li><strong>LVIS</strong>：大规模实例分割数据集，对超过$1000$类物体进行了约$200$万个高质量的实例分割标注，包含$164$<strong>k</strong>张图像。</li>
</ul>

<p>本文介绍一些解决图像数据集中长尾分布问题的方法：</p>
<ol>
  <li>重采样 <strong>Re-sampling</strong>：通过对<strong>head class</strong>进行欠采样或对<strong>tail class</strong>进行过采样，人为地构造类别均衡的数据集。包括<strong>Random under/over-sampling</strong>, <strong>Class-balanced sampling</strong>, <strong>Meta Sampler</strong>等。</li>
  <li>重加权 <strong>Re-weighting</strong>：在损失函数中对不同类别样本的损失设置不同的权重，通常是对<strong>tail class</strong>对应的损失设置更大的权重。其中在$\log$运算之外调整损失函数的本质是在调节样本权重或者类别权重(如<strong>Inverse Class Frequency Weighting</strong>, <strong>Cost-Sensitive Cross-Entropy Loss</strong>, <strong>Focal Loss</strong>, <strong>Class-Balanced Loss</strong>)。在$\log$运算之内调整损失函数的本质是调整<strong>logits</strong>得分$z$，从而缓解对<strong>tail</strong>类别的负梯度(如<strong>Equalization Loss</strong>, <strong>Equalization Loss v2</strong>, <strong>Logit Adjustment Loss</strong>, <strong>Balanced Softmax Loss</strong>, <strong>Seesaw Loss</strong>)。</li>
  <li>其他方法：一些方法将长尾分布问题解耦为特征的表示学习和特征的分类。一些方法按照不同类别的样本数量级对类别进行分组(如<strong>BAGS</strong>)。</li>
</ol>

<h1 id="1-重采样-re-sampling">1. 重采样 Re-sampling</h1>
<p><strong>重采样(re-sampling)</strong>的思想是通过对<strong>head class</strong>进行欠采样或对<strong>tail class</strong>进行过采样，人为地让模型学习时接触到的训练样本是类别均衡的，从而一定程度上减少对<strong>head class</strong>的过拟合。不过由于<strong>tail class</strong>的少量数据往往被反复学习，缺少足够多的样本从而容易过拟合；而<strong>head class</strong>又往往得不到充分学习。</p>

<p><img src="https://pic.imgdb.cn/item/60a4da286ae4f77d35753c60.jpg" alt="" /></p>

<p>常用的重采样方法包括：</p>
<ul>
  <li><strong>Random over-sampling</strong>：对<strong>tail class</strong>进行过采样，这种方法容易过拟合。</li>
  <li><strong>Random under-sampling</strong>：对<strong>head class</strong>进行欠采样，这种方法会降低模型在<strong>head class</strong>上的性能。</li>
  <li><a href="https://0809zheng.github.io/2021/01/17/decouple.html"><font color="Blue">Class-balanced sampling</font></a>：控制重采样时每个类别$j$被采样的概率相同：</li>
</ul>

\[p_j^{CB} = \frac{1}{C}\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/17/decouple.html"><font color="Blue">Progressively-balanced sampling</font></a>：训练前期偏向类别不平衡采样，训练后期偏向类别平衡采样；缺点是每轮训练都要重新采样构成数据集：</li>
</ul>

\[p_j^{PB}(t) = (1-\frac{t}{T})\frac{N_j}{N} +\frac{t}{T}\frac{1}{C}\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/07/19/bbn.html"><font color="Blue">PReversed sampling</font></a>：控制重采样时每个类别$j$被采样的概率与该类别样本数量成反比例：</li>
</ul>

\[p_j^{Re} = \frac{w_j}{\sum_{j=1}^{C}w_j}, \quad w_j=\frac{n_{max}}{n_j}\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/21/metasoftmax.html"><font color="Blue">Meta Sampler</font></a>：通过元学习学习不同类别的最佳采样率：</li>
</ul>

\[p_j \gets p_j - \nabla_{p_j}L_{D_{meta}}(\theta)\]

<p>在<strong>pytorch</strong>中，可以为<code class="language-plaintext highlighter-rouge">DataLoader</code>传入采样器<code class="language-plaintext highlighter-rouge">sample</code>，从而实现不同类别的重采样（本质是调整样本权重）。下面给出一个实现<strong>Class-balanced</strong>采样的例子：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class_label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">targets</span>
<span class="n">class_count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">class_label</span><span class="o">==</span><span class="n">c</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">class_label</span><span class="p">)])</span>
<span class="n">weight</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">class_count</span>
<span class="n">samples_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">weight</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">class_label</span><span class="p">])</span>
<span class="n">samples_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">samples_weight</span><span class="p">)</span>
<span class="n">samples_weight</span> <span class="o">=</span> <span class="n">samples_weight</span><span class="p">.</span><span class="nf">double</span><span class="p">()</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">WeightedRandomSampler</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">samples_weight</span><span class="p">,</span>
                                                 <span class="n">num_samples</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">samples_weight</span><span class="p">),</span>
                                                 <span class="n">replacement</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># 有放回采样
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="2-重加权-re-weighting">2. 重加权 Re-weighting</h1>
<p><strong>重加权(re-weighting)</strong>的思想是在损失函数中对不同类别样本的损失设置不同的权重，通常是对<strong>tail class</strong>对应的损失设置更大的权重。但是这类方法需要针对不同的数据集和模型等条件设置不同的超参数，泛化性较差。</p>

<p><img src="https://pic.imgdb.cn/item/60a4da0d6ae4f77d3574999b.jpg" alt="" /></p>

<p>假设共有$C$个类别，类别$c$共有$n_c$个样本，总样本数为$n$；模型的输出<strong>logits</strong>(<strong>softmax</strong>前的输出)为$z=[z_1,z_2,…,z_C]^T$，则属于类别$c$的样本$x$的<strong>交叉熵</strong>损失函数计算为：</p>

\[\mathcal{L}(x,c) = -\log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)})\]

<p>根据对损失函数进行调整的位置是在$\log$运算之外还是在$\log$运算之内，重加权方法又可以粗略地分为两种情况。</p>

<h2 id="1-log外调整class-level-re-weighting">(1) log外调整：class-level re-weighting</h2>

<p>在$\log$运算之外调整损失函数的本质是在调节样本权重或者类别权重。</p>

<ul>
  <li><strong>Inverse Class Frequency Weighting</strong>：根据类别出现频率的倒数进行加权：</li>
</ul>

\[\mathcal{L}_{\text{ICFW}}(x,c) = -\frac{n}{n_c}\log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)})\]

<ul>
  <li><strong>Cost-Sensitive Cross-Entropy Loss</strong>：根据类别出现(与最少类别的)倍数的倒数进行加权：</li>
</ul>

\[\mathcal{L}_{\text{CS}}(x,c) = -\frac{n_{min}}{n_c}\log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)})\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/03/21/retinanet.html"><font color="blue">Focal Loss</font></a>：根据类别预测概率的指数进行加权，为容易预测的类别分配更低的权重：</li>
</ul>

\[\mathcal{L}_{\text{FO}}(x,c) = -(1-\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)})^{\gamma}\log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)})\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/18/classbalanced.html"><font color="Blue">Class-Balanced Loss</font></a>：使用不同类别的有效样本数量$E_{n_c}$进行加权：</li>
</ul>

\[\mathcal{L}_{\text{CB}}(x,c) = -\frac{1}{E_{n_c}} \log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)}) = -\frac{1-\beta}{1-\beta^{n_c}} \log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)})\]

<h2 id="2-log内调整class-level-re-margining">(2) log内调整：class-level re-margining</h2>

<p>注意到交叉熵损失也可以表示为：</p>

\[-\log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \exp(z_i)}) = -\log(\frac{1}{\sum_{i=1}^{C} \exp(z_i-z_c)})  = \log(\sum_{i=1}^{C} \exp(z_i-z_c))\]

<p>注意到<a href="https://0809zheng.github.io/2021/11/16/mollifier.html#-maxx_1x_2x_ntextlogsumexpx_1x_2x_n"><strong>logsumexp</strong></a>函数是最大值函数的光滑近似，则交叉熵损失实际上相当于：</p>

\[\mathcal{L}(x,c) = \max(z_1-z_c,z_2-z_c,...,z_c-z_c,...,z_C-z_c)\]

<p>最小化上述损失函数，即使得所有非目标类的<strong>logits</strong>得分$z_{i\ne c}$均小于目标类的<strong>logits</strong>得分$z_c$。上述优化过程会对所有非目标类产生负样本梯度，抑制这些类别的预测过程，这个现象称为<strong>负梯度过抑制(negative gradient over-suppression)</strong>。<strong>tail</strong>类样本作为非目标类样本的频率更高，因此抑制程度更强。</p>

<p>在$\log$运算之内调整损失函数的本质是调整<strong>logits</strong>得分$z$，从而缓解对<strong>tail</strong>类别的负梯度。一种可行的方法是为每个类别引入<strong>margin</strong> $m_c&gt;0$，则<strong>logits</strong>得分调整为$z_c-m_c$，样本数量越少的类别具有越大的<strong>margin</strong>。此时损失函数的形式为：</p>

\[-\log(\frac{\exp(z_c-m_c)}{\sum_{i=1}^{C} \exp(z_i-m_i)}) = -\log(\frac{\exp(-m_c)\exp(z_c)}{\sum_{i=1}^{C} \exp(-m_i)\exp(z_i)})\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/19/equalization.html"><font color="Blue">Equalization Loss</font></a>：减轻对其余属于<strong>tail</strong>类别的梯度抑制：</li>
</ul>

\[\mathcal{L}_{\text{EQ}}(x,c) = -\log(\frac{\exp(z_c)}{\sum_{i=1}^{C} \tilde{w}_i \exp(z_i)}),  \tilde{w}_i=1-\beta \Bbb{I}(f_i&lt;\lambda)(1-y_i)\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/07/14/eqlossv2.html"><font color="Blue">Equalization Loss v2</font></a>：根据梯度引导重加权机制动态调整每轮训练中正梯度和负梯度的权重：</li>
</ul>

\[\nabla_{z_j}^{pos'}(\mathcal{L}^{(t)}) = 1+\alpha(1-\frac{1}{1+e^{-\gamma(g_j^{(t)}-\mu)}}) \nabla_{z_j}^{pos}(\mathcal{L}^{(t)})\]

\[\nabla_{z_j}^{neg'}(\mathcal{L}^{(t)}) = \frac{1}{1+e^{-\gamma(g_j^{(t)}-\mu)}} \nabla_{z_j}^{neg}(\mathcal{L}^{(t)})\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/05/logitadjust.html"><font color="Blue">Logit Adjustment Loss</font></a>：将类别出现频率$p_c=\frac{n_c}{n}$引入<strong>logits</strong>:</li>
</ul>

\[\mathcal{L}_{\text{LA}}(x,c) =  -\log (\frac{\exp(z_c+ \log p_c)}{\sum_{i=1}^{C}\exp(z_i+ \log p_i)})\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/21/metasoftmax.html"><font color="Blue">Balanced Softmax Loss</font></a>：消除不平衡训练集与平衡测试集之间的标签分布偏移：</li>
</ul>

\[\mathcal{L}_{\text{BS}}(x,c) = -\log(\frac{z_c\exp(z_c)}{\sum_{i=1}^{C} z_i\exp(z_i)})\]

<ul>
  <li><a href="https://0809zheng.github.io/2021/01/20/seesaw.html"><font color="Blue">Seesaw Loss</font></a>：通过平衡系数\(\mathcal{S}_{ij}\)(由缓解因子\(\mathcal{M}_{ij}\)和补偿因子\(\mathcal{C}_{ij}\)控制)控制施加在其余类别上的负样本梯度：</li>
</ul>

\[\mathcal{L}_{\text{SS}}(x,c) = -\log(\frac{\exp(z_c)}{\sum_{j≠c}^{C} \mathcal{S}_{ij} \exp(z_j) + \exp(z_c)}), \mathcal{S}_{ij} = \mathcal{M}_{ij} \cdot \mathcal{C}_{ij}\]

<h1 id="3-其他方法-others">3. 其他方法 Others</h1>

<h3 id="1-解耦特征表示与分类">(1) 解耦特征表示与分类</h3>

<ul>
  <li>
    <p><a href="https://0809zheng.github.io/2021/01/17/decouple.html"><font color="Blue">Decoupling Representation and Classifier for Long-Tailed Recognition</font></a>：将长尾分布的图像分类问题解耦为特征的表示学习和特征的分类。采用两阶段的训练方法，首先在原始数据集上进行特征学习，然后在构造的类别平衡数据集上进行微调。</p>
  </li>
  <li>
    <p><a href="https://0809zheng.github.io/2021/07/19/bbn.html"><font color="Blue">BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</font></a>：采用双分支的网络结构同时进行特征学习和分类器学习，通过累积学习在训练过程中调整两个分支的权重。
<img src="https://pic.imgdb.cn/item/60f50e0e5132923bf86d95e7.jpg" alt="" /></p>
  </li>
</ul>

<h3 id="2-类别分组分类">(2) 类别分组分类</h3>

<ul>
  <li><a href="https://0809zheng.github.io/2021/06/14/groupsoftmax.html"><font color="Blue">Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax</font></a>：提出了<strong>BAGS</strong>方法，即按照不同类别的样本数量级对类别进行分组，每一组额外增加<strong>others</strong>类别，训练时分组分类训练，测试时分组测试并合并结果。
<img src="https://pic.imgdb.cn/item/60f52e7a5132923bf8751d07.jpg" alt="" /></li>
</ul>

<h1 id="-参考文献">⚪ 参考文献</h1>
<ul>
  <li><a href="http://www.lamda.nju.edu.cn/zhangys/papers/AAAI_tricks.pdf">Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks</a>：(AAAI2021)一篇长尾分布的综述。</li>
  <li><a href="https://arxiv.org/abs/2110.04596">Deep Long-Tailed Learning: A Survey</a>：(arXiv2110)一篇长尾分布的综述。</li>
  <li><a href="https://0809zheng.github.io/2021/01/18/classbalanced.html"><font color="Blue">Class-Balanced Loss Based on Effective Number of Samples</font></a>：(arXiv1901)Class-balanced Loss：基于有效样本数的类别平衡损失。</li>
  <li><a href="https://0809zheng.github.io/2021/01/17/decouple.html"><font color="Blue">Decoupling Representation and Classifier for Long-Tailed Recognition</font></a>：(arXiv1910)将长尾分布的图像分类问题解耦为表示学习和分类。</li>
  <li><a href="https://0809zheng.github.io/2021/07/19/bbn.html"><font color="Blue">BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</font></a>：(arXiv1912)BBN：通过累积学习进行长尾分类的双分支网络。</li>
  <li><a href="https://0809zheng.github.io/2021/01/19/equalization.html"><font color="Blue">Equalization Loss for Long-Tailed Object Recognition</font></a>：(arXiv2003)Equalization Loss：长尾目标检测中的均衡损失。</li>
  <li><a href="https://0809zheng.github.io/2021/06/14/groupsoftmax.html"><font color="Blue">Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax</font></a>：(arXiv2006)BAGS：按照类别样本的量级对长尾数据集进行分组分类。</li>
  <li><a href="https://0809zheng.github.io/2021/01/05/logitadjust.html"><font color="Blue">Long-tail learning via logit adjustment</font></a>：(arXiv2007)Logit Adjustment Loss: 将类别出现频率引入logits。</li>
  <li><a href="https://0809zheng.github.io/2021/01/21/metasoftmax.html"><font color="Blue">Balanced Meta-Softmax for Long-Tailed Visual Recognition</font></a>：(arXiv2007)BALMS: 长尾图像分类中的平衡元Softmax函数。</li>
  <li><a href="https://0809zheng.github.io/2021/01/20/seesaw.html"><font color="Blue">Seesaw Loss for Long-Tailed Instance Segmentation</font></a>：(arXiv2008)Seesaw Loss：长尾实例分割中的平衡损失。</li>
  <li><a href="https://0809zheng.github.io/2021/07/14/eqlossv2.html"><font color="Blue">Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection</font></a>：(arXiv2012)Equalization Loss v2：通过梯度引导重加权机制解决目标检测的正负梯度不平衡问题。</li>
</ul>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2021/05/21/gfl.html" class="read-next-link"></a>
        <section>
          <span>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</span>
          <p>  GFocal Loss: 为密集目标检测学习合格且分散的边界框.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/65289e0cc458853aef256495.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2021/05/19/kernalmachine.html" class="read-next-link"></a>
          <section>
            <span>Every Model Learned by Gradient Descent Is Approximately a Kernel Machine</span>
            <p>  使用梯度下降优化的深度学习模型近似于核方法.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/60a383cf6ae4f77d35980995.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
