<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>多目标优化的帕累托最优(Pareto Optimality) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="多目标优化的帕累托最优(Pareto Optimality)">
    <meta name="keywords"  content="数学">
    <!-- Open Graph -->
    <meta property="og:title" content="多目标优化的帕累托最优(Pareto Optimality) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2021/09/25/pareto.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%95%B0%E5%AD%A6" class="post-tag">数学</a>
          
        
      </div>
      <h1>多目标优化的帕累托最优(Pareto Optimality)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="21-09-25"><i class="iconfont icon-date"></i>25 Sep 2021</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/62dcab3df54cd3f93721c092.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>寻找多目标优化问题的帕累托最优解.</p>
</blockquote>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/1810.04650">Multi-Task Learning as Multi-Objective Optimization</a></li>
</ul>

<p>多目标优化是指同时优化多个相关任务的目标，<a href="https://0809zheng.github.io/2021/08/28/MTL.html">多任务学习</a>是一个典型的多目标优化问题，其总目标函数是每个任务的目标函数的加权求和式：\(\mathcal{L}_{total} = \sum_{i}^{n} w_i\mathcal{L}_i\)。为使得每个任务在训练时都获得有益的提升，需要合理的设置任务权重$w_i$，使得每一次更新时目标损失函数\(\mathcal{L}_1,\mathcal{L}_2,\cdots, \mathcal{L}_n\)都下降或保持不变。</p>

<p>对于参数$\theta^{*}$，若该参数任意变化都会导致某个目标的损失函数\(\mathcal{L}_i\)上升，则称$\theta^{*}$为<strong>帕累托最优解(Pareto Optimality)</strong>。帕累托最优意味着不能通过牺牲某个任务来换取其他任务的性能提升。</p>

<h1 id="1-建模多目标优化问题">1. 建模多目标优化问题</h1>

<p>本文主要讨论基于<strong>梯度下降法</strong>的多目标优化问题。记参数$\theta$在任务$i$上的梯度为\(g_i = \nabla_{\theta} \mathcal{L}_i\)，则寻找帕累托最优的过程等价于构造参数变化量$\Delta \theta$满足：</p>

\[\quad \langle g_i, \Delta \theta \rangle \leq 0, \quad \forall i\]

<p>注意到上述方程组存在平凡解$\Delta \theta=0$，我们主要关心可行域中是否存在非零解。若能求得非零解，则将其作为参数更新方向；否则有可能已经达到了帕累托最优（必要不充分条件），此时的状态称为<strong>帕累托稳定点 (Pareto Stationary)</strong>。</p>

<p>上述问题等价于寻找一个向量$u$，使得对所有任务$i$的梯度$g_i$都满足$\langle g_i,u \rangle \geq 0$，此时可取$\Delta \theta = -\eta u$作为参数更新量。</p>

<p>将该问题转化为最优化问题：</p>

\[\forall i, \langle g_i,u \rangle \geq 0 \quad \Leftrightarrow \quad \mathop{\min}_{i} \langle g_i,u \rangle \geq 0\]

<p>问题可进一步转化为最大化内积$\langle g_i,u \rangle$的最小值：</p>

\[\mathop{\max}_{u} \mathop{\min}_{i} \langle g_i,u \rangle\]

<p>注意到上述优化目标可能会趋于无穷大，因此为了结果的稳定性，为$u$的模长增加正则项：</p>

\[\mathop{\max}_{u} \mathop{\min}_{i} \langle g_i,u \rangle - \frac{1}{2}||u||^2\]

<p>注意到$u=0$时上述目标取值也为$0$，因此最优解$u^{*}$满足：</p>

\[\mathop{\min}_{i} \langle g_i,u^* \rangle - \frac{1}{2}||u^*||^2 \geq 0 \quad \Leftrightarrow \quad \mathop{\min}_{i} \langle g_i,u^* \rangle \geq \frac{1}{2}||u^*||^2 \geq 0\]

<p>即加入正则项之后的最优解$u^{*}$也必然是满足原最优化问题$\mathop{\min}_{i} \langle g_i,u \rangle \geq 0$的解。</p>

<h1 id="2-求解多目标优化问题">2. 求解多目标优化问题</h1>

<p>定义\(\Bbb{P}^n\)为所有$n$元离散分布的集合：</p>

\[\Bbb{P}^n = \{ (w_1,w_2,\cdots,w_n) | w_1,w_2,\cdots,w_n\geq 0,\sum_{i} w_i = 1 \}\]

<p>则原优化目标等价于：</p>

\[\mathop{\min}_{i} \langle g_i,u \rangle  =  \mathop{\min}_{w \in \Bbb{P}^n} \langle \sum_i w_ig_i,u \rangle  =  \mathop{\min}_{w \in \Bbb{P}^n} \langle \tilde{g}_i(w),u \rangle\]

<p>因此加入正则项之后的目标也等价于：</p>

\[\mathop{\max}_{u} \mathop{\min}_{w \in \Bbb{P}^n} \langle \tilde{g}_i(w),u  \rangle - \frac{1}{2}||u||^2\]

<p>该目标函数是关于$u$的凹函数，关于$w$的凸函数，且$u,w$的可行域都是凸集，根据<a href="https://0809zheng.github.io/2022/09/23/minimax.html#1-%E6%9E%81%E5%B0%8F%E6%9E%81%E5%A4%A7%E5%AE%9A%E7%90%86-minimax-theorem">Minimax定理</a>可以将上述问题转化为<strong>对偶</strong>问题：</p>

\[\mathop{\min}_{w \in \Bbb{P}^n} \mathop{\max}_{u} \langle \tilde{g}_i(w),u  \rangle - \frac{1}{2}||u||^2\]

<p>注意到$\max$部分是关于$u$的无约束的二次函数最大值问题，其最大值在$u=\tilde{g}_i(w)$处取，则目标函数进一步写作：</p>

\[\mathop{\min}_{w \in \Bbb{P}^n}  \frac{1}{2}||\tilde{g}_i(w)||^2 = \mathop{\min}_{w \in \Bbb{P}^n}  \frac{1}{2}||\sum_i w_ig_i||^2\]

<p>上式等价于求各任务损失梯度$g_1,g_2,\cdots g_n$的所有凸组合中模长最小的情况，约束条件为$\sum_i w_i =1$。</p>

<h3 id="-无约束的梯度下降">⚪ 无约束的梯度下降</h3>

\[\mathop{\min}_{w \in \Bbb{P}^n}  \frac{1}{2}||\sum_i w_ig_i||^2\]

<p>该目标可以通过去约束的方式直接用梯度下降求解。常用方法是设置可学习参数$\beta_1,\beta_2,\cdots \beta_n \in \Bbb{R}$，使得：</p>

\[w_i = \frac{e^{\beta_i}}{\sum_i e^{\beta_i}}\]

<p>此时隐式地包含约束$\sum_i w_i =1$；且目标转化为$\beta$的无约束优化问题：</p>

\[\mathop{\min}_{\beta}  \frac{1}{2(\sum_i e^{\beta_i})^2}||\sum_i e^{\beta_i}g_i||^2\]

<h3 id="-带约束的梯度下降">⚪ 带约束的梯度下降</h3>

\[\mathop{\min}_{w \in \Bbb{P}^n}  \frac{1}{2}||\tilde{g}_i(w)||^2\]

<p>原问题也可以通过<strong>Frank-Wolfe</strong>算法求解。<strong>Frank-Wolfe</strong>算法可以看作一种带约束的梯度下降算法，适合于参数的可行域为凸集的情形。该算法的求解在当前$w$与位置$\tau$为$1$的<strong>one-hot</strong>向量$e_{\tau}$之间进行插值搜索，找出最优者作为迭代结果。</p>

<p>当$n=2$时，问题等价于求两个向量$g_1$（图中为$\theta$）和$g_2$（图中为$\overline{\theta}$）的模长最小凸组合：</p>

<p><img src="https://pic.imgdb.cn/item/62dcf777f54cd3f937d60866.jpg" alt="" /></p>

<p>当$n&gt;2$时，<strong>Frank-Wolfe</strong>算法将问题转化为多个$n=2$的情形进行迭代，迭代过程为：</p>

\[\begin{aligned} \tau &amp;= \mathop{\arg \min}_i \langle g_i,\tilde{g}(w^{(k)}) \rangle \\ \gamma &amp;=  \mathop{\arg \min}_{\gamma} \tilde{g}((1-\gamma)w^{(k)}+\gamma e_{\tau}) \\ &amp;=  \mathop{\arg \min}_{\gamma} ||(1-\gamma)\tilde{g}(w^{(k)})+\gamma g_{\tau}||^2 \\ w^{(k+1)} &amp;= (1-\gamma)w^{(k)}+\gamma e_{\tau} \end{aligned}\]

<p>其中$\gamma$的求解即为$n=2$的特例。</p>

<h1 id="3-优化求解过程">3. 优化求解过程</h1>

<p>前两节讨论了寻找帕累托稳定点的参数更新方向的方法，实现过程为在每一步的训练中，先通过另外的多步迭代来确定每个目标的权重，然后再更新模型参数。实际计算时计算量比较大，因此通过一些额外的技巧降低计算量。</p>

<h3 id="-梯度内积">⚪ 梯度内积</h3>

<p>在优化过程中需要多次计算梯度的内积\(\langle g_i,\tilde{g}(w^{(k)}) \rangle\)，由于梯度$g_i$的维度与模型参数量相同且通常比较大，因此内积运算的计算量很大。考虑展开式：</p>

\[\langle g_i,\tilde{g}(w^{(k)}) \rangle = \langle g_i,\sum_j w_jg_j \rangle =\sum_j w_j \langle g_i,g_j \rangle\]

<p>在实现时\(\langle g_i,g_j \rangle\)只需要计算一次并存储下来，可以减少大维度向量内积的重复计算。</p>

<h3 id="-共享编码">⚪ 共享编码</h3>

<p>若假设多个目标任务共享同一个特征提取网络，则还可以进一步近似地简化算法。具体地，假设批量为$b$的样本中第$j$个样本的特征编码输出为$h_j$，则第$i$个目标损失的梯度计算为：</p>

\[\begin{aligned} g_i &amp; = \nabla_{\theta} \mathcal{L}_i = \sum_j (\nabla_{h_j} \mathcal{L}_i ) (\nabla_{\theta} h_j) \\ &amp;=(\nabla_{h_1} \mathcal{L}_i,\cdots,\nabla_{h_b} \mathcal{L}_i) \begin{pmatrix} \nabla_{\theta} h_1 \\ \vdots \\ \nabla_{\theta} h_b \end{pmatrix} \end{aligned}\]

<p>若记$H=(h_1,\cdots,h_b)$，则梯度\(g_i=(\nabla_{H} \mathcal{L}_i ) (\nabla_{\theta} H)\)。优化目标为：</p>

\[||\sum_i w_ig_i||^2 = ||\sum_i w_i(\nabla_{H} \mathcal{L}_i ) (\nabla_{\theta} H)||^2 \leq ||\sum_i w_i\nabla_{H} \mathcal{L}_i ||^2 || \nabla_{\theta} H||^2\]

<p>因此如果不最小化$||\sum_i w_ig_i||^2$（需要求所有参数的梯度），而是最小化$||\sum_i w_i\nabla_{H} \mathcal{L}_i||^2$（只需要求编码向量的梯度），则计算量会明显减少。后者是前者的一个上界。</p>

<p>该上界也具有局限性，一般只适用于每一个样本都有多种标注信息的多目标优化问题，不适用于不同目标任务的训练数据无交集的场景。因为不同任务的数据无交集表示$\nabla_{H} \mathcal{L}_i$是相互正交的，该上界过于宽松，没有体现出任务之间的相关性。</p>

<h1 id="4-主次型多目标优化">4. 主次型多目标优化</h1>

<p>在前面几节的讨论中，多目标优化的目的是尽可能在所有目标任务上都取得较好的表现，即平等地处理每一个目标。在实际需求中，有时我们希望模型主要在一个或几个主目标上表现较好，并额外地增加一些辅助目标，通过学习辅助目标来提升在主目标上的表现。此时多目标优化也称为<strong>主次型</strong>多目标优化。</p>

<p>若记\(\mathcal{L}_0\)为主目标的损失函数，\(\mathcal{L}_1，\cdots \mathcal{L}_n\)为$n$个辅助目标的损失函数。则主目标仍然是寻找一个向量$u$，使得对主目标的梯度$g_0$满足$\langle g_0,u \rangle \geq 0$，并采用梯度下降法更新参数$\Delta \theta = -\eta u$。根据前面的讨论，优化目标为：</p>

\[\mathop{\max}_{u} \langle g_0,u \rangle - \frac{1}{2}||u||^2\]

<p>对于辅助目标\(\mathcal{L}_1，\cdots \mathcal{L}_n\)，我们希望更新方向$u$不要使得其中的任何一个目标损失增大即可(不一定最小化损失)，因此引入约束条件：</p>

\[\begin{aligned} \mathop{\max}_{u} &amp; \langle g_0,u \rangle - \frac{1}{2}||u||^2 \\ \text{s.t.} &amp; \langle g_1,u \rangle \geq 0,\cdots \langle g_n,u \rangle \geq 0 \end{aligned}\]

<p>建立拉格朗日方程，将上述问题转化成<strong>min-max</strong>问题：</p>

\[\mathop{\max}_{u} \mathop{\min}_{\lambda_i \geq 0} \langle g_0,u \rangle - \frac{1}{2}||u||^2 + \sum_i^n \lambda_i \langle g_i,u \rangle\]

<p>在该问题中，首先需要最小化$\lambda_i$的目标函数。假设$\langle g_i,u \rangle \geq 0$，则应使$\lambda_i \langle g_i,u \rangle = 0$；假设$\langle g_i,u \rangle &lt; 0$，则应使$\lambda_i \langle g_i,u \rangle \to - \infty$。而该目标还包含$u$的最大化，因此取前一种情况，此时<strong>min-max</strong>问题的优化结果与原带约束的<strong>max</strong>问题等价。</p>

<p>若定义\(\Bbb{Q}^n\)为所有$n$元离散分布的集合：</p>

\[\Bbb{Q}^n = \{ (\lambda_1,\lambda_2,\cdots,\lambda_n) | \lambda_1,\lambda_2,\cdots,\lambda_n\geq 0 \}\]

<p>则优化问题记为：</p>

\[\mathop{\max}_{u} \mathop{\min}_{\lambda \in \Bbb{Q}^n} \langle g_0+ \sum_i^n \lambda_i g_i,u \rangle - \frac{1}{2}||u||^2 \\ = \mathop{\max}_{u} \mathop{\min}_{\lambda \in \Bbb{Q}^n} \langle g_0+ \tilde{g}(\lambda),u \rangle - \frac{1}{2}||u||^2\]

<p>根据<a href="https://0809zheng.github.io/2022/09/23/minimax.html#1-%E6%9E%81%E5%B0%8F%E6%9E%81%E5%A4%A7%E5%AE%9A%E7%90%86-minimax-theorem">Minimax定理</a>，如果<strong>min</strong>问题和<strong>max</strong>问题的参数可行域都是凸集，并且目标函数关于<strong>min</strong>问题的参数是凸的、关于<strong>max</strong>问题的参数是凹的，则<strong>min</strong>和<strong>max</strong>的次序可以交换：</p>

\[\begin{aligned} &amp; \mathop{\max}_{u} \mathop{\min}_{\lambda \in \Bbb{Q}^n} \langle g_0+ \tilde{g}(\lambda),u \rangle - \frac{1}{2}||u||^2 \\ &amp;= \mathop{\min}_{\lambda \in \Bbb{Q}^n} \mathop{\max}_{u} \langle g_0+ \tilde{g}(\lambda),u \rangle - \frac{1}{2}||u||^2 \\ &amp;= \mathop{\min}_{\lambda \in \Bbb{Q}^n} \frac{1}{2}||g_0+ \tilde{g}(\lambda)||^2 \end{aligned}\]

<p>上式等价于求主目标损失梯度$g_0$与各辅助目标损失梯度$g_1,g_2,\cdots g_n$的所有加权组合之和中模长最小的情况。</p>

<p>当$n=1$时，问题等价于\(\mathop{\min}_{\lambda \geq 0} \frac{1}{2}\|g_0+ \lambda g_1\|^2\)，该问题具有明确的几何意义和简单的解析解：</p>

<p><img src="https://pic.imgdb.cn/item/62df5533f54cd3f937e08932.jpg" alt="" /></p>

<p>当$n&gt;1$时，采用<strong>Frank-Wolfe</strong>算法将问题转化为多个$n=1$的情形进行迭代，迭代过程为：</p>

\[\begin{aligned} \tau &amp;= \mathop{\arg \min}_i \langle g_i,g_0+\tilde{g}(\lambda^{(k)}) \rangle \\ \gamma &amp;=  \mathop{\arg \min}_{\gamma} ||\tilde{g}(\lambda^{(k)}-\lambda^{(k)}_{\tau}e_{\tau}+\gamma e_{\tau})+g_0||^2 \\ &amp;=  \mathop{\arg \min}_{\gamma} ||\tilde{g}(\lambda^{(k)})-\lambda^{(k)}_{\tau}g_{\tau}+\gamma g_{\tau})+g_0||^2  \\ \lambda^{(k+1)} &amp;= \lambda^{(k)}-\lambda^{(k)}_{\tau}e_{\tau}+\gamma e_{\tau} \end{aligned}\]

<p>对照上述讨论，具有$m$个主目标、$n$个辅助目标的混合型多目标优化，应具有的对偶目标函数为：</p>

\[\mathop{\min}_{w \in \Bbb{P}^n, \lambda \in \Bbb{Q}^n} \frac{1}{2}||\tilde{g}(w)+ \tilde{g}(\lambda)||^2 \\ = \mathop{\min}_{w \in \Bbb{P}^n, \lambda \in \Bbb{Q}^n} \frac{1}{2}||\sum_i^m w_ig_i+ \sum_j^n \lambda_j g_j ||^2\]

<h3 id="-主次型多目标优化的应用">⚪ 主次型多目标优化的应用</h3>

<p>一个典型的主次型多目标优化案例是向任务损失中加入正则项，如<strong>L2</strong>正则化：</p>

\[\mathcal{L}(\theta) + \frac{\lambda}{2} ||\theta||^2\]

<p>此时正则化损失并不需要越小越好，而是希望能够提高原损失\(\mathcal{L}(\theta)\)的泛化性能。此时两个目标损失的梯度分别为：</p>

\[g_0 = \nabla_{\theta}\mathcal{L}(\theta), g_1= \theta\]

<p>当$\langle g_0,g_1 \rangle&gt;0$时取$\lambda=0$；当$\langle g_0,g_1 \rangle&lt;0$时取与$g_1$正交的$g_0+\lambda g_1$：</p>

\[\langle g_1,g_0+\lambda g_1 \rangle =0 \Leftrightarrow \lambda = -\frac{\langle g_0,g_1 \rangle}{||g_1||^2}\]

<p>将上述两种情况统一写作：</p>

\[\lambda = \frac{\text{ReLU}(-\langle g_0,g_1 \rangle)}{||g_1||^2}\]

<p>代入$g_0,g_1$的表达式得：</p>

\[\lambda = \frac{\text{ReLU}(-\langle \nabla_{\theta}\mathcal{L}(\theta),\theta \rangle)}{||\theta||^2}\]

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2021/09/26/mrn.html" class="read-next-link"></a>
        <section>
          <span>Learning Multiple Tasks with Multilinear Relationship Networks</span>
          <p>  MRN：使用多线性关系网络进行多任务学习.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/62dd2f3ff54cd3f9371d68ed.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2021/09/24/umap.html" class="read-next-link"></a>
          <section>
            <span>一致流形近似与投影(Uniform Manifold Approximation and Projection, UMAP)</span>
            <p>  Uniform Manifold Approximation and Projection。</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/61e226c12ab3f51d91f55bed.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
