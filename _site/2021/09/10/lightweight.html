<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>轻量级(LightWeight)卷积神经网络 - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="轻量级(LightWeight)卷积神经网络">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="轻量级(LightWeight)卷积神经网络 - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2021/09/10/lightweight.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>轻量级(LightWeight)卷积神经网络</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="21-09-10"><i class="iconfont icon-date"></i>10 Sep 2021</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/6183331c2ab3f51d915b0c47.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Lightweight Convolutional Neural Networks.</p>
</blockquote>

<p>卷积神经网络被广泛应用在图像分类、目标检测等视觉任务中，并取得了巨大的成功。然而，卷积神经网络通常需要较大的运算量和内存占用，在嵌入式设备等资源受限的环境中受到限制，因此需要进行网络压缩。</p>

<p><strong>轻量级网络设计</strong>是网络压缩的一种方法，旨在设计计算复杂度更低的网络结构。
从<strong>结构</strong>的角度考虑，卷积层提取的特征存在冗余，可以设计特殊的卷积操作，减少卷积操作的冗余，从而减少计算量。
从<strong>计算</strong>的角度，模型推理过程中存在大量乘法运算，而乘法操作(相比于加法)对于目前的硬件设备不友好，可以对乘法运算进行优化，也可以减少计算量。</p>

<p>本文目录：</p>
<ol>
  <li>设计特殊的卷积</li>
  <li>寻找乘法的替代</li>
</ol>

<h1 id="1-设计特殊的卷积">1. 设计特殊的卷积</h1>

<p>一个标准的$3\times 3$卷积层表示如下：</p>

<p><img src="https://pic.imgdb.cn/item/6183340f2ab3f51d915bf334.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">(convolution =&gt; [BN] =&gt; [ReLU])</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> 
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bn</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vanilla_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> 
                      <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">bn</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vanilla_conv</span><span class="p">.</span><span class="nf">add_module</span><span class="p">(</span><span class="sh">'</span><span class="s">batchnorm</span><span class="sh">'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">relu</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vanilla_conv</span><span class="p">.</span><span class="nf">add_module</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">vanilla_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>下面介绍一些特殊设计的卷积神经网络：</p>

<table>
  <thead>
    <tr>
      <th>轻量级网络</th>
      <th style="text-align: center">卷积层</th>
      <th style="text-align: center">特殊结构</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/16/squeezenet.html"><font color="Blue">SqueezeNet</font></a></td>
      <td style="text-align: center">标准卷积</td>
      <td style="text-align: center">Fire模块</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/17/squeezenext.html"><font color="Blue">SqueezeNext</font></a></td>
      <td style="text-align: center">标准卷积</td>
      <td style="text-align: center">分离卷积($3\times 1+1\times 3$)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/13/mobilenetv1.html"><font color="Blue">MobileNet</font></a></td>
      <td style="text-align: center">深度可分离卷积</td>
      <td style="text-align: center">深度(depth-wise)卷积, 逐点(point-wise)卷积</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/14/mobilenetv2.html"><font color="Blue">MobileNetV2</font></a></td>
      <td style="text-align: center">深度可分离卷积</td>
      <td style="text-align: center">线性瓶颈(linear bottleneck), 倒残差(inverted residual)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/15/mobilenetv3.html"><font color="Blue">MobileNetV3</font></a></td>
      <td style="text-align: center">深度可分离卷积</td>
      <td style="text-align: center">通道注意力机制(SENet), 神经结构搜索(NAS)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/18/shufflenet.html"><font color="Blue">ShuffleNet</font></a></td>
      <td style="text-align: center">组卷积+深度卷积</td>
      <td style="text-align: center">通道打乱(channel shuffle)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/19/shufflenetv2.html"><font color="Blue">ShuffleNet V2</font></a></td>
      <td style="text-align: center">标准卷积+深度卷积</td>
      <td style="text-align: center">通道拆分(channel split), 通道打乱(channel shuffle)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/21/igc.html"><font color="Blue">IGCNet</font></a></td>
      <td style="text-align: center">组卷积</td>
      <td style="text-align: center">交错组卷积(overleaved group conv)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/22/igcv2.html"><font color="Blue">IGCV2</font></a></td>
      <td style="text-align: center">组卷积</td>
      <td style="text-align: center">交错结构化稀疏卷积(overleaved structured sparse conv)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/20/channelnet.html"><font color="Blue">ChannelNet</font></a></td>
      <td style="text-align: center">深度卷积+组卷积+通道卷积</td>
      <td style="text-align: center">组通道卷积, 深度可分离通道卷积, 卷积分类层</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/11/efficientv1.html"><font color="Blue">EfficientNet</font></a></td>
      <td style="text-align: center">MBConv(即MobileNetV3)</td>
      <td style="text-align: center">复合缩放(compound scaling)</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/09/12/efficientv2.html"><font color="Blue">EfficientNetV2</font></a></td>
      <td style="text-align: center">Fused-MBConv</td>
      <td style="text-align: center">渐进训练</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/08/ghostnet.html"><font color="Blue">GhostNet</font></a></td>
      <td style="text-align: center">Ghost模块</td>
      <td style="text-align: center">Ghost BottleNeck</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/11/09/micronet.html"><font color="Blue">MicroNet</font></a></td>
      <td style="text-align: center">微因子卷积</td>
      <td style="text-align: center">微因子(micro-factorized)深度卷积和逐点卷积</td>
    </tr>
    <tr>
      <td><a href="https://0809zheng.github.io/2021/08/03/compconv.html"><font color="Blue">CompConv</font></a></td>
      <td style="text-align: center">分治卷积</td>
      <td style="text-align: center">-</td>
    </tr>
  </tbody>
</table>

<h3 id="--squeezenet使用fire模块代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/09/16/squeezenet.html"><font color="Blue">SqueezeNet</font></a>：使用Fire模块代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/618334e22ab3f51d915cc600.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Fire</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (1x1 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; 1x1+3x3 convolution =&gt; [BN] =&gt; ReLU)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">e1x1</span> <span class="o">=</span> <span class="n">out_channels</span><span class="o">//</span><span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">e3x3</span> <span class="o">=</span> <span class="n">out_channels</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">e1x1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">s1x1</span> <span class="o">=</span> <span class="n">out_channels</span><span class="o">//</span><span class="mi">4</span>
        <span class="n">self</span><span class="p">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">s1x1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand1x1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s1x1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">e1x1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expand3x3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s1x1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">e3x3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tail</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">e1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand1x1</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">e2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">expand3x3</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">e1</span><span class="p">,</span><span class="n">e2</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">tail</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="-squeezenext使用分离卷积构造标准卷积块">⚪ <a href="https://0809zheng.github.io/2021/09/17/squeezenext.html"><font color="Blue">SqueezeNext</font></a>：使用分离卷积构造标准卷积块</h3>

<p><img src="https://pic.imgdb.cn/item/6183357f2ab3f51d915d4997.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SqNxt</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (1x1 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; 3x1 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; 1x3 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; ReLU)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">in_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sqnxt</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">sqnxt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="-mobilenet使用深度可分离卷积depthwise-separable-conv代替普通卷积">⚪ <a href="https://0809zheng.github.io/2021/09/13/mobilenetv1.html"><font color="Blue">MobileNet</font></a>：使用深度可分离卷积(Depthwise Separable Conv)代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/6183364c2ab3f51d915e09e9.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DSConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (depthwise convolution =&gt; [BN] =&gt; ReLU6
        =&gt; 1x1 convolution =&gt; [BN] =&gt; ReLU6)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">depthwise_separable_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">in_channels</span><span class="p">),</span> <span class="c1"># 此处激活函数为 nn.ReLU6(inplace=True)
</span>            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 此处激活函数为 nn.ReLU6(inplace=True)
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">depthwise_separable_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="-mobilenetv2为mobilenet引入线性瓶颈linear-bottleneck并设计倒残差inverted-residual结构">⚪ <a href="https://0809zheng.github.io/2021/09/14/mobilenetv2.html"><font color="Blue">MobileNetV2</font></a>：为MobileNet引入线性瓶颈(linear bottleneck)，并设计倒残差(inverted residual)结构</h3>

<p><img src="https://pic.imgdb.cn/item/618337072ab3f51d915eadba.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DSConvv2</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (1x1 convolution =&gt; [BN] =&gt; ReLU6
    =&gt; depthwise convolution =&gt; [BN] =&gt; ReLU6 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; Linear)
        </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inverted_residual</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 此处激活函数为 nn.ReLU6(inplace=True)
</span>            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">),</span> <span class="c1"># 此处激活函数为 nn.ReLU6(inplace=True)
</span>            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">inverted_residual</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="--mobilenetv3引入通道注意力channel-attention通过神经结构搜索网络">⚪  <a href="https://0809zheng.github.io/2021/09/15/mobilenetv3.html"><font color="Blue">MobileNetV3</font></a>：引入通道注意力(Channel Attention)，通过神经结构搜索网络</h3>

<p><img src="https://pic.imgdb.cn/item/618337ee2ab3f51d915f7e3f.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SELayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SELayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="n">channel</span><span class="o">//</span><span class="n">reduction</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channel</span><span class="o">//</span><span class="n">reduction</span><span class="p">,</span><span class="n">channel</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">.</span><span class="nf">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DSConvv3</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (1x1 convolution =&gt; [BN] =&gt; Hardswish 
    =&gt; depthwise convolution =&gt; [BN] =&gt; Hardswish 
    =&gt; SELayer 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; Linear)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># 此处激活函数为 nn.Hardswish(inplace=True)
</span>            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">),</span> <span class="c1"># 此处激活函数为 nn.Hardswish(inplace=True)
</span>            <span class="nc">SELayer</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="-shufflenet使用组卷积group-conv和通道打乱channel-shuffle代替普通卷积">⚪ <a href="https://0809zheng.github.io/2021/09/18/shufflenet.html"><font color="Blue">ShuffleNet</font></a>：使用组卷积(Group Conv)和通道打乱(Channel Shuffle)代替普通卷积</h3>
<p><img src="https://pic.imgdb.cn/item/618338722ab3f51d915ff3bd.jpg" alt="" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ShuffleBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ShuffleBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">Channel shuffle: [N,C,H,W] -&gt; [N,g,C/g,H,W] -&gt; [N,C/g,g,H,w] -&gt; [N,C,H,W]</span><span class="sh">'''</span>
        <span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">groups</span>
        <span class="c1"># 维度变换之后必须要使用.contiguous()使得张量在内存连续之后才能调用view函数
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="nf">int</span><span class="p">(</span><span class="n">C</span><span class="o">/</span><span class="n">g</span><span class="p">),</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ShuffleNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (1x1 group convolution =&gt; [BN] =&gt; ReLU =&gt; ChannelShuffle
    =&gt; depthwise convolution =&gt; [BN] 
    =&gt; 1x1 group convolution =&gt; [BN] =&gt; Linear)</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">mid_channels</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.25</span><span class="o">*</span><span class="n">in_channels</span><span class="p">)</span>
        <span class="c1"># 如果输入通道太少则无法分组
</span>        <span class="n">g</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">in_channels</span><span class="o">&lt;</span><span class="n">groups</span><span class="o">**</span><span class="mi">2</span> <span class="k">else</span> <span class="n">groups</span>
        <span class="n">self</span><span class="p">.</span><span class="n">shuffle_block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">g</span><span class="p">),</span>
            <span class="nc">ShuffleBlock</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="n">g</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">shuffle_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="-shufflenet-v2为shufflenet引入通道拆分channel-split">⚪ <a href="https://0809zheng.github.io/2021/09/19/shufflenetv2.html"><font color="Blue">ShuffleNet V2</font></a>：为ShuffleNet引入通道拆分(Channel Split)</h3>
<p><img src="https://pic.imgdb.cn/item/618338d52ab3f51d916041a3.jpg" alt="" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ShuffleBlock定义见ShuffleNet  
</span><span class="k">class</span> <span class="nc">ShuffleNetv2</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (ChannelSplit 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; depthwise convolution =&gt; [BN] 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; ReLU 
    =&gt; ChannelShuffle)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># 需要处理输入输出特征通道数不相等的情况
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cin</span> <span class="o">=</span> <span class="n">in_channels</span><span class="o">//</span><span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cout</span> <span class="o">=</span> <span class="n">out_channels</span><span class="o">//</span><span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cout</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cin</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cout</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="nc">ShuffleBlock</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># chunk方法可以对张量分块，后面的块通道数可能少一些
</span>        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">block</span><span class="p">(</span><span class="n">x2</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="--igcnet使用交错组卷积代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/09/21/igc.html"><font color="Blue">IGCNet</font></a>：使用交错组卷积代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/617a5dac2ab3f51d91e01847.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ShuffleBlock定义见ShuffleNet    
</span><span class="k">class</span> <span class="nc">IGCNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">M</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">self</span><span class="p">.</span><span class="n">L</span> <span class="o">=</span> <span class="n">out_channels</span><span class="o">//</span><span class="n">self</span><span class="p">.</span><span class="n">M</span>
        <span class="n">self</span><span class="p">.</span><span class="n">igconv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">L</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
            <span class="nc">ShuffleBlock</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">L</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">M</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
            <span class="nc">ShuffleBlock</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">M</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">igconv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="--igcv2-使用交错结构化稀疏卷积代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/09/22/igcv2.html"><font color="Blue">IGCV2 </font></a>：使用交错结构化稀疏卷积代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/617bbd5b2ab3f51d910a8af6.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ShuffleBlock定义见ShuffleNet    
</span><span class="k">class</span> <span class="nc">IGCV2</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">L</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">))</span><span class="o">+</span><span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">igcv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">out_channels</span><span class="o">//</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">igcv2</span><span class="p">.</span><span class="nf">add_module</span><span class="p">(</span><span class="sh">'</span><span class="s">shuffle</span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span>
                                  <span class="nc">ShuffleBlock</span><span class="p">(</span><span class="n">groups</span><span class="o">=</span><span class="n">out_channels</span><span class="o">//</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">igcv2</span><span class="p">.</span><span class="nf">add_module</span><span class="p">(</span><span class="sh">'</span><span class="s">groupconv</span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span> 
                                  <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> 
                                            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">out_channels</span><span class="o">//</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">igcv2</span><span class="p">.</span><span class="nf">add_module</span><span class="p">(</span><span class="sh">'</span><span class="s">batchnorm</span><span class="sh">'</span><span class="o">+</span><span class="nf">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span> 
                                  <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">igcv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="--channelnet使用通道卷积代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/09/20/channelnet.html"><font color="Blue">ChannelNet</font></a>：使用通道卷积代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/6177d9972ab3f51d91f518b8.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ChannelConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv3d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> 
                              <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                              <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
                              <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">GCWConv</span> <span class="o">=</span> <span class="nc">ChannelConv</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="n">g</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
<span class="n">DWSCWConv</span> <span class="o">=</span> <span class="nc">ChannelConv</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
<span class="n">CCL</span> <span class="o">=</span> <span class="nc">ChannelConv</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="o">-</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">df</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="--efficientnet复合缩放网络深度宽度和分辨率">⚪  <a href="https://0809zheng.github.io/2021/09/11/efficientv1.html"><font color="Blue">EfficientNet</font></a>：复合缩放网络深度、宽度和分辨率</h3>

<p>基本结构与<strong>MobileNetV3</strong>相同，作者称之为<strong>MBConv</strong>：</p>

<p><img src="https://pic.imgdb.cn/item/618337ee2ab3f51d915f7e3f.jpg" alt="" /></p>

<p>复合缩放网络的深度、宽度和分辨率。</p>

<p><img src="https://pic.imgdb.cn/item/61d4ed982ab3f51d91ddc9ed.jpg" alt="" /></p>

<h3 id="--efficientnetv2复合缩放结构渐进训练网络">⚪  <a href="https://0809zheng.github.io/2021/09/12/efficientv2.html"><font color="Blue">EfficientNetV2</font></a>：复合缩放结构，渐进训练网络</h3>

<p>基本结构采用<strong>MBConv</strong>和一种改进的<strong>Fused-MBConv</strong>。<strong>MBConv</strong>与<strong>MobileNetV3</strong>相同，<strong>Fused-MBConv</strong>是将其中的深度可分离卷积还原为标准卷积。</p>

<p><img src="https://pic.imgdb.cn/item/61d558bb2ab3f51d91428d0c.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SELayer定义见MobileNetV3
</span><span class="k">class</span> <span class="nc">Fused</span><span class="o">-</span><span class="nc">MBConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    (3x3 convolution =&gt; [BN] =&gt; ReLU
    =&gt; SELayer 
    =&gt; 1x1 convolution =&gt; [BN] =&gt; Linear)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> 
            <span class="nc">SELayer</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">),</span>
            <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="--ghostnet使用ghost模块代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/11/08/ghostnet.html"><font color="Blue">GhostNet</font></a>：使用Ghost模块代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/6188de152ab3f51d912076c8.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GhostModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">s</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid</span> <span class="o">=</span> <span class="n">out_channels</span><span class="o">//</span><span class="n">self</span><span class="p">.</span><span class="n">s</span>
        <span class="n">self</span><span class="p">.</span><span class="n">primary_conv</span> <span class="o">=</span> <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">mid</span><span class="p">,</span> 
                                        <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cheap_operation</span> <span class="o">=</span> <span class="nc">VanillaConv</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mid</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">mid</span><span class="p">,</span> 
                                           <span class="n">kernel_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> 
                                           <span class="n">groups</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">mid</span><span class="p">,</span> <span class="n">relu</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">primary_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cheap_operation</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<h3 id="--micronet使用微因子卷积代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/11/09/micronet.html"><font color="Blue">MicroNet</font></a>：使用微因子卷积代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/61921ecf2ab3f51d9124c0c7.jpg" alt="" /></p>

<h3 id="--compconv使用分治卷积代替普通卷积">⚪  <a href="https://0809zheng.github.io/2021/08/03/compconv.html"><font color="Blue">CompConv</font></a>：使用分治卷积代替普通卷积</h3>

<p><img src="https://pic.imgdb.cn/item/610919d05132923bf8261f46.jpg" alt="" /></p>

<h1 id="2-寻找乘法的替代">2. 寻找乘法的替代</h1>

<h3 id="--addernet使用l1距离代替卷积乘法">⚪  <a href="https://0809zheng.github.io/2020/09/26/addernet.html"><font color="Blue">AdderNet</font></a>：使用L1距离代替卷积乘法</h3>

<p>卷积神经网络的计算可以表示为卷积滤波器$F \in \Bbb{R}^{d \times d \times c_{in} \times c_{out}}$和输入特征$X \in \Bbb{R}^{H \times W \times c_{in}}$的乘积：</p>

\[Y(m,n,t)=\sum_{i=0}^{d} {\sum_{j=0}^{d} {\sum_{k=0}^{c_{in}} {S(X(m+i,n+j,k),F(i,j,k,t))}}}\]

<p>使用<strong>L1</strong>距离代替卷积计算中的乘法：</p>

\[Y(m,n,t)=-\sum_{i=0}^{d} {\sum_{j=0}^{d} {\sum_{k=0}^{c_{in}} {| X(m+i,n+j,k)-F(i,j,k,t) | }}}\]

<h3 id="--mitchells-approximate使用mitchell近似代替卷积乘法">⚪  <a href="https://0809zheng.github.io/2021/08/08/mitchell.html"><font color="Blue">Mitchell’s approximate</font></a>：使用Mitchell近似代替卷积乘法</h3>
<p>二进制下的乘法运算可以通过对数和指数转换转变成加法运算：</p>

\[pq=2^s, \quad s=\log_2p+\log_2q\]

<p>因此计算$p$和$q$的乘积，可以先通过<strong>Mitchell</strong>近似计算快速对数$\log_2p$和$\log_2q$，将其相加后得到$s$；再通过<strong>Mitchell</strong>近似计算快速指数$2^s$。</p>

<h1 id="-参考文献">⚪ 参考文献</h1>

<ul>
  <li><a href="https://0809zheng.github.io/2021/09/16/squeezenet.html"><font color="Blue">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</font></a>：(arXiv1602)SqueezeNet: 与AlexNet精度相当的轻量级模型。</li>
  <li><a href="https://0809zheng.github.io/2021/09/13/mobilenetv1.html"><font color="Blue">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</font></a>：(arXiv1704)MobileNet: 使用深度可分离卷积构造轻量网络。</li>
  <li><a href="https://0809zheng.github.io/2021/09/18/shufflenet.html"><font color="Blue">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</font></a>：(arXiv1707)ShuffleNet: 使用组卷积与通道打乱构造高效网络。</li>
  <li><a href="https://0809zheng.github.io/2021/09/21/igc.html"><font color="Blue">Interleaved Group Convolutions for Deep Neural Networks</font></a>：(arXiv1707)IGCNet: 交错组卷积网络。</li>
  <li><a href="https://0809zheng.github.io/2021/09/14/mobilenetv2.html"><font color="Blue">MobileNetV2: Inverted Residuals and Linear Bottlenecks</font></a>：(arXiv1801)MobileNetV2: 倒残差与线性瓶颈。</li>
  <li><a href="https://0809zheng.github.io/2021/09/17/squeezenext.html"><font color="Blue">SqueezeNext: Hardware-Aware Neural Network Design</font></a>：(arXiv1803)SqueezeNext: 针对硬件特性的神经网络设计。</li>
  <li><a href="https://0809zheng.github.io/2021/09/22/igcv2.html"><font color="Blue">IGCV2: Interleaved Structured Sparse Convolutional Neural Networks</font></a>：(arXiv1804)IGCV2: 交错结构化稀疏卷积。</li>
  <li><a href="https://0809zheng.github.io/2021/09/19/shufflenetv2.html"><font color="Blue">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</font></a>：(arXiv1807)ShuffleNet V2: 高效卷积神经网络结构设计的实践准则。</li>
  <li><a href="https://0809zheng.github.io/2021/09/20/channelnet.html"><font color="Blue">ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions</font></a>：(arXiv1809)ChannelNets: 使用通道卷积构建高效卷积神经网络。</li>
  <li><a href="https://0809zheng.github.io/2021/09/15/mobilenetv3.html"><font color="Blue">Searching for MobileNetV3</font></a>：(arXiv1905)使用神经结构搜索寻找MobileNet V3。</li>
  <li><a href="https://0809zheng.github.io/2021/09/11/efficientv1.html"><font color="Blue">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</font></a>：(arXiv1905)EfficientNet: 重新考虑卷积神经网络的缩放。</li>
  <li><a href="https://0809zheng.github.io/2021/11/08/ghostnet.html"><font color="Blue">GhostNet: More Features from Cheap Operations</font></a>：(arXiv1911)GhostNet：使用廉价操作构造更多特征。</li>
  <li><a href="https://0809zheng.github.io/2020/09/26/addernet.html"><font color="Blue">AdderNet: Do We Really Need Multiplications in Deep Learning?</font></a>：(arXiv1912)AdderNet：仅使用加法运算的卷积神经网络。</li>
  <li><a href="https://0809zheng.github.io/2021/11/09/micronet.html"><font color="Blue">MicroNet: Towards Image Recognition with Extremely Low FLOPs</font></a>：(arXiv2011)MicroNet：极低FLOPs的图像识别网络。</li>
  <li><a href="https://0809zheng.github.io/2021/08/08/mitchell.html"><font color="Blue">Deep Neural Network Training without Multiplications</font></a>：(arXiv2012)使用Mitchell近似构造加法神经网络。</li>
  <li><a href="https://0809zheng.github.io/2021/09/12/efficientv2.html"><font color="Blue">EfficientNetV2: Smaller Models and Faster Training</font></a>：(arXiv2104)EfficientNetV2: 更小的模型和更快的训练。</li>
  <li><a href="https://0809zheng.github.io/2021/08/03/compconv.html"><font color="Blue">CompConv: A Compact Convolution Module for Efficient Feature Learning</font></a>：(arXiv2106)CompConv：使用分治法的紧凑卷积模块。</li>
</ul>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2021/09/11/efficientv1.html" class="read-next-link"></a>
        <section>
          <span>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</span>
          <p>  EfficientNet: 重新考虑卷积神经网络的缩放.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/61b303ae2ab3f51d9165bc0a.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2021/09/08/gradnorm.html" class="read-next-link"></a>
          <section>
            <span>GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</span>
            <p>  GradNorm: 使用梯度标准化调整多任务损失权重.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/6136cc8844eaada739fa0fb9.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
