<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding">
    <meta name="keywords"  content="论文阅读">
    <!-- Open Graph -->
    <meta property="og:title" content="DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2024/11/25/dinox.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB" class="post-tag">论文阅读</a>
          
        
      </div>
      <h1>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="24-11-25"><i class="iconfont icon-date"></i>25 Nov 2024</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/67441c1e88c538a9b5bb934f.png') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>DINO-X：开放世界目标检测与理解的统一视觉模型.</p>
</blockquote>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/2411.14347">DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</a></li>
</ul>

<h1 id="1-方法简介">1. 方法简介</h1>

<p><strong>DINO-X</strong>是由<strong>IDEA Research</strong>开发的以目标为中心的统一视觉模型，支持各种开放世界感知和目标级理解任务，包括开放世界目标检测和分割、短语定位、视觉提示目标计数、姿态估计、无提示目标检测和识别、密集区域描述等。</p>

<p><img src="https://pic.imgdb.cn/item/6744254888c538a9b5bb97b9.png" alt="" /></p>

<p><strong>DINO-X</strong>采用与<strong>Transformer</strong>相同的编码器-解码器架构，并将开集检测作为其核心训练任务。为了使长尾目标检测更容易，<strong>DINO-X</strong>在模型输入阶段采用了更全面的提示设计：</p>
<ol>
  <li>文本提示：基于用户提供的文本输入来识别感兴趣的目标，这可以覆盖大多数检测场景；</li>
  <li>视觉提示：<strong>DINO-X</strong>支持画框或点等视觉提示，进一步覆盖了仅靠文本无法很好描述的检测场景；</li>
  <li>定制提示符：<strong>DINO-X</strong>特别引入了定制提示符，它可以作为预定义的或用户调整的提示符嵌入来满足定制需求。通过提示调优，可以为不同的域或特定于功能的提示创建域定制的提示，以满足各种功能需求。</li>
</ol>

<p><strong>DINO-X</strong>能够集成多个感知头，同时支持多个目标感知和理解任务，对输入图像提供更详细的目标级理解。除了用于目标检测的边界框头之外，<strong>DINO-X</strong>还实现了三个额外的头：</p>
<ol>
  <li>用于预测检测目标的分割掩码的掩码头；</li>
  <li>用于预测特定类别的关键点的关键点头；</li>
  <li>用于为每个检测目标生成细粒度描述性标题的语言头。</li>
</ol>

<p><img src="https://pic.imgdb.cn/item/67441c4c88c538a9b5bb9361.png" alt="" /></p>

<p><strong>DINO-X</strong>包括两个模型：<strong>DINO-X Pro</strong>模型为各种场景提供增强的感知能力；<strong>DINO-X Edge</strong>模型优化了更快的推理速度，更适合部署在边缘设备上。</p>

<p>为了增强模型的预训练能力，作者构建了一个包含超过1亿个高质量样本的大型数据集<strong>Grounding-100M</strong>，以提高模型的开放词汇检测性能。在这样一个大规模的基础数据集上进行预训练可以得到一个目标的基础级表示。</p>

<p><strong>DINO-X Pro</strong>模型在<strong>COCO</strong>、<strong>LVIS-minival</strong>和<strong>LVIS-val</strong>的零样本基准测试中分别达到了<strong>56.0 AP</strong>、<strong>59.8 AP</strong>和<strong>52.4 AP</strong>。值得注意的是，它在<strong>LVIS-minival</strong>和<strong>LVIS-val</strong>的罕见类别中得分为<strong>63.3</strong>和<strong>56.5</strong>，比<strong>Grounding DINO 1.6 Pro</strong>提高了<strong>5.8 AP</strong>和<strong>5.0 AP</strong>，比<strong>Grounding DINO 1.5 Pro</strong>提高了<strong>7.2 AP</strong>和<strong>11.9 AP</strong>，突出了其识别长尾物体的能力显著提高。</p>

<p><img src="https://pic.imgdb.cn/item/67441c0788c538a9b5bb9347.png" alt="" /></p>

<h1 id="2-模型细节">2. 模型细节</h1>

<h2 id="1dino-x-pro模型">（1）DINO-X Pro模型</h2>

<p><strong>DINO-X</strong>利用<strong>ViT</strong>作为特征提取的骨干，并结合了类似的<strong>Transformer</strong>编码器-解码器架构。<strong>DINO-X</strong>支持的提示编码器：</p>
<ul>
  <li>文本提示编码器：<strong>DINO-X Pro</strong>使用预训练的<strong>CLIP</strong>模型作为文本编码器；</li>
  <li>视觉提示编码器：<strong>DINO-X Pro</strong>采用<strong>T-Rex2</strong>的视觉提示编码器，通过使用框和点两种格式的用户自定义视觉提示来增强目标检测：使用正弦余弦层将这些提示转换为位置嵌入，然后投影到统一的特征空间中；使用不同的线性投影分离框和点提示；然后采用多尺度可变形交叉注意力层，以用户提供的视觉提示为条件，从多尺度特征图中提取视觉提示特征。</li>
  <li>定制提示：<strong>DINO-X Pro</strong>定义了一系列定制提示，可以通过提示调优技术对其进行微调，以资源效率和成本效益高的方式覆盖更多的长尾、特定于领域或特定于功能的场景。例如作者开发了一个通用的目标提示符来支持无提示的开放世界检测，使检测图像中的任何目标成为可能，</li>
</ul>

<p><strong>DINO-X</strong>会在上述提示和从输入图像中提取的视觉特征之间进行深度特征融合，然后对不同的感知任务应用不同的头部。</p>
<ul>
  <li>边界框头：采用语言引导的查询选择模块，选择与输入提示最相关的特征作为解码器目标查询。然后将每个查询馈送到<strong>Transformer</strong>解码器并逐层更新；最后应用简单的<strong>MLP</strong>层预测每个目标查询的相应边界框坐标。使用<strong>L1</strong>损失和<strong>G-IoU</strong>损失进行边界框回归，同时使用对比损失将每个目标查询与输入提示对齐。</li>
  <li>掩码头：通过融合来自<strong>Transformer</strong>编码器的<strong>1/4</strong>分辨率骨干特征和上采样的<strong>1/8</strong>分辨率特征来构建像素嵌入图。然后在<strong>Transformer</strong>解码器的每个目标查询和像素嵌入映射之间进行点积，以获得查询的掩码输出。为了提高训练效率，只将主干的<strong>1/4</strong>分辨率特征图用于掩码预测。在最终的掩码损失计算中，只计算采样点的掩码损失。</li>
  <li>关键点头：<strong>DINO-X</strong>为人和手实例化了两个关键点头，它们分别有<strong>17</strong>个和<strong>21</strong>个预定义的关键点。关键点头从<strong>DINO-X</strong>获取与关键点相关的检测输出（人和手），每个检测输出都被视为查询，并利用一个单独的解码器解码目标关键点，然后将这些关键点发送到多个可变形的<strong>Transformer</strong>解码器层，以预测关键点位置及其可见性。</li>
  <li>语言头：对于任何从<strong>DINO-X</strong>中检测到的目标，首先使用<strong>RoIAlign</strong>操作符从<strong>DINO-X</strong>主干特征中提取其区域特征，并结合其查询嵌入形成目标标记；然后应用一个简单的线性投影来确保它们的尺寸与文本嵌入对齐；轻量级语言解码器将这些区域表示与任务令牌集成在一起，以自动回归的方式生成输出；可学习的任务令牌使语言解码器能够处理各种任务。
<img src="https://pic.imgdb.cn/item/67441fb988c538a9b5bb94a2.png" alt="" /></li>
</ul>

<h2 id="2dino-x-edge模型">（2）DINO-X Edge模型</h2>

<p><strong>DINO-X Edge</strong>采用<strong>EfficientViT</strong>作为高效特征提取的骨干。为了进一步提高<strong>DINO-X Edge</strong>模型的性能和计算效率，作者对模型架构和训练技术进行了以下几个方面的改进：</p>
<ul>
  <li>更强的文本提示编码器：<strong>DINO-X Edge</strong>采用与<strong>Pro</strong>型号相同的<strong>CLIP</strong>文本编码器。在实践中文本提示嵌入可以预先计算，并且不会影响视觉编码器和解码器的推理速度。使用更强大的文本提示编码器通常会产生更好的结果。</li>
  <li>知识蒸馏：<strong>DINO-X Edge</strong>从<strong>Pro</strong>模型中提取知识以提高性能。具体来说，作者利用了基于特征的蒸馏和基于响应的蒸馏，它们分别在<strong>Edge</strong>模型和<strong>Pro</strong>模型之间对齐特征和预测<strong>logits</strong>值。</li>
  <li>改进的<strong>FP16</strong>推理：作者采用一种归一化技术进行浮点乘法，使模型量化到<strong>FP16</strong>而不影响精度。</li>
</ul>

<h2 id="3训练数据">（3）训练数据</h2>

<p>为了确保核心的开放词汇目标检测能力，作者开发了一个高质量和语义丰富的<strong>Grounding</strong>数据集，该数据集由从网络上收集的1亿多张图像组成，称为<strong>Grounding-100M</strong>。</p>

<p>作者使用<strong>T-Rex2</strong>的训练数据和一些额外的工业场景数据进行基于视觉提示的预训练。</p>

<p>作者使用开源分割模型，如<strong>SAM</strong>和<strong>SAM2</strong>为<strong>Grounding-100M</strong>数据集的一部分生成伪掩码注释作为掩码头的主要训练数据。</p>

<p>作者从<strong>Grounding-100M</strong>数据集中采样了一个高质量的数据子集，并利用它们的框注释作为无提示检测训练数据。</p>

<p>作者还收集了超过<strong>1000</strong>万个区域理解数据用于语言头训练，涵盖了目标识别、区域描述、<strong>OCR</strong>和区域级<strong>QA</strong>场景。</p>

<h2 id="4训练策略">（4）训练策略</h2>

<p><strong>DINO-X</strong>采用了两阶段训练策略：</p>
<ol>
  <li>第一阶段对基于文本提示的检测、基于视觉提示的检测和目标分割进行了联合训练；这种大规模的预训练确保了<strong>DINO-X</strong>出色的开放词汇检测性能，并产生了基本的目标级表示。</li>
  <li>第二阶段冻结了<strong>DINO-X</strong>骨干网络，并添加了两个关键点头（人和手）和一个语言头，每个都被单独训练；通过增加更多的头部，极大地扩展了<strong>DINO-X</strong>执行更细粒度感知和理解任务的能力。</li>
</ol>

<p>随后作者利用提示调优技术训练了一个通用目标提示符，允许在保留模型的其他功能的同时进行无提示的任何目标检测。</p>

<p>这种两阶段训练方法确保模型的核心检测能力不受引入新能力的影响，它还验证了大规模开集检测预训练可以作为以目标为中心的模型的强大基础，允许无缝转移到其他开放世界的理解任务。</p>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2024/12/15/quaternion-kinimatics.html" class="read-next-link"></a>
        <section>
          <span>uaternion-kinimatics</span>
          <p>旋转的朴素分解</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241217225501921.png" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2024/11/22/doda.html" class="read-next-link"></a>
          <section>
            <span>DODA: Diffusion for Object-detection Domain Adaptation in Agriculture</span>
            <p>  DODA：农业中目标检测领域自适应的扩散模型.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/67404754d29ded1a8ccf529c.png" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
