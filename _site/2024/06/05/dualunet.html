<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unsupervised Human Contour Extraction From Through-Wall Radar Images Using Dual UNet - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="Unsupervised Human Contour Extraction From Through-Wall Radar Images Using Dual UNet">
    <meta name="keywords"  content="论文阅读">
    <!-- Open Graph -->
    <meta property="og:title" content="Unsupervised Human Contour Extraction From Through-Wall Radar Images Using Dual UNet - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2024/06/05/dualunet.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB" class="post-tag">论文阅读</a>
          
        
      </div>
      <h1>Unsupervised Human Contour Extraction From Through-Wall Radar Images Using Dual UNet</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="24-06-05"><i class="iconfont icon-date"></i>05 Jun 2024</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/675046cdd0e0a243d4dd7995.png') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>通过对偶UNet实现穿墙雷达图像的无监督人体轮廓提取.</p>
</blockquote>

<ul>
  <li>paper：<a href="https://ieeexplore.ieee.org/document/9989420">Unsupervised Human Contour Extraction From Through-Wall Radar Images Using Dual UNet</a></li>
</ul>

<h1 id="1-背景介绍">1. 背景介绍</h1>

<p>跨模态数据标注方法一定程度地缓解了穿墙雷达数据的标注困难，但是在实践中部署跨模态数据标注系统仍然会引入大量开销。由于采集无标签的穿墙雷达数据相对比较容易，因此有必要研究以无监督的方式从未标记的穿墙雷达图像中直接获取人体姿态信息的方法。</p>

<p>无监督学习的目的是在没有人为干预的情况下学习感兴趣的数据特征。目前大多数图像无监督学习方法都是为光学图像设计的，为雷达图像设计的无监督学习方法的相关研究较少。尽管从穿墙雷达图像中可以模糊地识别出人体轮廓，但仍然缺乏有效的无监督人体轮廓提取方法。本文提出了一种基于无监督学习的穿墙雷达人体轮廓提取算法。</p>

<h1 id="2-方法介绍">2. 方法介绍</h1>

<p>这项工作使用<strong>UNet</strong>模型作为图像处理的卷积神经网络模型。<strong>UNet</strong>模型是计算机视觉领域中一种流行的图像分割方法，强大的特征提取能力得益于对称结构和可学习的上采样操作。<strong>UNet</strong>模型采用由下采样路径和上采样路径组成的对称<strong>U</strong>形结构。对称结构有助于从图像中获取细粒度的位置信息。下采样路径首先使用卷积层来处理输入图像，然后使用最大池化层来将特征图的空间大小减半并将通道数加倍。上采样路径使用转置卷积层执行相反的操作，即放大特征图并减少通道数。在每个对应的最大池化层和转置卷积层之间建立残差连接，以恢复采样过程中丢失的空间信息。</p>

<p>所提方法使用两个结构同质的<strong>UNet</strong>模型，一个作为编码器，另一个作为解码器。编码器<strong>UNet</strong>接收穿墙雷达图像并将其转换为特征表示，解码器<strong>UNet</strong>根据特征表示来重构原始图像。在这两种图像之间建立重构损失。为了促进编码器<strong>UNet</strong>学习对人体轮廓有意义的特征表示，进一步构建平滑的归一化割损失。两个损失以端到端的无监督学习形式进行训练，并通过后处理过程将学习到的特征表示转换为人体轮廓的预测结果。本节分别从网络结构设计、损失函数设计与后处理过程三个角度出发，介绍所提基于无监督学习的穿墙雷达人体轮廓提取算法。</p>

<h2 id="1网络结构设计">（1）网络结构设计</h2>

<p>所设计的网络结构如图所示。编码器<strong>UNet</strong>将穿墙雷达图像映射为紧凑的特征表示，解码器<strong>UNet</strong>从该特征表示重建原始雷达图像。通过这种对偶学习的形式，编码器<strong>UNet</strong>在特征表示中存储尽可能多的原始输入信息，使得后续轮廓提取结果可以更好地与输入雷达图像对准。在训练完成之后，解码器<strong>UNet</strong>被丢弃，编码器<strong>UNet</strong>的输出通过后处理生成人体轮廓的最终提取结果。</p>

<p><img src="https://pic.imgdb.cn/item/67504820d0e0a243d4dd7a34.png" alt="" /></p>

<h2 id="2损失函数设计">（2）损失函数设计</h2>

<p>假设\(\left\{x^{(i)}\right\}^N_{i=1}\)是穿墙雷达图像数据集，$U_{Enc}(\cdot)$和$U_{Dec}(\cdot)$分别代表编码器<strong>UNet</strong>和解码器<strong>UNet</strong>，则在原始图像和重构结果之间构建重构损失\(\mathcal{L}_{\text{reco}}\)，使用均方误差来测量所有像素之间的差异：</p>

\[\mathcal{L}_{\text{reco}}=\frac{1}{N}\sum_{i=1}^{N}\left\| x^{(i)} - \text{U}_{\text{Dec}}\left(\text{U}_{\text{Enc}}\left(x^{(i)}\right)\right) \right\|^2\]

<p>重构损失会使编码器<strong>UNet</strong>提取的特征表示随机地落入特征空间中的任意位置。为了使特征表示包含充足的人体轮廓信息，需要引入额外的约束。在这项工作中，使用特征表示来构建输入图像的归一化割，将无监督的像素级条件聚类应用于原始图像，作为衡量特征全局分割质量的标准。假设$U$是属于人体轮廓的像素集合，$V$是所有像素的集合。$w_{uv}$和$w_{us}$分别测量轮廓像素$u$与任意像素$v$和非接触像素$s$之间的权重。则归一化割损失\(\mathcal{L}_{\text{ncut}}\)定义如下：</p>

\[\mathcal{L}_{\text{ncut}}=\frac{\sum_{u \in U,s \in V-U}w_{us}}{\sum_{u \in U,v \in V}w_{uv}}\]

<p>归一化割可以最大程度地地减少人体轮廓和背景之间的总归一化分离，并最大程度地增加人体轮廓的总归一化关联。然而在构造上述归一化割损失时，需要将固定的阈值应用于特征表示来获得像素集合$U$，该操作是不可微分的，阻碍了梯度传播的过程，并进一步阻碍了该方法的端到端训练。保留特征表示中的概率值，构造上述归一化割损失的平滑版本\(\mathcal{L}_{\text{s-ncut}}\)：</p>

\[\begin{aligned}
    \mathcal{L}_{\text{s-ncut}}&amp;=\frac{\sum_{u \in U,s \in V-U}w_{us}}{\sum_{u \in U,v \in V}w_{uv}} =1- \frac{\sum_{u \in U,s \in U}w_{us}}{\sum_{u \in U,v \in V}w_{uv}}
    \\ &amp;= 1- \frac{\sum_{u \in V,s \in V}w_{us}p(u)p(s)}{\sum_{u \in U,v \in V}w_{uv}p(u)}
    \\ &amp;= 1- \frac{\sum_{u \in V,s \in V}w_{us}p(u)p(s)}{\sum_{u \in V}p(u)\sum_{v \in V}w_{uv}}
\end{aligned}\]

<p>其中权重$w_{uv}$设置为：</p>

\[w_{uv}=\mathbb{1}\left(\left\|P(u)-P(v)\right\|_2 \leq r\right) e^{-\frac{\left\|V(u)-V(v)\right\|_2^2}{\sigma_V^2}-\frac{\left\|P(u)-P(v)\right\|_2^2}{\sigma_P^2}}\]

<p>其中$P$和$V$是像素的位置和取值。\(\mathbb{1}\)是示性函数，当条件满足时为1否则为0。其他参数设置$r=5,\sigma_V=10,\sigma_P=4$。通过协同优化重构损失\(\mathcal{L}_{\text{reco}}\)和平滑归一化割损失\(\mathcal{L}_{\text{s-ncut}}\)，所提方法可以准确地重构穿墙雷达图像，并学习适用于人体轮廓分割的特征表示。</p>

<h2 id="3后处理过程">（3）后处理过程</h2>

<p>编码器<strong>UNet</strong>提取的特征表示可以用作像素级预测。由于缺乏有效的监督，特征表示中不可避免地存在噪声。例如，输入雷达图像中的杂波会干扰目标和背景之间的边界。而缺乏平滑度约束将进一步导致目标轮廓区域的定位精度降低和连通性变差。为了获得细粒度的人体轮廓边界，引入全连通条件随机场（<strong>Fully-Connected Conditional Random Field，FC-CRF</strong>）作为后处理方法。用于处理二维图像的<strong>FC-CRF</strong>使用一元势和成对势来构造能量函数：</p>

\[E(z) = \sum_{u} \Phi(u) + \sum_{u,v} \Psi(u,v)\]

<p>其中$u$和$v$是特征表示$z$上的像素。一元势采用输出的负对数似然计算：$\Phi(u) = -\log p(u)$。由于轮廓结果不携带颜色信息，因此成对势仅使用高斯核来对像素之间的位置相似性进行建模，鼓励相似的像素分配相同的标签（轮廓或背景）:</p>

\[\Psi(u,v)= \lambda e^{-\frac{\left\|P(u)-P(v)\right\|_2^2}{2\theta_{\text{x/y}}^2}}\]

<p>其中$P(\cdot)$是像素的位置。设置兼容性$\lambda=10$，标准偏差\(\theta_{\text{x/y}}=10\)。通过后处理过程，特征表示给出了目标的近似轮廓，<strong>FC-CRF</strong>使边界更清晰，并平滑了背景中的噪声。</p>

<h1 id="3-实验分析">3. 实验分析</h1>

<p>使用自研穿墙雷达系统构造一个无监督人体轮廓提取数据集。在不同实验场景中采集五个人体目标的数据，雷达系统以1 m的中心高度工作，检测场景的高度向范围为-1 m至1.5 m，方位向范围为-1 m至1 m，距离向范围为0至2 m。采集结果总共包括64800对雷达-光学图像对，采用10折交叉验证进行模型训练与性能评估。将原始三维雷达图像沿着距离向维度求和得到二维雷达图像，并将雷达图像的尺寸调整为$(64, 64)$。通过预训练的<strong>UNet</strong>模型从光学图像中提取对应的人体轮廓热图，并作为定量评估模型预测结果的标签数据，注意标签数据并不参与模型的训练过程。</p>

<p>所提方法使用的<strong>UNet</strong>模型的整体结构细节如表所示。数据批量大小和总训练轮数分别设置为8和200。使用AdamW优化器训练模型，权重衰减率设置为0.01。初始学习率设置为$0.001$，每50个训练轮数减少$50\%$。所有实验均在<strong>Nvidia RTX3090 GPU</strong>上进行加速。</p>

<p><img src="https://pic.imgdb.cn/item/675048dad0e0a243d4dd7c2c.png" alt="" /></p>

<p>在训练期间是否引入平滑归一化割损失分别对应的重构损失曲线如图所示。从图中可以看出，在没有平滑归一化割损失的情况下，重构损失收敛得更快。当引入平滑归一化割损失后，重构损失降低缓慢且不太稳定。这是因为当引入平滑归一化割损失时，特征表示空间被迫与图像的良好分割结果更加一致，代价是重构原始图像的能力减弱。所提方法在最小化图像差异性和最大化像素总关联之间进行权衡，使得特征表示倾向于提供更好的分割结果。</p>

<p><img src="https://pic.imgdb.cn/item/675048eed0e0a243d4dd7c5b.png" alt="" /></p>

<p>由于穿墙雷达图像缺乏无监督分割的基准方法，将所提方法与几种常用的无监督光学图像分割方法进行比较，如$k$-means聚类和图分割方法。这些方法使用亮度或纹理信息提取图像局部区域的特征，并将这些特征用于像素级聚类。Ji等人提出恒定信息聚类（<strong>Invariant Information Clustering，IIC</strong>），这是一种严格基于信息论的无监督图像分割方法。此外在超过<strong>1100</strong>万张光学图像上进行大规模预训练的开集图像分割算法“分割一切模型”（<strong>Segment Anything Model，SAM</strong>）也被用于定量评估。为了公平地比较不同方法的性能，使用<strong>IoU</strong>指标来计算预测结果和标签之间的像素相似性。<strong>IoU</strong>指标的取值范围为$[0,1]$，值越大表示相似性越强。在验证集上采用不同阈值$[0.5:0.95;0.05]$计算<strong>IoU</strong>指标，并将结果列于表中。</p>

<p><img src="https://pic.imgdb.cn/item/67504928d0e0a243d4dd7ce4.png" alt="" /></p>

<p>所提方法的mIoU指标为0.374。考虑到所提方法在整个训练过程中都遵循无监督的训练方式，分割结果是具有竞争力的。传统方法的预测结果与所提方法相比存在明显差距，这是因为卷积神经网络具有强大的特征提取能力，可以从图像中提取空间不变的语义特征，其对特定任务数据集的适应性也超过了其他基于统计的方法。尽管<strong>SAM</strong>模型在大规模光学图像数据上进行了预训练，由于光学图像与雷达图像具有较大的语义差异，因此将其直接迁移到雷达图像数据集上表现受限。</p>

<p>为了进一步验证所提方法的有效性，分开处理该方法的两个关键组成部分：平滑归一化割损失和<strong>FC-CRF</strong>。分别丢弃这两个部分对应的<strong>IoU</strong>曲线如图所示。在没有平滑归一化割损失的情况下，所提方法只关注穿墙雷达图像的重构过程，不能提供潜在的分割特征表示，因此所提方法完全不能提取人体轮廓。在没有<strong>FC-CRF</strong>的情况下，提取的轮廓将包含噪声，并且无法清楚地确定边界，因此整体性能会有所下降。</p>

<p><img src="https://pic.imgdb.cn/item/67504949d0e0a243d4dd7d43.png" alt="" /></p>

<p>所提方法的单目标人体轮廓提取结果如图所示。结果表明所提方法提取了人体躯干相对完整的轮廓，但对于四肢的提取结果较差。由于穿透墙壁的电磁波频率范围是0-3 GHz，在该频段下人体相当于反射体，因此雷达系统采集的目标回波信号通常只包含关于人体四肢子集的信息，在单一信号中可能会错过偏离雷达方向的部分肢体等身体部位。此外，人体躯干附近的部位有较大的反射区域，很容易被雷达捕捉到，而手臂和腿等较小的轮廓细节很容易丢失。当四肢远离躯干时，例如伸展手臂的动作，此时它们很容易在提取结果中被忽略。</p>

<p><img src="https://pic.imgdb.cn/item/67504961d0e0a243d4dd7d78.png" alt="" /></p>

<p>所提方法的多目标人体轮廓提取结果如图所示。由于目标之间的多径效应，采集到的雷达回波会受到干扰。此时虽然可以提取人体躯干的主要部分，但一些反射区域较小的躯体部分很难捕捉到，例如手臂的空间位置。并且随着目标数量的增加，如何从有限的信息中以无监督的形式提取更准确的人体轮廓，仍然是一个较为困难的挑战。</p>

<p><img src="https://pic.imgdb.cn/item/6750496ad0e0a243d4dd7d8c.png" alt="" /></p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2024/06/06/rpsnet.html" class="read-next-link"></a>
        <section>
          <span>Recovering Human Pose and Shape From Through-the-Wall Radar Images</span>
          <p>  从穿墙雷达图像中恢复人体姿态和形状.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/67518b52d0e0a243d4de4c18.png" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2024/06/04/twhpr.html" class="read-next-link"></a>
          <section>
            <span>Through-Wall Human Pose Reconstruction Based on Cross-Modal Learning and Self-Supervised Learning</span>
            <p>  基于跨模态学习和自监督学习的穿墙人体姿态重构.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/6750411ed0e0a243d4dd76cf.png" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
