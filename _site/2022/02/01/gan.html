<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成对抗网络(Generative Adversarial Network) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="生成对抗网络(Generative Adversarial Network)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="生成对抗网络(Generative Adversarial Network) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2022/02/01/gan.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>生成对抗网络(Generative Adversarial Network)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="22-02-01"><i class="iconfont icon-date"></i>01 Feb 2022</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.downk.cc/item/5ebd0c69c2a9a83be54ed281.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Generative Adversarial Networks.</p>
</blockquote>

<p>本文目录：</p>
<ol>
  <li>理解生成对抗网络：博弈论视角、优化视角、能量模型视角、动力学视角</li>
  <li>使用<strong>Pytorch</strong>实现生成对抗网络：<strong>MMGAN</strong>、<strong>NSGAN</strong>、合并交替优化</li>
  <li>生成对抗网络的评估指标：<strong>KDE</strong>、<strong>IS</strong>、<strong>FID</strong>、<strong>KID</strong></li>
  <li>生成对抗网络的训练困难：纳什均衡、低维流形、梯度消失、模式崩溃</li>
  <li>生成对抗网络的各种变体：改进目标函数、改进网络结构、改进优化过程、其他应用</li>
</ol>

<h1 id="1-理解生成对抗网络">1. 理解生成对抗网络</h1>

<p><strong>生成对抗网络(Generative Adversarial Network, GAN)</strong>是一种生成模型，可以用来生成图像、文本、语音等结构化数据(<strong>structured data</strong>)。</p>

<p>假设真实数据具有概率分布\(P_{data}(x)\)，<strong>GAN</strong>使用一个<strong>生成器generator</strong>构造真实分布的一个近似分布\(P_G(x)\)，并使用一个<strong>判别器discriminator</strong>衡量生成分布和真实分布之间的差异。</p>

<ul>
  <li><strong>生成器</strong> $G$：生成器是一个神经网络，从一个形式简单的概率分布$P_Z(z)$中采样$z$，经过生成器$G$得到输入数据概率分布\(P_{data}\)的估计\(P_G(x)=G(z)\)；<img src="https://pic.downk.cc/item/5eb555dcc2a9a83be5b2b166.jpg" alt="" /></li>
  <li><strong>判别器</strong> $D$：判别器是一个二分类器，用于区分从真实数据分布\(P_{data}\)（标记为$1$）和生成分布\(P_G\)（标记为$0$）中采样得到的数据。<img src="https://pic.downk.cc/item/5eb55bb7c2a9a83be5b7fced.jpg" alt="" /></li>
</ul>

<p>对于判别器$D$，希望其能正确地区分真实数据与生成数据。若输入数据来自真实分布\(P_{data}\)，则希望其输出结果接近$1$；反之若数据来自生成分布$P_G$，则希望其输出结果接近$0$。优化目标采用二元交叉熵：</p>

\[D^* = \mathop{\arg \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z)))]\]

<p>对于生成器$G$，希望其能够成功地欺骗判别器，使其将生成样本误分类成真实样本：</p>

\[G^* = \mathop{\arg \min}_{G} \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z)))]\]

<h3 id="-从博弈论视角理解生成对抗网络">⚪ 从博弈论视角理解生成对抗网络</h3>

<p>若将判别器$D$和生成器$G$的目标函数合并，记为：</p>

\[\begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D} L(G,D) &amp; =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z)))] \\ &amp; =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))] \end{aligned}\]

<p>上式表示判别器$D$和生成器$G$在进行<strong>极小极大博弈(minimax game)</strong>：这也是一个<strong>零和博弈(zero-sum game)</strong>，参与博弈的双方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”。
在交替迭代的过程中，双方都极力优化自己的网络，从而形成竞争对抗，直到双方达到<strong>纳什均衡 (Nash equilibrium)</strong>。</p>

<p>观察目标可知博弈过程旨在寻找使得判别器$D$造成的损失$L(G,D)$最大值最小的生成器$G$。</p>

<p><img src="https://pic.imgdb.cn/item/632eaa8a16f2c2beb1cc7ff3.jpg" alt="" /></p>

<h3 id="-从优化视角理解生成对抗网络">⚪ 从优化视角理解生成对抗网络</h3>

<p>将目标函数写成积分形式：</p>

\[\begin{aligned}  L(G,D)  &amp; =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))]  \\ &amp; =\int_x  (P_{data}(x)\log D(x) + P_{G}(x)\log(1-D(x))) dx \end{aligned}\]

<p>下面先求判别器$D$的最优值$D^{*}$，注意到积分不影响最优值的取得，因此计算被积表达式的极值\(\frac{\partial L(G,D)}{\partial D} = 0\)，得：</p>

\[D^*(x) = \frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)} \in [0,1]\]

<p>若生成器$G$也训练到最优值，此时有\(P_{data}(x)≈P_{G}(x)\)，则判别器退化为<strong>常数</strong> $D^{*}(x)=\frac{1}{2}$，失去判别能力。</p>

<p>当判别器$D$取得最优值$D^{*}$时，目标函数为：</p>

\[\begin{aligned}  L(G,D^*)  &amp; =\int_x  (P_{data}(x)\log D^*(x) + P_{G}(x)\log(1-D^*(x))) dx \\ &amp; =\int_x  (P_{data}(x)\log \frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)} + P_{G}(x)\log\frac{P_{G}(x)}{P_{data}(x)+P_{G}(x)}) dx \\ &amp; =\int_x  (P_{data}(x)\log \frac{P_{data}(x)}{\frac{P_{data}(x)+P_{G}(x)}{2}} + P_{G}(x)\log\frac{P_{G}(x)}{\frac{P_{data}(x)+P_{G}(x)}{2}}-2\log 2) dx \\ &amp; = 2D_{JS}[P_{data}(x) || P_G(x)]-2\log 2 \end{aligned}\]

<p>其中$D_{JS}$表示<a href="https://0809zheng.github.io/2020/02/03/kld.html#-js%E6%95%A3%E5%BA%A6-jenson-shannon-divergence"><font color="blue">JS散度</font></a>。因此当判别器$D$取得最优时，<strong>GAN</strong>的损失函数衡量了真实分布\(P_{data}(x)\)与生成分布\(P_G(x)\)之间的<strong>JS散度</strong>。若生成器$G$也取得最优值，则损失函数取得<strong>最小值</strong> $-2\log 2$。</p>

<p>另一方面，传统的基于<strong>极大似然估计(maximum likelihood estimation)</strong>的生成模型本质上是在最小化真实分布\(P_{data}(x)\)与生成分布\(P_G(x)\)之间的<a href="https://0809zheng.github.io/2020/02/03/kld.html#-kl%E6%95%A3%E5%BA%A6-kullback-leibler-divergence"><font color="blue">KL散度</font></a>：</p>

\[\begin{aligned} \mathop{ \max}_{G} L(G)  &amp; = \Bbb{E}_{x \text{~} P_{data}(x)}[\log P_G(x)] = \int_x P_{data}(x)\log P_G(x) dx \\ &amp;  = \int_x P_{data}(x)\log \frac{P_G(x)}{P_{data}(x)} dx +  \int_x P_{data}(x)\log P_{data}(x) dx \\ &amp; = -D_{KL}[P_{data}(x) || P_G(x)] + Const.  \end{aligned}\]

<p>由于<strong>JS</strong>散度相比于<strong>KL</strong>散度具有对称性、平滑性等优点，因此通常认为<strong>GAN</strong>相比于传统的生成模型能够取得更好的表现。</p>

<p>值得一提的是，<strong>GAN</strong>的目标函数只有先精确完成\(\mathop{\max}_{D}\)，然后再进行\(\mathop{ \min}_{G}\)，才相当于优化两个分布的<strong>JS</strong>散度；在实际训练时常采用<strong>交替优化</strong>步骤，即先优化判别器再优化生成器，并且判别器的更新次数通常更多一些(或使用的学习率更大一些)；这样做的目的也是为了让判别器先取得局部最优，从而让生成器的优化过程趋近于<strong>JS</strong>散度的最小化过程(但理论上不可能精确逼近分布度量)。</p>

<h3 id="-从能量模型视角理解生成对抗网络">⚪ 从能量模型视角理解生成对抗网络</h3>

<p><a href="https://0809zheng.github.io/2020/04/12/energy.html">能量模型</a>是指使用如下能量分布拟合一批真实数据$x_1,x_2,\cdots,x_n$~\(P_{data}(x)\)：</p>

\[q_{\theta}(x) = \frac{e^{-U_{\theta}(x)}}{Z_{\theta}},Z_{\theta} = \int e^{-U_{\theta}(x)}dx\]

<p>其中$U_{\theta}(x)$是带参数的能量函数；$Z_{\theta}$是配分函数(归一化因子)。直观地，真实数据分布在能量函数中势最小的位置。我们希望通过对抗学习使得生成数据$\hat{x}_1,\hat{x}_2,\cdots \hat{x}_n$的势也尽可能小。</p>

<p><img src="https://pic1.imgdb.cn/item/634e13f716f2c2beb1b9d59f.jpg" alt="" /></p>

<p>使用判别器$D(x)$拟合能量函数$U_{\theta}(x)$，使用生成器构造生成分布$P_G(x)$。根据能量模型的<a href="https://0809zheng.github.io/2020/04/12/energy.html#2-%E8%83%BD%E9%87%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%AD%A3%E7%9B%B8-%E8%B4%9F%E7%9B%B8%E5%88%86%E8%A7%A3">正相-负相分解</a>构造目标函数：</p>

\[D^* \leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  D(x)]-  \Bbb{E}_{x \text{~} P_G(x)}[D(x) ]\]

<p>上式表示判别器的目标为最小化真实数据分布的能量，最大化生成数据分布的能量。</p>

<p>与此同时，生成分布$P_G(x)$与能量分布$q_{\theta}(x)$应该足够接近，使用<strong>KL</strong>散度衡量两者差异：</p>

\[\begin{aligned} D_{KL}[P_G(x) || q_{\theta}(x)] &amp;= \int P_G(x) \log \frac{P_G(x)}{q_{\theta}(x)} dx \\ &amp;= \int P_G(x) \log P_G(x) dx - \int P_G(x) \log q_{\theta}(x) dx \\ &amp;= -H(G(z))+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] + \log Z_{\theta} \end{aligned}\]

<p>上式第一项$H(G(z))$表示生成样本的熵，希望熵越大越好(对应样本多样性越大)；第二项表示生成样本的势能，希望势越小越好(接近真实样本)；第三项是一个常数。通过上式可以构造生成器的目标函数：</p>

\[G^* \leftarrow \mathop{ \min}_{G} -H(G(z))+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ]\]

<p>上式表示生成器的目标为最小化生成数据分布的能量，并最大化生成数据分布的熵。</p>

<h3 id="-从动力学视角理解生成对抗网络">⚪ 从动力学视角理解生成对抗网络</h3>

<p><strong>GAN</strong>的学习过程为交替优化以下目标函数：</p>

\[\begin{aligned} \mathop{ \min}_{\theta_G} \mathop{\max}_{\theta_D} L(\theta_G,\theta_D) &amp; =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x;\theta_D)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z;\theta_G);\theta_D))]  \end{aligned}\]

<p>若学习算法采用<a href="https://0809zheng.github.io/2020/03/02/optimization.html#-%E5%8A%A8%E5%8A%9B%E5%AD%A6%E8%A7%92%E5%BA%A6">梯度下降算法</a>，则上式对应一个由常微分方程组<strong>ODEs</strong>表示的动力学系统：</p>

\[\begin{pmatrix} \dot{\theta}_D \\ \dot{\theta}_G  \end{pmatrix}= \begin{pmatrix} \nabla_{\theta_D} L(\theta_G,\theta_D) \\ -\nabla_{\theta_G} L(\theta_G,\theta_D) \end{pmatrix}\]

<p>通过对上述<strong>ODEs</strong>的分析，可以了解<strong>GAN</strong>的以下性态：</p>
<ul>
  <li>收敛性态：系统的理论均衡点是否存在；令上式右端为$0$得$P_{data}(x)=P_{G}(x)$，因此<strong>GAN</strong>的均衡点通常存在。</li>
  <li>局部渐近收敛性态：从任意一个初值（模型初始化）出发，经过迭代后最终能否到达理论均衡点；可以在在均衡点附近做线性展开分析。</li>
</ul>

<h1 id="2-使用pytorch实现生成对抗网络">2. 使用Pytorch实现生成对抗网络</h1>

<h3 id="-gan的算法流程">⚪ GAN的算法流程</h3>

<p>初始化生成器$G(z;\theta_G)$和判别器$D(x;\theta_D)$，在训练的每一次迭代中，先更新判别器$D$，再更新生成器$G$：</p>
<ul>
  <li><strong>固定生成器$G$，更新判别器$D$</strong>，重复$k$次：
    <ol>
      <li>从训练数据集中采样\(\{x^1,x^2,...,x^n\}\)；</li>
      <li>从随机噪声中采样\(\{z^1,z^2,...,z^n\}\)；</li>
      <li>根据生成器$G$获得生成数据\(\{\tilde{x}^1,\tilde{x}^2,...,\tilde{x}^n\}\)；</li>
    </ol>
  </li>
</ul>

\[θ_D \leftarrow \mathop{\arg \max}_{\theta_D} \frac{1}{n} \sum_{i=1}^{n} {\log D(x^i)} + \frac{1}{n} \sum_{i=1}^{n} {\log (1-D(\tilde{x}^i))}\]

<ul>
  <li><strong>固定判别器$D$，更新生成器$G$</strong>，进行$1$次：
    <ol>
      <li>从随机噪声中采样\(\{z^1,z^2,...,z^n\}\)；</li>
      <li>根据生成器$G$获得生成数据\(\{\tilde{x}^1,\tilde{x}^2,...,\tilde{x}^n\}\)；</li>
    </ol>
  </li>
</ul>

\[\theta_G \leftarrow \mathop{\arg \min}_{\theta_G} \frac{1}{n} \sum_{i=1}^{n} {\log (1-D(\tilde{x}^i))}\]

<p>上述即为<strong>GAN</strong>的标准训练流程，采用该流程的<strong>GAN</strong>也被称作<strong>Minimax GAN (MMGAN)</strong>。</p>

<h3 id="-从mmgan到nsgan">⚪ 从MMGAN到NSGAN</h3>

<p>由于上式在计算$θ_G$时，函数$\log (1-D(x))$在$D(x)$接近$0$（即训练的初始阶段）时梯度较小，在$D(x)$接近$1$（即优化后期）时梯度较大，会造成优化困难；因此在实践中采用下式代替：</p>

\[\theta_G \leftarrow \mathop{\arg \max}_{\theta_G} \frac{1}{n} \sum_{i=1}^{n} {\log D(\tilde{x}^i)}\]

<p>使用上式的<strong>GAN</strong>被称作<strong>Non-saturating GAN (NSGAN)</strong>。</p>

<p><img src="https://pic.imgdb.cn/item/632ec75916f2c2beb1f0b96d.jpg" alt="" /></p>

<h3 id="-gan的pytorch实现">⚪ GAN的Pytorch实现</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># 定义生成器和判别器
</span><span class="n">generator</span> <span class="o">=</span> <span class="nc">Generator</span><span class="p">()</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="nc">Discriminator</span><span class="p">()</span>

<span class="c1"># 定义损失函数
</span><span class="n">adversarial_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">BCELoss</span><span class="p">()</span>

<span class="c1"># 定义优化器
</span><span class="n">optimizer_G</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">generator</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
<span class="n">optimizer_D</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">discriminator</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">real_imgs</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="c1"># 构造对抗标签
</span>        <span class="n">valid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">).</span><span class="n">requires_grad_</span><span class="p">.(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">fake</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">).</span><span class="n">requires_grad_</span><span class="p">.(</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># 从噪声中采样生成图像
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="n">gen_imgs</span> <span class="o">=</span> <span class="nf">generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="c1"># 训练判别器
</span>        <span class="n">optimizer_D</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="c1"># 计算判别器的损失
</span>        <span class="n">real_loss</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">),</span> <span class="n">valid</span><span class="p">)</span>
        <span class="n">fake_loss</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">gen_imgs</span><span class="p">.</span><span class="nf">detach</span><span class="p">()),</span> <span class="n">fake</span><span class="p">)</span> <span class="c1"># 此处不计算生成器的梯度
</span>        <span class="n">d_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">real_loss</span> <span class="o">+</span> <span class="n">fake_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="c1"># 更新判别器参数
</span>        <span class="n">d_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer_D</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># 训练生成器
</span>        <span class="n">optimizer_G</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">g_loss</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">gen_imgs</span><span class="p">),</span> <span class="n">valid</span><span class="p">)</span>
        <span class="n">g_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer_G</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="-技巧合并交替优化">⚪ 技巧：合并交替优化</h3>

<p><strong>GAN</strong>的训练过程采用交替优化的形式：</p>

\[\begin{aligned} D^* &amp;\leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_z}[\log(1-D(G(z)))]  \\ G^* &amp; \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{z \text{~} P_z}[\log D(G(z))] \end{aligned}\]

<p>如果能把交替优化步骤合并为一个步骤，则可以节省优化所需的时间（代价是增大优化过程占用的显存）。</p>

<p>合并交替优化的关键是引入<strong>stop gradient</strong>操作（对应<strong>pytorch</strong>中的<code class="language-plaintext highlighter-rouge">.detach()</code>方法）。</p>

<p>对于判别器，目标是优化判别器的参数，因此把生成器的梯度置零：</p>

\[\begin{aligned} D^* \leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_z}[\log(1-D(\text{StopGrad}(G(z))))] \end{aligned}\]

<p>对于生成器，目标时优化生成器的参数。注意到反向传播中$D(G(z))$会产生生成器和判别器的联合梯度$(\nabla_G L,\nabla_D L)$，而生成器梯度置零后\(\text{StopGrad}(D(G(z)))\)会产生判别器的梯度$(0,\nabla_D L)$；两者作差即为生成器的梯度：</p>

\[\begin{aligned}  G^* &amp; \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{z \text{~} P_z}[\log D(G(z))-\log D(\text{StopGrad}(G(z)))] \end{aligned}\]

<p>值得一提的是，上述形式的生成器目标数值将恒为零，然而梯度<strong>不为零</strong>；此时生成器损失不宜再作为指示训练过程的指标，但是参数更新过程仍然在正常进行。</p>

<p>因此<strong>GAN</strong>的目标函数可以合并写作：</p>

\[\begin{aligned} (D^*, G^*) \leftarrow \mathop{ \max}_{D,G} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_z}[\log(1-D(\text{StopGrad}(G(z))))] \\ &amp;+ \lambda \Bbb{E}_{z \text{~} P_z}[\log D(G(z))-\log D(\text{StopGrad}(G(z)))] \end{aligned}\]

<p>其中$\lambda$控制判别器和生成器的学习率之比为$1:\lambda$。至此<strong>GAN</strong>的更新过程可以被合并为一步：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 定义优化器
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span>
    <span class="n">itertools</span><span class="p">.</span><span class="nf">chain</span><span class="p">(</span><span class="n">generator</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">discriminator</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()),</span> 
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">real_imgs</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="c1"># 构造对抗标签
</span>        <span class="n">valid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">).</span><span class="n">requires_grad_</span><span class="p">.(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">fake</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">).</span><span class="n">requires_grad_</span><span class="p">.(</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># 从噪声中采样生成图像
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="n">gen_imgs</span> <span class="o">=</span> <span class="nf">generator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="c1"># 构造损失函数
</span>        <span class="n">d_real_loss</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">real_imgs</span><span class="p">),</span> <span class="n">valid</span><span class="p">)</span>
        <span class="n">d_fake_loss</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">gen_imgs</span><span class="p">.</span><span class="nf">detach</span><span class="p">()),</span> <span class="n">fake</span><span class="p">)</span> <span class="c1"># 此处不计算生成器的梯度
</span>        <span class="n">d_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">d_real_loss</span> <span class="o">+</span> <span class="n">d_fake_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">g_fake_loss</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">gen_imgs</span><span class="p">),</span> <span class="n">valid</span><span class="p">)</span>
        <span class="n">g_fake_loss_sg</span> <span class="o">=</span> <span class="nf">adversarial_loss</span><span class="p">(</span><span class="nf">discriminator</span><span class="p">(</span><span class="n">gen_imgs</span><span class="p">.</span><span class="nf">detach</span><span class="p">()),</span> <span class="n">valid</span><span class="p">)</span> <span class="c1"># 此处不计算生成器的梯度
</span>        <span class="n">g_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">g_fake_loss</span> <span class="o">-</span> <span class="n">g_fake_loss_sg</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">d_loss</span> <span class="o">+</span> <span class="k">lambda</span> <span class="o">*</span> <span class="n">g_loss</span>

        <span class="c1"># 更新网络参数
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<h1 id="3-生成对抗网络的评估指标">3. 生成对抗网络的评估指标</h1>

<p><strong>GAN</strong>的训练过程是交替迭代的，其目标函数值无法直观地给出生成器和判别器的训练程度，使得<strong>GAN</strong>的训练过程不知何时停止，也不方便对不同模型的性能进行比较。</p>

<p>由于无法直接建立真实数据和生成数据之间的一一对应关系，因此需要为<strong>GAN</strong>设计有效的评估指标，用于评估真实数据分布和生成数据分布之间的“距离”。
本节介绍几种<strong>GAN</strong>的评估指标：</p>
<ul>
  <li><strong>Kernel Density Estimation (KDE)</strong></li>
  <li><strong>Inception Score (IS)</strong></li>
  <li><strong>Fréchet Inception Distance (FID)</strong></li>
  <li><strong>Kernel Inception Distance (KID)</strong></li>
</ul>

<h3 id="-kernel-density-estimation-kde">⚪ Kernel Density Estimation (KDE)</h3>

<p>从生成器$G$中随机采样一些样本，使用<a href="https://0809zheng.github.io/2020/05/29/gaussian-mixture-model.html">高斯混合模型</a>拟合这些样本，得到生成样本分布的估计$P_G$；选择一些真实样本$x^i$，计算其在估计分布中的极大似然概率：</p>

\[L = \frac{1}{N} \sum_{i=1}^{N} {\log (P_G(x^i))}\]

<p>将这个似然值作为<strong>GAN</strong>模型质量的评估指标。这种评估方法存在一些问题，比如模型可能产生不同于已知样本集的高质量样本；也可能存在一些低质量样本的似然值较高。</p>

<p><img src="https://pic.downk.cc/item/5ed9e0abc2a9a83be5e94672.jpg" alt="" /></p>

<h3 id="-inception-score-is">⚪ <a href="https://0809zheng.github.io/2022/03/25/is.html"><font color="Blue">Inception Score (IS)</font></a></h3>

<p><strong>Inception Score</strong>借助图像分类任务评估生成图像的质量。具体地，使用<strong>Inception</strong>模型对生成图像$x$进行分类，得到类别标签$y$；则<strong>IS</strong>定义如下：</p>

\[\begin{aligned} \text{IS}(G) &amp;= e^{\Bbb{E}_{x \text{~} P_G(x)}[D_{KL}(p(y|x)||p(y))]} \\ &amp; ≈ e^{\Bbb{E}_{x \text{~} P_G(x)}[-H[p(y|x)] + H[p(y)]]} \end{aligned}\]

<p>直观地，对于任意一张图像$x$，若其分类标签$p(y|x)$的熵较大，则说明其包含的的主要物体容易被分类，对应图像生成的质量较高；对于所有生成的图像，若平均标签$p(y)$的熵较小，则说明图像类别比较平均，对应图像生成具有多样性。</p>

<p><strong>Inception Score</strong>的缺点是没有使用真实世界样本的统计数据；并且依赖于分类任务，比如使用<strong>ImageNet</strong>数据集训练的<strong>Inception</strong>评估生成其他数据集的<strong>GAN</strong>模型是不合适的。</p>

<h3 id="-fréchet-inception-distance-fid">⚪ <a href="https://0809zheng.github.io/2022/03/24/ttur.html"><font color="Blue">Fréchet Inception Distance (FID)</font></a></h3>

<p><strong>FID</strong>使用<strong>Inception</strong>模型的编码层提取数据特征$x$，并假设编码特征服从多维高斯分布，通过<strong>Fréchet</strong>距离 (也称为<strong>Wasserstein-2</strong>距离) 衡量真实数据特征和生成数据特征的差异。</p>

<p>记真实特征分布$P_{data}(x)$为$N(\mu,\Sigma)$，生成特征分布$P_G(x)$为$N(\mu_G,\Sigma_G)$，则两个分布之间的<strong>FID</strong>定义为：</p>

\[\text{FID}(P_{data}(x),P_G(x))= ||\mu-\mu_G||_2^2+\text{Tr}(\Sigma+\Sigma_G-2(\Sigma\Sigma_G)^{1/2})\]

<h3 id="-kernel-inception-distance-kid">⚪ <a href="https://0809zheng.github.io/2022/03/30/kid.html"><font color="Blue">Kernel Inception Distance (KID)</font></a></h3>

<p><strong>KID</strong>定义为在<strong>Inception</strong>模型提取的特征空间中的<a href="https://0809zheng.github.io/2022/12/06/ipm.html">最大平均差异</a>：</p>

\[\begin{aligned} \text{KID}(P_{data}(x),P_G(x))=&amp; \Bbb{E}_{x,x' \text{~} P_{data}(x)} [\kappa(x,x')] + \Bbb{E}_{x,x' \text{~} P_G(x)} [\kappa(x,x')] \\&amp; -2\Bbb{E}_{x \text{~} P_{data}(x),x' \text{~} P_G(x)} [\kappa(x,x')] \end{aligned}\]

<p>相比于<strong>FID</strong>仅考虑了均值和方差，<strong>KID</strong>通过核技巧考虑到任意阶数的特征矩。</p>

<h1 id="4-生成对抗网络的训练困难">4. 生成对抗网络的训练困难</h1>

<p>众所周知，<strong>GAN</strong>模型的训练比较困难，训练速度较慢且训练过程不稳定。主要体现在以下几点：</p>

<h2 id="1难以实现纳什均衡-hard-to-achieve-nash-equilibrium">（1）难以实现纳什均衡 Hard to achieve Nash equilibrium</h2>

<p><strong>GAN</strong>使用梯度下降算法优化两人<strong>非合作博弈(non-cooperative)</strong>，通常梯度下降算法只有在目标函数为凸函数时才能保证实现纳什均衡。而判别器和生成器独立地优化各自的损失，在博弈中没有考虑到另一方。因此同时更新两个模型的梯度不能保证收敛。</p>

<h2 id="2低维流形-low-dimensional-manifold">（2）低维流形 Low dimensional manifold</h2>

<p>许多真实世界的数据集分布\(P_{data}(x)\)通常集中在高维空间中的一个低维流形上，这是因为真实图像要遵循主题或目标的限制；生成分布\(P_G(x)\)通常也位于一个低维流形上，因为它是由一个低维噪声变量$z$定义的。高维空间中的两个低维流形几乎不可能重叠；即使两个流形有重合的部分，对概率分布采样时也很难采集到重合的样本，从而导致<strong>GAN</strong>训练的不稳定性。</p>

<p><img src="https://pic.imgdb.cn/item/633107ba16f2c2beb14cbe22.jpg" alt="" /></p>

<h2 id="3梯度消失-vanishing-gradient">（3）梯度消失 Vanishing gradient</h2>

<p><strong>GAN</strong>的训练过程进退两难：如果判别器表现较差，则生成器没有准确的反馈，损失函数不能代表真实情况；如果判别器表现较好，则损失函数及其梯度趋近于$0$，训练过程变得非常慢甚至卡住。</p>

<p><img src="https://pic.imgdb.cn/item/6331125716f2c2beb156fc56.jpg" alt="" /></p>

<p>通常<strong>GAN</strong>的训练过程被视为最优化真实分布与生成分布的<strong>f</strong>散度(如<strong>JS</strong>散度)。这类散度在两个概率分布不重叠时始终为常数(或无穷大)，导致更新梯度为$0$。</p>

<h2 id="4模式崩溃-mode-collapse">（4）模式崩溃 Mode collapse</h2>

<p><strong>Mode Collapse</strong>是指在训练过程中生成器可能会崩溃到一种始终产生相同输出的设置。由于生成器无法学习表示复杂的真实世界数据分布，所学习到的生成分布会陷入一个缺乏多样性的局部空间中，只能集中在真实数据分布的一小部分。</p>

<p><img src="https://pic.imgdb.cn/item/633115cf16f2c2beb15a9918.jpg" alt="" /></p>

<p>与之类似的一个问题是<strong>Mode Dropping</strong>，是指真实数据分布通常有多个簇，而生成器的每次迭代过程中只能生成其中某一个簇。</p>

<p><img src="https://pic.imgdb.cn/item/6331164716f2c2beb15b5d28.jpg" alt="" /></p>

<p>从能量的角度，真实数据分布在能量函数所有可能的极小值点附近。在生成数据时通常只需要找到附近的极小值点；若采用带<strong>动量</strong>的优化器，则有可能使其跳出附近的极小值点，集中在一些更小的极小值点附近，从而丧失了数据生成的多样性。因此在<strong>GAN</strong>的优化过程中，尽量使用不带动量的优化器，或者设置较小的动量和学习率。</p>

<h3 id="解决措施生成数据的熵约束">解决措施①：生成数据的熵约束</h3>

<p><strong>Mode Collapse</strong>体现在生成数据太集中，因此可以向生成器的目标中加入约束项，使得生成数据更加分散。一种常用的约束是生成数据的熵$H(G(z))$：</p>

\[\begin{aligned} G^* &amp; \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{z \text{~} P_z}[\log D(G(z))] + H(G(z)) \end{aligned}\]

<h3 id="解决措施gan的集成">解决措施②：GAN的集成</h3>

<p>在<strong>GAN</strong>的训练过程中同时训练多个生成器，每次随机选择一个生成器来生成样本。</p>

<h1 id="5-生成对抗网络的各种变体">5. 生成对抗网络的各种变体</h1>

<p>生成对抗网络的设计是集目标函数、网络结构、优化过程于一体的。本节分别从这三个方面介绍<strong>GAN</strong>的各种变体：</p>
<ul>
  <li>改进目标函数：基于分布散度(如<strong>f-GAN</strong>, <strong>BGAN</strong>, <strong>Softmax GAN</strong>, <strong>RGAN</strong>, <strong>LSGAN</strong>, <strong>WGAN-div</strong>, <strong>GAN-QP</strong>, <strong>Designing GAN</strong>)、基于积分概率度量(如<strong>WGAN</strong>, <strong>WGAN-GP</strong>, <strong>DRAGAN</strong>, <strong>SN-GAN</strong>, <strong>GN-GAN</strong>, <strong>GraN-GAN</strong>, <strong>c-transform</strong>, <strong>McGAN</strong>, <strong>MMD GAN</strong>, <strong>Fisher GAN</strong>)</li>
  <li>改进网络结构：调整神经网络(如<strong>DCGAN</strong>, <strong>SAGAN</strong>, <strong>BigGAN</strong>, <strong>Self-Modulation</strong>, <strong>StyleGAN1,2,3</strong>, <strong>TransGAN</strong>)、引入编码器(如<strong>VAE-GAN</strong>, <strong>BiGAN</strong>, <strong>VQGAN</strong>)、使用能量模型(如<strong>EBGAN</strong>, <strong>LSGAN</strong>, <strong>BEGAN</strong>, <strong>MAGAN</strong>, <strong>MEG</strong>)、由粗到细的生成(如<strong>LAPGAN</strong>, <strong>StackGAN</strong>, <strong>PGGAN</strong>, <strong>SinGAN</strong>)</li>
  <li>改进优化过程：<strong>TTUR</strong>, <strong>Dirac-GAN</strong>, <strong>VDB</strong>, <strong>Cascading Rejection</strong>, <strong>ADA</strong>, <strong>Hubness Prior</strong></li>
  <li>其他应用：条件生成(如<strong>CGAN</strong>, <strong>InfoGAN</strong>, <strong>ACGAN</strong>, <strong>Projection Discriminator</strong>)、图像到图像翻译(有配对数据, 如<strong>Pix2Pix</strong>, <strong>BicycleGAN</strong>, <strong>LPTN</strong>; 无配对数据, 如<strong>CoGAN</strong>, <strong>PixelDA</strong>, <strong>CycleGAN</strong>, <strong>DiscoGAN</strong>, <strong>DualGAN</strong>, <strong>UNIT</strong>, <strong>MUNIT</strong>, <strong>TUNIT</strong>, <strong>StarGAN</strong>, <strong>StarGAN v2</strong>, <strong>GANILLA</strong>, <strong>NICE-GAN</strong>)、超分辨率(如<strong>SRGAN</strong>, <strong>ESRGAN</strong>)、图像修补(如<strong>Context Encoder</strong>, <strong>CCGAN</strong>, <strong>SPADE</strong>)、机器学习应用(如<strong>Semi-Supervised GAN</strong>, <strong>AnoGAN</strong>, <strong>ClusterGAN</strong>)</li>
</ul>

<h2 id="1改进目标函数">（1）改进目标函数</h2>

<p>根据前述分析可知，<strong>GAN</strong>的判别器$D$定义了真实数据分布\(P_{data}(x)\)与生成数据分布\(P_G(x)\)之间的分布“距离”，生成器$G$的目标函数为最小化两个分布的距离。</p>

<p>对于标准的<strong>GAN</strong>模型，当判别器$D$取得最优时，其目标函数衡量了两个分布之间的<strong>JS</strong>散度。对于<strong>JS</strong>散度，当两个概率分布没有重合时，散度取值通常是固定值，从而导致优化梯度为0。</p>

<p>在实践中可以选择具有更平滑的值空间的分布距离度量指标。常用的距离度量包括<strong>分布散度</strong>和<strong>积分概率度量</strong>。</p>

<h3 id="-基于分布散度的gan模型">⚪ 基于分布散度的GAN模型</h3>

<p><a href="https://0809zheng.github.io/2020/02/03/kld.html"><font color="blue">分布散度</font></a> $D[p,q]$是关于概率分布$p(x)$和$q(x)$的标量函数，并且满足：</p>
<ul>
  <li>非负性：$D[p,q]\geq 0$恒成立；</li>
  <li>$D[p,q]=0 \leftrightarrow p=q$</li>
</ul>

<p>分布散度可以用于衡量两个概率分布的距离，可以通过构造真实数据分布\(P_{data}(x)\)和生成分布\(P_G(x)\)之间的分布散度来设计目标函数。</p>

<p>然而由于分布的形式通常是未知的，因此散度无法直接求解，此时可通过<a href="https://0809zheng.github.io/2020/02/03/kld.html#2-f%E6%95%A3%E5%BA%A6%E7%9A%84%E5%B1%80%E9%83%A8%E5%8F%98%E5%88%86%E4%BC%B0%E7%AE%97">凸函数的共轭函数</a>将散度转化为对偶形式（带$\max$的形式）；进而最小化该散度的对偶形式，从而得到一个$\min$-$\max$过程。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">方法</th>
      <th style="text-align: center">目标函数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Minimax GAN <br /> (MMGAN)</td>
      <td style="text-align: center">\(\begin{aligned} \mathop{\min}_{G}\mathop{\max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">Non-Saturating GAN <br /> (NSGAN)</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))]  \\  &amp;  \mathop{ \max}_{G}  \Bbb{E}_{x \text{~} P_{G}(x)}[\log D(x)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/07/fgan.html"><font color="Blue">f-GAN</font></a> <br /> 使用f散度构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned}  \mathop{\min}_{G} \mathop{\max}_{D \in f'(\Bbb{D})}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]- \Bbb{E}_{x \text{~} P_{G}(x)}[f^*(D(x))] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/28/bgan.html"><font color="Blue">Boundary-Seeking GAN <br /> (BGAN) </font></a> <br /> 边界搜索</td>
      <td style="text-align: center">\(\begin{aligned} &amp; \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))]  \\ &amp;  \mathop{ \min}_{G} \Bbb{E}_{x \text{~} P_G(x)}[ \frac{1}{2}(\log D(x) - \log (1-D(x)))^2 ] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/05/15/softmaxgan.html"><font color="Blue">Softmax GAN</font></a> <br /> 使用<strong>Softmax</strong>函数构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \min}_{D}  \frac{1}{|P_{data}|}\Bbb{E}_{x \text{~} P_{data}(x)} [ D(x)] + \log  Z_P \\  &amp; \mathop{ \min}_{G} \frac{1}{|P_{data}|+|P_{G}|}(\Bbb{E}_{x \text{~} P_{data}(x)} [ D(x)]+\Bbb{E}_{x \text{~} P_G(x)} [ D(x)] )+ \log  Z_P \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/21/rgan.html"><font color="Blue">Relativistic GAN <br /> (RGAN)</font></a> <br /> 相对判别器</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \min}_{D}  -\Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_r)-D(x_f))] \\  &amp; \mathop{ \min}_{G}- \Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_f)-D(x_r))] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/15/lsgan.html"><font color="Blue">Least Squares GAN <br /> (LSGAN) </font></a> <br /> 使用均方误差构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[(D(x)-b)^2] + \Bbb{E}_{x \text{~} P_{G}(x)}[(D(x)-a)^2]  \\  &amp;  \mathop{ \max}_{G}  \Bbb{E}_{x \text{~} P_{G}(x)}[(D(x)-c)^2] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/09/wgandiv.html"><font color="Blue">Wasserstein Divergence GAN <br /> (WGAN-div)</font></a> <br /> 使用W散度构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned}  \mathop{ \min}_{G} \mathop{ \max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] - k\Bbb{E}_{x \text{~} r(x)}[ || \nabla_xD(x) ||^p ]   \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/22/ganqp.html"><font color="Blue">GAN-QP</font></a> <br /> 使用平方势散度构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \max}_{D} \Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[D(x_r,x_f)-D(x_f,x_r) -\frac{(D(x_r,x_f)-D(x_f,x_r))^2}{2 \lambda d(x_r,x_f)} ]  \\  &amp; \mathop{ \min}_{G}\Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[D(x_r,x_f)-D(x_f,x_r)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/03/07/design.html"><font color="Blue">Designing GAN</font></a> <br /> 在对偶空间构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned} \mathop{ \min}_{G} \mathop{ \max}_{D \in \Omega} \Bbb{E}_{x\text{~}P_{data}(x)}[ \phi(D(x))]+ \Bbb{E}_{x\text{~}P_{G}(x)}[\psi(D(x))] \end{aligned}\)</td>
    </tr>
  </tbody>
</table>

<h3 id="-基于积分概率度量的gan模型">⚪ 基于积分概率度量的GAN模型</h3>

<p><a href="https://0809zheng.github.io/2022/12/06/ipm.html"><font color="blue">积分概率度量</font></a>寻找满足某种限制条件的函数集合\(\mathcal{F}\)中的连续函数$f(\cdot)$，然后寻找一个最优的\(f(x)\in \mathcal{F}\)使得两个概率分布$p(x)$和$q(x)$之间的期望差异最大，该最大差异定义为两个分布之间的距离：</p>

\[d_{\mathcal{F}}(p(x),q(x)) = \mathop{\sup}_{f(x)\in \mathcal{F}} \Bbb{E}_{x \text{~} p(x)}[f(x)]-\Bbb{E}_{x \text{~} q(x)}[f(x)]\]

<table>
  <thead>
    <tr>
      <th style="text-align: center">方法</th>
      <th style="text-align: center">目标函数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/04/wgan.html"><font color="Blue">Wasserstein GAN <br /> (WGAN)</font></a> <br /> 使用W距离构造目标函数</td>
      <td style="text-align: center">\(\begin{aligned}  \mathop{ \min}_{G} \mathop{ \max}_{D,|D|_L \leq K} \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)]  \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/06/wgangp.html"><font color="Blue">Wasserstein GAN <br /> gradient penalty <br /> (WGAN-GP)</font></a> <br /> 使用梯度惩罚约束Lipschitz连续性</td>
      <td style="text-align: center">\(\begin{aligned} \mathop{ \max}_{D} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ &amp; - λ \Bbb{E}_{x \text{~} \epsilon P_{data}(x) + (1-\epsilon)P_{G}(x) }[(|| \nabla_xD(x) || -1)^2]  \\   \mathop{ \min}_{G}&amp;  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/05/14/dragan.html"><font color="Blue">DRAGAN</font></a> <br /> 调整梯度惩罚的插值空间</td>
      <td style="text-align: center">\(\begin{aligned} \mathop{ \max}_{D} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ &amp; - λ \Bbb{E}_{x \text{~} P_{data}(x), \delta \text{~} N(0,cI) }[(|| \nabla_xD(x+\delta) || -k)^2]  \\   \mathop{ \min}_{G}&amp;  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/08/sngan.html"><font color="Blue">Spectral Normalized GAN <br /> (SN-GAN) </font></a> <br /> 使用谱归一化约束Lipschitz连续性</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\max(0,1-D(x))]-\Bbb{E}_{x \text{~} P_{G}(x)}[\max(0,1+D(x))] \\   &amp;  \mathop{ \min}_{G} -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/10/gngan.html"><font color="Blue">Gradient Normalized GAN <br /> (GN-GAN) </font></a> <br /> 使用梯度归一化约束Lipschitz连续性</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ &amp; D^*  \leftarrow \frac{D(x)}{||\nabla_x D(x)||+|D(x)|} \\  &amp; \mathop{ \min}_{G}  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/11/grangan.html"><font color="Blue">Gradient Normalized GAN v2 <br /> (GraN-GAN)</font></a> <br /> 使用梯度归一化约束Lipschitz连续性</td>
      <td style="text-align: center">\(\begin{aligned}  &amp; \mathop{ \max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ &amp; D^*  \leftarrow \frac{D(x) \cdot ||\nabla_x D(x)||}{||\nabla_x D(x)||^2+\epsilon} \\  &amp;  \mathop{ \min}_{G}  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/02/12/ctrans.html"><font color="Blue">c-transform WGAN</font></a> <br /> 更精确的W距离近似</td>
      <td style="text-align: center">\(\begin{aligned}  \mathop{ \min}_{G} \mathop{ \max}_{D,|D|_L \leq K}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]  +\Bbb{E}_{x \text{~} P_{G}(x)}[\mathop{\min}_{\tilde{x} \text{~} P_{data}(x)} \{ c(\tilde{x}-x)- D(\tilde{x})\}]  \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/03/26/mcgan.html"><font color="Blue"> Mean and Covariance Feature Matching GAN <br /> (McGAN)</font></a> <br /> 均值和协方差特征匹配</td>
      <td style="text-align: center">\(\begin{aligned} \mathop{ \min}_{G} \mathop{ \max}_{D} &amp;||\Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] ||_p \\ &amp;+ ||\Bbb{E}_{x \text{~} P_{data}(x)}[D(x)D^T(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)D^T(x)] ||_{*} \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/03/28/mmdgan.html"><font color="Blue"> Maximum Mean Discrepancy <br /> (MMD GAN)</font></a> <br /> 最大平均差异</td>
      <td style="text-align: center">\(\begin{aligned} &amp; \mathop{ \min}_{G} \mathop{ \max}_{D} \Bbb{E}_{x,x' \text{~} P_{data}(x)} [\kappa(x,x')] + \Bbb{E}_{x,x' \text{~} P_G(x)} [\kappa(x,x')] -2\Bbb{E}_{x \text{~} P_{data}(x),x' \text{~} P_G(x)} [\kappa(x,x')] \\ &amp;\kappa(x,x') = \exp(-||D(x)-D(x')||) \end{aligned}\)</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://0809zheng.github.io/2022/03/29/fisher.html"><font color="Blue">Fisher GAN</font></a> <br /> Fisher差异</td>
      <td style="text-align: center">\(\begin{aligned} \mathop{ \min}_{G} \mathop{ \max}_{D} \frac{\Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_G(x)}[D(x)]}{\sqrt{\frac{1}{2}(\Bbb{E}_{x \text{~} P_{data}(x)}[D^2(x)]+\Bbb{E}_{x \text{~} P_G(x)}[D^2(x)])}}  \end{aligned}\)</td>
    </tr>
  </tbody>
</table>

<h2 id="2改进网络结构">（2）改进网络结构</h2>

<h3 id="a-调整神经网络">a. 调整神经网络</h3>

<h3 id="-deep-convolutional-gan-dcgan">⚪ <a href="https://0809zheng.github.io/2022/02/05/dcgan.html"><font color="Blue">Deep Convolutional GAN (DCGAN)</font></a></h3>

<p><strong>DCGAN</strong>使用卷积神经网络构造生成对抗网络，为了稳定卷积网络的训练过程，作者提出了以下几点设计思路：</p>
<ol>
  <li>去掉网络中的<strong>pooling</strong>层，在判别器中使用<strong>Strided convolution</strong> (步幅卷积)进行下采样，在生成器中使用<strong>transposed convolution</strong>（转置卷积）进行上采样；</li>
  <li>在判别器和生成器中使用<strong>batch norm</strong>；</li>
  <li>移除网络中的所有全连接层；</li>
  <li>生成器的输出层使用<strong>Tanh</strong>激活函数，其他层使用<strong>ReLU</strong>激活函数；</li>
  <li>判别器的所有层使用<strong>LeakyReLU</strong>激活函数。</li>
</ol>

<p><img src="https://pic1.imgdb.cn/item/6343747916f2c2beb122d53e.jpg" alt="" /></p>

<h3 id="-resnet">⚪ ResNet</h3>

<p>为进一步增强卷积网络的非线性表示能力，将<strong>ResNet</strong>引入了<strong>GAN</strong>的模型构建。这类网络的主要特点如下：</p>
<ol>
  <li>由于<strong>stride</strong> $&gt;1$的卷积操作存在<a href="">棋盘效应</a>，因此移除了步幅卷积和转置卷积；在下采样时采用平均池化，在上采样时采用线性插值；</li>
  <li>通过增加残差模块的数量，能够同时增加网络的非线性表示能力和深度；</li>
  <li>卷积核设置为$3\times 3$；激活函数统一使用<strong>ReLU</strong>；<strong>BatchNorm</strong>可以替换为<strong>InstanceNorm</strong>等；</li>
  <li>由于<strong>GAN</strong>的模型初始化通常比较小，将残差形式调整为$x + \alpha f(x); \alpha &lt; 1$具有更好的稳定性。</li>
</ol>

<p><img src="https://pic1.imgdb.cn/item/6349633716f2c2beb1785f6d.jpg" alt="" /></p>

<h3 id="-self-attention-gan-sagan">⚪ <a href="https://0809zheng.github.io/2022/05/25/sagan.html"><font color="Blue">Self-Attention GAN (SAGAN)</font></a></h3>

<p><strong>SAGAN</strong>把自注意力机制引入模型结构中，有助于对图像区域中长距离、多层次的依赖关系进行建模。自注意力机制在计算输入位置$i$的特征$y_i$时，考虑所有位置$j$的加权：</p>

\[y_i =  \sum_{j}^{} \frac{e^{f(x_i)^Tg(x_j)}}{\sum_j e^{f(x_i)^Tg(x_j)}} h(x_j)\]

<p><strong>SAGAN</strong>的输出为$γy + x$，其中$γ$是一个可学习的参数，并且初始化为$0$。网络开始训练时，首先学习局部信息，不采用自注意力模块；随着训练的进行，网络逐渐采用注意力模块学习更多长距离的特征。</p>

<p><img src="https://pic.imgdb.cn/item/639fc17fb1fccdcd36cb7e96.jpg" alt="" /></p>

<h3 id="-biggan">⚪ <a href="https://0809zheng.github.io/2022/05/27/biggan.html"><font color="blue">BigGAN</font></a></h3>

<p><strong>BigGAN</strong>整体结构与<strong>SAGAN</strong>相同，在此基础上采用了更大的数据批量、截断策略和模型稳定性的控制。<strong>BigGAN</strong>把<strong>batch size</strong>调整到<strong>SAGAN</strong>的$8$倍($2048$)；从先验分布$z$采样时，通过设置阈值的方式来截断采样，其中超出范围的采样值被丢弃并重新采样；通过约束权重矩阵的最大奇异值来提高训练稳定性。</p>

<h3 id="-self-modulation">⚪ <a href="https://0809zheng.github.io/2022/05/28/selfmod.html"><font color="Blue">Self-Modulation</font></a></h3>

<p>自调制模块用于增强训练过程中的稳定性，把生成网络中的<strong>BN</strong>替换为条件<strong>BN</strong>，其中仿射参数$\gamma,\beta$是由生成器的输入噪声$z$构造的。</p>

<p><img src="https://pic.imgdb.cn/item/63a1182ab1fccdcd36e4a76a.jpg" alt="" /></p>

<h3 id="-stylegan">⚪ <a href="https://0809zheng.github.io/2022/05/30/stylegan.html"><font color="Blue">StyleGAN</font></a></h3>

<p><strong>StyleGAN</strong>采用<strong>PGGAN</strong>的渐进式学习过程。通过映射网络对隐空间$z$进行特征解耦；通过<strong>AdaIN</strong>控制不同层级的视觉特征生成；调整常数输入降低生成图像异常；增加噪声输入补充图像细节；通过混合正则化实现视觉特征组合。</p>

<p><img src="https://pic.imgdb.cn/item/63a1589eb1fccdcd364cc3e7.jpg" alt="" /></p>

<h3 id="-stylegan2">⚪ <a href="https://0809zheng.github.io/2022/05/31/styleganv2.html"><font color="Blue">StyleGAN2</font></a></h3>

<p><strong>StyleGAN2</strong>在<strong>StyleGAN</strong>的基础上通过权重解调制消除了生成图像中的伪影；使用残差网络替换了渐进式学习结构，并增大了高分辨率层中的特征通道数。</p>

<p><img src="https://pic.imgdb.cn/item/63a17532b1fccdcd36833730.jpg" alt="" /></p>

<h3 id="-stylegan3">⚪ <a href="https://0809zheng.github.io/2022/06/15/stylegan3.html"><font color="Blue">StyleGAN3</font></a></h3>

<p><strong>StyleGAN3</strong>从<strong>StyleGAN2</strong>的结构出发，从频域角度改进网络结构以实现特征的平移与旋转等变性，消除了生成图像的高频混叠现象。</p>

<p><img src="https://pic.imgdb.cn/item/63a27fc9b1fccdcd36243fb3.jpg" alt="" /></p>

<h3 id="-transformer-gan-transgan">⚪ <a href="https://0809zheng.github.io/2021/03/02/transgan.html"><font color="Blue">Transformer GAN (TransGAN)</font></a></h3>

<p><strong>TransGAN</strong>使用<strong>Transformer</strong>构造了生成对抗网络。生成器使用分段式设计迭代地增加输入序列长度，进而提升图像分辨率。判别器接收输入图像划分的图像块序列，用于在语义上判断输入图像是否真实。</p>

<p><img src="https://img.imgdb.cn/item/602e295699aa8726f298aca3.jpg" alt="" /></p>

<h3 id="b-引入编码器">b. 引入编码器</h3>

<h3 id="-vae-gan">⚪ <a href="https://0809zheng.github.io/2022/02/17/vaegan.html"><font color="Blue">VAE-GAN</font></a></h3>

<p><strong>VAE-GAN</strong>是一种用<strong>GAN</strong>训练<strong>VAE</strong>（或用<strong>VAE</strong>训练<strong>GAN</strong>）的方法。该模型包括编码器、解码器（生成器）、判别器三部分。编码器把真实图像编码成正态分布$z$；解码器从$z$中采样生成重构图像；判别器区分真实图像和重构图像。</p>

<p><img src="https://pic1.imgdb.cn/item/634b621e16f2c2beb18db84c.jpg" alt="" /></p>

<h3 id="-bidirectional-gan-bigan">⚪ <a href="https://0809zheng.github.io/2022/02/18/bigan.html"><font color="Blue">Bidirectional GAN (BiGAN)</font></a></h3>

<p><strong>BiGAN</strong>既可以将隐空间的噪声分布映射到任意复杂的数据分布，又可以将数据映射回隐空间，以此学习有价值的特征表示。该模型包括编码器、解码器（生成器）、判别器三部分。编码器把真实图像$x$编码成$z$；生成器把$z$解码成重构图像；判别器区分图像$x$和编码$z$是编码器还是解码器提供的。</p>

<p><img src="https://pic1.imgdb.cn/item/634b704916f2c2beb1a1df6e.jpg" alt="" /></p>

<h3 id="-vqgan">⚪ <a href="https://0809zheng.github.io/2022/05/26/vqgan.html"><font color="Blue">VQGAN</font></a></h3>

<p><strong>VQGAN</strong>的生成器采用<strong>VQ-VAE</strong>模型，对图像块进行编码，学习隐空间中离散的编码表；判别器采用<strong>PatchGAN</strong>结构，用于提高生成图像的感知质量。</p>

<p><img src="https://pic.imgdb.cn/item/63a01369b1fccdcd36502ed9.jpg" alt="" /></p>

<h3 id="c-使用能量模型">c. 使用能量模型</h3>

<p>根据前面的讨论，能量模型角度下<strong>GAN</strong>的目标函数写作：</p>

\[\begin{aligned} D^* &amp;\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [ D(x)]-  \Bbb{E}_{x \text{~} P_G(x)}[D(x) ]  \\ G^* &amp;\leftarrow \mathop{ \min}_{G} -H(G(z))+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] \end{aligned}\]

<p>其中判别器$D(x)$用于近似能量函数$U(x)$，不同方法中能量函数的实现形式不同。</p>

<p>若直接优化上述目标，判别器倾向于对真实样本的能量$U(x)$~$D(x) \to -\infty$，对生成样本的能量$U(x)$~$D(x) \to +\infty$，从而导致训练不稳定。因此不同方法会对能量的上下限进行不同的约束。</p>

<p>此外，不同方法对生成图像的熵$H(G(z))$的实现形式也不同。</p>

<h3 id="-energy-based-gan-ebgan">⚪ <a href="https://0809zheng.github.io/2022/02/16/ebgan.html"><font color="Blue">Energy-based GAN (EBGAN)</font></a></h3>

<p>判别器采用自编码器形式，能量函数为自编码器的<strong>L2</strong>重构误差：</p>

\[U(x) = ||D(x)-x||_2 = ||Dec(Enc(x))-x||_2\]

<p>能量的最小值是$0$，最大值设置为$m$。生成图像的熵$H(G(z))$采用判别器的编码器提取的编码特征的余弦相似度衡量：</p>

\[\begin{aligned} D^* &amp;\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  U(x)]+  \Bbb{E}_{x \text{~} P_G(x)}[\max(0, m-U(x)) ] \\ G^* &amp;\leftarrow \mathop{ \min}_{G} \Bbb{E}_{(x_i,x_j) \text{~} P_G(x)}[(\frac{Enc(x_i)^TEnc(x_j)}{||Enc(x_i)|| \cdot ||Enc(x_j)||})^2]+\Bbb{E}_{x \text{~} P_G(x)}[U(x) ] \end{aligned}\]

<h3 id="-loss-sensitive-gan-lsgan">⚪ <a href="https://0809zheng.github.io/2022/02/25/lsgan.html"><font color="Blue">Loss-Sensitive GAN (LSGAN)</font></a></h3>

<p>能量函数为判别器$U(x)=D(x)$。</p>

<p>如果生成图像与真实图像差异较大，则把生成图像的能量调整得大一些。</p>

\[\begin{aligned} D^* \leftarrow \mathop{ \min}_{D} &amp;\Bbb{E}_{x \text{~} P_{data}(x)} [  D(x)] \\ &amp;+  \Bbb{E}_{(x,z) \text{~}(P_{data}(x),P_z(z))}[\max \{ 0, \Delta(x,G(z))+D(x)-D(G(z)) ] \\ G^* \leftarrow \mathop{ \min}_{G} &amp;\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] \end{aligned}\]

<h3 id="-boundary-equilibrium-gan-began">⚪ <a href="https://0809zheng.github.io/2022/02/27/began.html"><font color="Blue">Boundary Equilibrium GAN (BEGAN)</font></a></h3>

<p>判别器采用自编码器形式，能量函数为自编码器的<strong>L1</strong>重构误差：</p>

\[U(x) = ||D(x)-x||_1 = ||Dec(Enc(x))-x||_1\]

<p>当生成图像的能量小于$\gamma$倍真实图像的能量时，判别器才会考虑增大生成图像的能量：</p>

\[\begin{aligned} D^* &amp;\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  U(x)]-  k_t \Bbb{E}_{x \text{~} P_G(x)}[U(x) ] \\ k_{t+1} &amp;\leftarrow  k_t + \lambda (\gamma U(x)-U(G(z))) \\ G^* &amp;\leftarrow \mathop{ \min}_{G} \Bbb{E}_{x \text{~} P_G(x)}[U(x) ]  \end{aligned}\]

<h3 id="-margin-adaptation-gan-magan">⚪ <a href="https://0809zheng.github.io/2022/02/24/magan.html"><font color="Blue">Margin Adaptation GAN (MAGAN)</font></a></h3>

<p>判别器采用自编码器形式，能量函数为自编码器的<strong>L2</strong>重构误差：</p>

\[U(x) = ||D(x)-x||_2 = ||Dec(Enc(x))-x||_2\]

<p>能量的最小值是$0$，最大值设置为$m_t$；$m_t$随训练轮数增大而减小。</p>

\[\begin{aligned} D^* &amp;\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  U(x)]+  \Bbb{E}_{x \text{~} P_G(x)}[\max(0, m_t-U(x)) ] \\ G^* &amp;\leftarrow \mathop{ \min}_{G} \Bbb{E}_{x \text{~} P_G(x)}[U(x) ] \end{aligned}\]

<h3 id="-maximum-entropy-generator-meg">⚪ <a href="https://0809zheng.github.io/2022/02/23/meg.html"><font color="Blue">Maximum Entropy Generator (MEG)</font></a></h3>

<p>能量函数为判别器$U(x)=D(x)$。</p>

<p>对于判别器，真实样本应该落在能量函数的极小值点附近，因此引入以$0$为中心的梯度惩罚，形式上等价于<a href="https://0809zheng.github.io/2022/02/06/wgangp.html"><font color="Blue">WGAN-GP</font></a>的目标函数。</p>

<p>对于生成器，生成图像的熵$H(G(z))$用互信息的下界近似(引入一个编码器$E(x)$估计互信息)。</p>

\[\begin{aligned}  D^* &amp;\leftarrow  \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  D(x)]-  \Bbb{E}_{x \text{~} P_G(x)}[D(x) ] + \lambda \Bbb{E}_{x \text{~} P_{data}(x)} [ || \nabla_x D(x) ||^2 ] \\G^*,E^* &amp;\leftarrow \mathop{ \min}_{G,E} \Bbb{E}_{z \text{~} q(z)}[||z-E(G(z))||^2]+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] \end{aligned}\]

<h3 id="d-由粗到细的生成-coarse-to-fine">d. 由粗到细的生成 (Coarse-to-Fine)</h3>

<h3 id="-laplacian-pyramid-gan-lapgan">⚪ <a href="https://0809zheng.github.io/2022/03/22/lapgan.html"><font color="Blue">Laplacian Pyramid GAN (LAPGAN)</font></a></h3>

<p><strong>LAPGAN</strong>通过拉普拉斯金字塔生成图像。拉普拉斯金字塔存储不同尺寸的插值图像：先通过下采样操作构造低分辨率图像，然后上采样后与上一层图像作差。在生成图像时，从噪声样本出发，使用生成器构造低分辨率图像，对其进行上采样；然后将噪声和低分辨率图像通过条件生成器构造插值图像，相加后恢复为高分辨率图像。</p>

<p><img src="https://pic1.imgdb.cn/item/636e066816f2c2beb1ceb01d.jpg" alt="" /></p>

<h3 id="-stacked-gan-stackgan">⚪ <a href="https://0809zheng.github.io/2022/05/23/stackgan.html"><font color="Blue">Stacked GAN (StackGAN)</font></a></h3>

<p><strong>StackGAN</strong>解决了文本到图像生成分辨率不高的问题。<strong>StackGAN</strong>堆叠了两阶段的<strong>GAN</strong>，第一阶段根据给定的文本描述，生成低分辨率的图像；第二阶段根据生成的低分辨率图像以及原始文本描述，生成具有更多细节的高分辨率图像。</p>

<p><img src="https://pic.imgdb.cn/item/639edc7fb1fccdcd366c61dd.jpg" alt="" /></p>

<h3 id="-progressive-growing-gan-pggan">⚪ <a href="https://0809zheng.github.io/2022/05/21/pggan.html"><font color="Blue">Progressive Growing GAN (PGGAN)</font></a></h3>

<p><strong>PGGAN</strong>通过在训练时逐渐地增大生成图像的分辨率获得更高质量的生成图像。渐进式的学习过程是从低分辨率图像开始生成，通过向网络中添加新的层逐步增加生成图像的分辨率。该种方法主观上允许模型首先学习图像分布的整体结构特征(低分辨率)，然后逐步学习图像的细节部分(高分辨率)。</p>

<p><img src="https://pic.imgdb.cn/item/639e81bcb1fccdcd36c43499.jpg" alt="" /></p>

<h3 id="-single-natural-image-gan-singan">⚪ <a href="https://0809zheng.github.io/2022/05/22/singan.html"><font color="Blue">Single Natural Image GAN (SinGAN)</font></a></h3>

<p><strong>SinGAN</strong>通过使用单张图像来训练<strong>GAN</strong>，使用多个<strong>GAN</strong>结构分别学习了不同尺度下图像块的分布，并从低分辨率到高分辨率逐步生成真实图像。判别器采用<strong>PatchGAN</strong>结构，输出的每个元素对应输入图像的一个子区域，用来评估该子区域的真实性。生成器接收输入噪声和上一层的低分辨率输出图像，生成高分辨率图像。</p>

<p><img src="https://pic.imgdb.cn/item/639ed3feb1fccdcd365e408d.jpg" alt="" /></p>

<h2 id="3改进优化过程">（3）改进优化过程</h2>

<h3 id="-improved-techniques-for-training-gans">⚪ <a href="https://0809zheng.github.io/2022/02/02/improve.html"><font color="Blue">Improved Techniques for Training GANs</font></a></h3>

<p>本文提出了几点使得<strong>GAN</strong>训练更快收敛的方法：</p>
<ul>
  <li><strong>feature matching</strong>：检测生成器的输出是否与真实样本的预期统计值(如均值或中位数)相匹配。</li>
  <li><strong>minibatch discrimination</strong>：使得判别器了解一批训练样本中的数据点之间的近似程度，而不是独立地处理每个样本。</li>
  <li><strong>historical averaging</strong>：强迫生成器和判别器的参数接近过去训练过程中的历史平均参数。</li>
  <li><strong>label smoothing</strong>：设置判别器的标签为软标签(如$0.1$和$0.9$)，以此降低模型的脆弱性。</li>
  <li><strong>virtual batch normalization</strong>：在进行批归一化时使用一个固定批次(参考批次, 在训练开始时选定)的统计量进行归一化。</li>
</ul>

<h3 id="-towards-principled-methods-for-training-generative-adversarial-networks">⚪ <a href="https://0809zheng.github.io/2022/02/03/principle.html"><font color="Blue">Towards Principled Methods for Training Generative Adversarial Networks</font></a></h3>

<p>本文提出了几点解决低维流形中分布不匹配的方法：</p>
<ul>
  <li>增加噪声：通过在判别器的输入中增加连续噪声，可以人为地扩大真实分布\(P_{data}(x)\)与生成分布\(P_G(x)\)的范围，使得两个概率分布有更大的概率重叠。</li>
  <li>使用更好的分布相似度度量：<strong>GAN</strong>的损失函数(在一定条件下)衡量真实分布\(P_{data}(x)\)与生成分布\(P_G(x)\)之间的<strong>JS</strong>散度，这在两个分布不相交时没有意义；可以选择具有更平滑值空间的分布度量。</li>
</ul>

<h3 id="-wasserstein-gans-work-because-they-fail-to-approximate-the-wasserstein-distance">⚪ <a href="https://0809zheng.github.io/2022/02/13/fail.html"><font color="Blue">Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)</font></a></h3>

<p>本文提出效果比较好的<strong>WGAN</strong>在训练过程中并没有精确地近似<strong>Wasserstein</strong>距离；相反如果对<strong>Wasserstein</strong>距离做更好的近似，效果反而会变差。主要原因如下：</p>
<ul>
  <li>交替训练：<strong>WGAN</strong>的目标函数目标函数只有先精确完成\(\mathop{\max}_{D}\)，然后再进行\(\mathop{ \min}_{G}\)，才相当于优化两个分布的<strong>Wasserstein</strong>距离；在实际训练时采用交替优化，理论上不可能精确逼近分布度量。</li>
  <li>批量训练：<strong>WGAN</strong>在训练时采用批量训练的方法，导致目标为最小化训练集中两个批量之间的<strong>Wasserstein</strong>距离，该目标仍然大于一个批量与训练集平均样本之间的<strong>Wasserstein</strong>距离。</li>
  <li>成本函数：<strong>WGAN</strong>的成本函数一般选择欧氏距离\(\|x-y\|_2\)。欧氏距离在衡量两张图像的相似程度时在视觉效果上是不合理的；两张相似的图像对应的欧氏距离不一定小。</li>
</ul>

<h3 id="-two-time-scale-update-rule-ttur">⚪ <a href="https://0809zheng.github.io/2022/03/24/ttur.html"><font color="Blue">Two Time-Scale Update Rule (TTUR)</font></a></h3>

<p>在设置优化函数时，应设法保证判别器的判别能力比生成器的生成能力要好。通常的做法是先更新判别器的参数多次，再更新一次生成器的参数。</p>

<p><strong>TTUR</strong>是指判别器和生成器的更新次数相同，将判别器的学习率设置得比生成器的学习率更大，此时网络收敛于局部纳什均衡：</p>

\[\begin{aligned} θ_D &amp; \leftarrow θ_D + \alpha \nabla_{θ_D}L(D,G) \\ \theta_G &amp; \leftarrow θ_G - \beta \nabla_{θ_G}L(D,G) \end{aligned}\]

<h3 id="-dirac-gan">⚪ <a href="https://0809zheng.github.io/2022/03/23/diracgan.html"><font color="Blue">Dirac GAN</font></a></h3>

<p><strong>Dirac GAN</strong>把<strong>GAN</strong>的交替优化过程建模为一个由常微分方程组<strong>ODEs</strong>表示的动力系统：</p>

\[\begin{pmatrix} \dot{\theta}_D \\ \dot{\theta}_G  \end{pmatrix}= \begin{pmatrix} \nabla_{\theta_D} L(\theta_G,\theta_D) \\ -\nabla_{\theta_G} L(\theta_G,\theta_D) \end{pmatrix}\]

<p><strong>Dirac GAN</strong>的出发点是考虑真实样本分布只有一个样本点（记为零向量$0$）的情况。直接用向量$\theta_G$表示生成样本（也即生成器的参数），而(激活函数前的)判别器设置为线性模型$D(x)=x \cdot \theta_D$，$\theta_D$是判别器的参数。在该极简假设下，分析生成分布能否收敛到真实分布，即$\theta_G$能否最终收敛到$0$。</p>

<h3 id="-variational-discriminator-bottleneck-vdb">⚪ <a href="https://0809zheng.github.io/2020/09/27/vdb.html"><font color="Blue">Variational Discriminator Bottleneck (VDB)</font></a></h3>

<p>为防止判别器的表现能力远超生成器，<strong>变分判别瓶颈 (VDB)</strong>向判别器中引入信息瓶颈。把判别器进一步拆分成编码网络和判别网络，编码网络把$x$编码为一个隐变量$z$，判别网络把隐变量$z$预测为真假标签$y$。</p>

<p><img src="https://pic.imgdb.cn/item/6475cdf2f024cca17390ea9c.jpg" alt="" /></p>

<p><strong>VDB</strong>希望能尽可能地减少隐变量$z$包含的信息量，在实现时为损失函数引入互信息$I(x,z)$的变分上界。此时判别器的损失函数表示为：</p>

\[\begin{aligned}\mathop{ \min}_{D,E} \mathop{ \max}_{\beta \geq 0}  &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[ \Bbb{E}_{z \text{~} E(z|x)}[-\log D(z)]] + \Bbb{E}_{x \text{~} G(x)}[\Bbb{E}_{z \text{~} E(z|x)}[-\log(1-D(z))]] \\ &amp; + \beta \left( \mathbb{E}_{x \text{~} P_{data}(x)} \left[ KL\left[ E(z|x) \mid\mid q(z)\right] \right] - I_c \right) \end{aligned}\]

<h3 id="-orthogonal-gan-o-gan">⚪ <a href="https://0809zheng.github.io/2022/06/16/ogan.html"><font color="Blue">Orthogonal GAN (O-GAN)</font></a></h3>

<p><strong>O-GAN</strong>通过对判别器的正交分解操作，把判别器变成一个编码器，从而让<strong>GAN</strong>同时具备生成能力和编码能力。实现过程为把判别器$D$写成复合函数\(D(x) = T(E(x)) = \text{avg}(E(x))\)，并在损失函数中引入<strong>Pearson</strong>相关系数：</p>

\[\begin{aligned} \mathop{\max}_{E}&amp; \Bbb{E}_{x \text{~} P_{data}(x)}[\log \text{avg}(E(x))] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-\text{avg}(E(G(z))))] \\ &amp; + \lambda \Bbb{E}_{z \text{~} P_{Z}(z)}[\rho(z,E(G(z)))] \\ \mathop{ \max}_{G}&amp;  \Bbb{E}_{z \text{~} P_{Z}(z)}[\log \text{avg}(E(G(z)))] + \lambda \Bbb{E}_{z \text{~} P_{Z}(z)}[\rho(z,E(G(z)))] \end{aligned}\]

<h3 id="-cascading-rejection">⚪ <a href="https://0809zheng.github.io/2022/04/25/cascading.html"><font color="Blue">Cascading Rejection</font></a></h3>

<p>级联抑制(<strong>cascading rejection</strong>)方法是指在判别器中使用内积对特征向量$v$进行分类后，对特征向量$v$的垂直分量再做一次分类；并且再次分类也会导致一个新的垂直分量，从而实现迭代地分类：</p>

<p><img src="https://pic.imgdb.cn/item/639597a9b1fccdcd3666bc29.jpg" alt="" /></p>

<h3 id="-adaptive-discriminator-augmentation-ada">⚪ <a href="https://0809zheng.github.io/2022/06/11/ganaug.html"><font color="Blue">Adaptive Discriminator Augmentation (ADA)</font></a></h3>

<p>在小样本训练时，可以通过引入数据增强缓解判别器的过拟合现象。为了防止增强操作渗透到生成分布中，采用自适应判别器增强方法，即在训练的初始阶段设置增强概率$p=0$，当判别器出现过拟合现象时自适应地增大$p$。</p>

<p><img src="https://pic.imgdb.cn/item/63a2624eb1fccdcd36f507e7.jpg" alt="" /></p>

<h3 id="-hubness-prior">⚪ <a href="https://0809zheng.github.io/2022/03/31/hubness.html"><font color="Blue">Hubness Prior</font></a></h3>

<p><strong>枢纽度先验(Hubness Prior)</strong>是指在<strong>GAN</strong>的采样过程中，<strong>hub</strong>值越大的采样点对应的生成质量就越好。</p>

<p><strong>hub</strong>值统计每个样本点出现在其余点的$k$邻域的次数；<strong>hub</strong>值越大，则越接近样本的密度中心，则该样本不太可能是没有经过充分训练的离群点，因此采样质量相对更高。</p>

<p>从\(\mathcal{N}(0,1)\)中采样$N$个样本点后，计算每个样本点的<strong>hub</strong>值，只保留<strong>hub</strong>值超过阈值$t$的样本点用来生成新的样本。</p>

<h2 id="4其他应用">（4）其他应用</h2>

<h3 id="a-条件生成-conditional-generation">a. 条件生成 (Conditional Generation)</h3>

<h3 id="-conditional-gan-cgan">⚪ <a href="https://0809zheng.github.io/2022/03/02/cgan.html"><font color="Blue">Conditional GAN (CGAN)</font></a></h3>

<p><strong>CGAN</strong>的生成器接收随机噪声$z$和随机标签$c$，生成给定标签$c$时的图像$x=G(z,c)$；判别器接收图像$x$和对应的标签$c$，判断图像$x$是否为给定标签$c$时的真实图像$D(x|c)$。
目标函数如下：</p>

\[\begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x|c)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z,c)))] \end{aligned}\]

<h3 id="-information-maximizing-gan-infogan">⚪ <a href="https://0809zheng.github.io/2022/03/06/infogan.html"><font color="Blue">Information Maximizing GAN (InfoGAN)</font></a></h3>

<p><strong>InfoGAN</strong>的生成器接收随机噪声$z$和条件编码$c$，生成给定条件$c$时的图像$x=G(z,c)$；判别器$D(x)$接收图像$x$，判断图像$x$是否为真实图像。与判别器共享参数的编码器预测生成图像$x$对应的条件编码$\hat{c}=E(G(c))$。损失函数额外引入条件编码的重构误差（实际上是条件编码$c$和生成图像$G(z,c)$的互信息$I(c;G(z,c))$的一个下界）：</p>

\[\begin{aligned} \mathop{ \min}_{G,E} \mathop{\max}_{D} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z,c)))] \\ &amp; +\lambda \Bbb{E}_{z \text{~} P_{Z}(z)}[||c-E(G(z,c))||^2]  \end{aligned}\]

<h3 id="-auxiliary-classifier-gan-acgan">⚪ <a href="https://0809zheng.github.io/2022/03/03/acgan.html"><font color="Blue">Auxiliary Classifier GAN (ACGAN)</font></a></h3>

<p><strong>ACGAN</strong>的生成器接收随机噪声$z$和随机标签$c$，生成给定标签$c$时的图像$G(z,c)$；判别器$D(x)$接收图像$x$，判断图像$x$是否为真实图像(二分类)以及是否属于对应的标签$c$(多分类)。目标函数如下：</p>

\[\begin{aligned}  \mathop{\max}_{D} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z,c)))] \\ &amp; +  \Bbb{E}_{c,x \text{~} P_{data}(x)}[\log D_c(x)]+  \Bbb{E}_{z \text{~} P_{Z}(z)}[\log D_c(G(z,c))] \\ \mathop{ \min}_{G} &amp; \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(D(G(z,c))] -  \Bbb{E}_{z \text{~} P_{Z}(z)}[\log D_c(G(z,c))] \end{aligned}\]

<h3 id="-projection-discriminator">⚪ <a href="https://0809zheng.github.io/2022/05/24/cgan.html"><font color="Blue">Projection Discriminator</font></a></h3>

<p>投影判别器是一种为条件生成设计的判别器结构，输入数据首先经过网络$\phi$提取特征，然后把特征分成两路：一路与编码后的类别标签$y$做点乘；另一路通过网络$\psi$映射成向量。最后两路相加作为判别器最终的输出。</p>

\[D(x,y=c) = v_c^T\phi(x) + \psi(\phi(x))\]

<p><img src="https://pic.imgdb.cn/item/639f0eacb1fccdcd36d4b369.jpg" alt="" /></p>

<h3 id="b-图像到图像翻译-image-to-image-translation">b. <a href="https://0809zheng.github.io/2020/05/23/image_translation.html"><font color="blue">图像到图像翻译 (Image-to-Image Translation)</font></a></h3>

<p><strong>图像到图像翻译(Image-to-Image Translation)</strong>旨在学习一个映射使得图像可以从源图像域(<strong>source domain</strong>)变换到目标图像域(<strong>target domain</strong>)，同时保留图像内容(<strong>context</strong>)。</p>

<p>根据是否提供了一对一的学习样本对，将图像到图像翻译任务划分为<strong>有配对数据(paired data)</strong>和<strong>无配对数据(unpaired data)</strong>两种情况。</p>
<ul>
  <li>有配对数据(监督图像翻译)是指在训练数据集中具有一对一的数据对；即给定联合分布$p(X,Y)$，学习条件映射$f_{x \to y}=p(Y|X)$和$f_{y \to x}=p(X|Y)$。代表方法有<strong>Pix2Pix</strong>, <strong>BicycleGAN</strong>, <strong>LPTN</strong>。</li>
  <li>无配对数据(无监督图像翻译)是指模型在多个独立的数据集之间训练，能够从多个数据集合中自动地发现集合之间的关联，从而学习出映射函数；即给定边缘分布$p(X)$和$p(Y)$，学习条件映射$f_{x \to y}=p(Y|X)$和$f_{y \to x}=p(X|Y)$。代表方法有<strong>CoGAN</strong>, <strong>PixelDA</strong>, <strong>CycleGAN</strong>, <strong>DiscoGAN</strong>, <strong>DualGAN</strong>, <strong>UNIT</strong>, <strong>MUNIT</strong>, <strong>TUNIT</strong>, <strong>StarGAN</strong>, <strong>StarGAN v2</strong>, <strong>GANILLA</strong>, <strong>NICE-GAN</strong>。</li>
</ul>

<h3 id="c-超分辨率-super-resolution">c. 超分辨率 (Super Resolution)</h3>

<h3 id="-super-resolution-gan-srgan">⚪ <a href="https://0809zheng.github.io/2020/08/10/srresnet.html"><font color="blue">Super Resolution GAN (SRGAN)</font></a></h3>

<p><strong>SRGAN</strong>通过生成对抗网络进行图像超分辨率任务。除对抗损失外，还利用<strong>VGGNet</strong>的网络特征构造内容损失函数(<strong>content loss</strong>)。</p>

<p><img src="https://pic.imgdb.cn/item/639c1cabb1fccdcd36254806.jpg" alt="" /></p>

<h3 id="-enhanced-super-resolution-gan-esrgan">⚪ <a href="https://0809zheng.github.io/2020/08/12/esrgan.html"><font color="blue">Enhanced Super Resolution GAN (ESRGAN)</font></a></h3>

<p><strong>ESRGAN</strong>在<strong>SRGAN</strong>的基础上进行改进。对于网络结构，引入没有批量归一化的残差密集块作为基本单元；对于对抗损失，采用相对平均判别器的思想，让判别器预测相对真实概率；对于感知损失，采用激活前的特征来构造损失。</p>

<p><img src="https://pic.imgdb.cn/item/639c2bc7b1fccdcd363f56b2.jpg" alt="" /></p>

<h3 id="d-图像修补-image-completion--inpainting">d. 图像修补 (Image Completion / Inpainting)</h3>

<h3 id="-context-encoder">⚪ <a href="https://0809zheng.github.io/2022/05/19/context.html"><font color="Blue">Context Encoder</font></a></h3>

<p><strong>上下文编码器</strong>能够根据周围像素生成任意图像区域，采用一种编码器-解码器结构，编码器接收<strong>masked</strong>输入图像，提取图像特征；解码器将图像特征解码为缺失的图像区域。</p>

<p><img src="https://pic.imgdb.cn/item/639c5719b1fccdcd36888b84.jpg" alt="" /></p>

<h3 id="-context-conditional-gan-ccgan">⚪ <a href="https://0809zheng.github.io/2022/05/20/ccgan.html"><font color="Blue">Context-Conditional GAN (CCGAN)</font></a></h3>

<p><strong>上下文条件GAN</strong>结合了<strong>Context Encoder</strong>和<strong>ACGAN</strong>。生成器采用一种自编码器结构，接收<strong>masked</strong>输入图像$x$，生成图像修补的结果；判别器接收图像$x$和图像标签$y$，判断图像$x$是否为真实图像(二分类)以及是否属于对应的标签$y$(多分类)。</p>

<p><img src="https://pic.imgdb.cn/item/639c7040b1fccdcd36b51d42.jpg" alt="" /></p>

<h3 id="-spatially-adaptive-denormalization-spade">⚪ <a href="https://0809zheng.github.io/2022/05/18/gaugan.html"><font color="Blue">Spatially-Adaptive Denormalization (SPADE)</font></a></h3>

<p><strong>SPADE</strong>能够将语义分割<strong>mask</strong>图像转换为真实图像，它通过空间自适应地学习和转换输入语义<strong>mask</strong>图像的信息。<strong>SPADE</strong>生成器将随机噪声作为输入，通过带有条件归一化<strong>SPADE</strong>层的残差块生成图像。由于每个残差块包含上采样层，因此对语义<strong>mask</strong>进行下采样以匹配空间分辨率。通过多尺度判别器构造对抗损失。</p>

<p><img src="https://pic.imgdb.cn/item/639a8d6cb1fccdcd36d69f14.jpg" alt="" /></p>

<h3 id="e-机器学习应用">e. 机器学习应用</h3>

<h3 id="-semi-supervised-gan">⚪ <a href="https://0809zheng.github.io/2022/06/14/ssgan.html"><font color="Blue">Semi-Supervised GAN</font></a></h3>

<p><strong>Semi-Supervised GAN</strong>使用<strong>GAN</strong>进行半监督学习。将原有的监督学习任务融合到<strong>GAN</strong>的判别器中，判别器同时实现数据真伪的判断和数据的分类；由生成器生成数据的标签是未知的，在原有类别的基础上多加一类作为生成数据的类别标签。</p>

\[\begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D} &amp; \Bbb{E}_{x \text{~} P_{data}(x)}[\log D_r(x) + D_c(\hat{y}=y |x)] \\ &amp; + \Bbb{E}_{z \text{~} P(z)} [\log(1-D_r(G(z)))+D_c(\hat{y}=y' |G(z))]  \end{aligned}\]

<h3 id="-anomaly-detection-gan-anogan">⚪ <a href="https://0809zheng.github.io/2020/10/23/anogan.html"><font color="Blue">Anomaly Detection GAN (AnoGAN)</font></a></h3>

<p><strong>AnoGAN</strong>使用<strong>GAN</strong>进行图像异常检测。首先使用预处理后的正常图像训练一个常规的<strong>GAN</strong>，使其能够从隐空间中采样得到正常图像；在检测新图像时，先在隐空间中找到与该新图像匹配度最高的隐变量，将其通过<strong>GAN</strong>的生成器获得生成图像，通过对比新图像与其对应的生成图像之间的差异判断其是否为异常图像。</p>

<p><img src="https://pic.downk.cc/item/5f9278531cd1bbb86bef45d0.jpg" alt="" /></p>

<h3 id="-clustering-gan-clustergan">⚪ <a href="https://0809zheng.github.io/2022/06/13/clustergan.html"><font color="blue">Clustering GAN (ClusterGAN)</font></a></h3>

<p><strong>ClusterGAN</strong>通过从一个<strong>one-hot</strong>编码变量和连续隐变量的混合分布中对隐变量进行采样，实现在隐空间的聚类。网络由生成器、判别器和编码器构成。生成器$G$从一个离散分布$z_c$和连续分布$z_n$共同组成的分布中采样生成图像$x_g$；判别器$D$用于区分生成图像$x_g$和真实图像$x_r$；编码器把生成图像$x_g$编码为重构的离散编码$\hat{z}_c$和连续编码$\hat{z}_n$。</p>

<p><img src="https://pic.imgdb.cn/item/63a2b3d6b1fccdcd367e5301.jpg" alt="" /></p>

<h1 id="-参考文献">⚪ 参考文献</h1>

<ul>
  <li><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a>：(arXiv1406)GAN的原始论文。</li>
  <li><a href="https://lilianweng.github.io/posts/2017-08-20-gan/">From GAN to WGAN</a>：Blog by Lilian Weng.</li>
  <li><a href="https://spaces.ac.cn/archives/4439">互怼的艺术：从零直达WGAN-GP</a>：Blog by 苏剑林.</li>
  <li><a href="https://github.com/AntixK/PyTorch-VAE">The GAN Zoo</a>：(github)A list of all named GANs!</li>
  <li><a href="https://github.com/eriklindernoren/PyTorch-GAN">PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks</a>：(github)GAN的PyTorch实现。</li>
  <li><a href="https://0809zheng.github.io/2022/03/02/cgan.html"><font color="Blue">Conditional Generative Adversarial Nets</font></a>：(arXiv1411)CGAN：条件生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/22/lapgan.html"><font color="Blue">Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks</font></a>：(arXiv1506)LAPGAN：使用拉普拉斯金字塔对抗网络生成高分辨率图像。</li>
  <li><a href="https://0809zheng.github.io/2022/02/05/dcgan.html"><font color="Blue">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</font></a>：(arXiv1511)DCGAN：使用深度卷积神经网络构造GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/02/17/vaegan.html"><font color="Blue">Autoencoding beyond pixels using a learned similarity metric</font></a>：(arXiv1512)VAE-GAN：结合VAE和GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/05/19/context.html"><font color="Blue">Context Encoders: Feature Learning by Inpainting</font></a>：(arXiv1604)上下文编码器：通过修补进行特征学习。</li>
  <li><a href="https://0809zheng.github.io/2022/02/18/bigan.html"><font color="Blue">Adversarial Feature Learning</font></a>：(arXiv1605)BiGAN：使用双向GAN进行对抗特征学习。</li>
  <li><a href="https://0809zheng.github.io/2022/02/02/improve.html"><font color="Blue">Improved Techniques for Training GANs</font></a>：(arXiv1606)训练生成对抗网络的改进技巧。</li>
  <li><a href="https://0809zheng.github.io/2022/02/07/fgan.html"><font color="Blue">f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</font></a>：(arXiv1606)fGAN：通过f散度构造GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/03/06/infogan.html"><font color="Blue">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</font></a>：(arXiv1606)InfoGAN：通过最大化互信息实现可插值的表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/03/08/cogan.html"><font color="Blue">Coupled Generative Adversarial Networks</font></a>：(arXiv1606)CoGAN：耦合生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/06/14/ssgan.html"><font color="Blue">Semi-Supervised Learning with Generative Adversarial Networks</font></a>：(arXiv1606)通过生成对抗网络进行半监督学习。</li>
  <li><a href="https://0809zheng.github.io/2022/02/16/ebgan.html"><font color="Blue">Energy-based Generative Adversarial Network</font></a>：(arXiv1609)EBGAN：基于能量的生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/03/acgan.html"><font color="Blue">Conditional Image Synthesis With Auxiliary Classifier GANs</font></a>：(arXiv1610)ACGAN：基于辅助分类器GAN的条件图像合成。</li>
  <li><a href="https://0809zheng.github.io/2022/02/15/lsgan.html"><font color="Blue">Least Squares Generative Adversarial Networks</font></a>：(arXiv1611)LSGAN：使用均方误差构造目标函数。</li>
  <li><a href="https://0809zheng.github.io/2022/03/10/p2p.html"><font color="Blue">Image-to-Image Translation with Conditional Adversarial Networks</font></a>：(arXiv1611)Pix2Pix：通过UNet和PatchGAN实现图像转换。</li>
  <li><a href="https://0809zheng.github.io/2022/05/20/ccgan.html"><font color="Blue">Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks</font></a>：(arXiv1611)通过上下文条件生成对抗网络实现半监督学习。</li>
  <li><a href="https://0809zheng.github.io/2022/03/15/pixelda.html"><font color="Blue">Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</font></a>：(arXiv1612)PixelDA：通过GAN实现像素级领域自适应。</li>
  <li><a href="https://0809zheng.github.io/2022/05/23/stackgan.html"><font color="Blue">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</font></a>：(arXiv1612)StackGAN: 通过堆叠生成对抗网络进行文本图像合成</li>
  <li><a href="https://0809zheng.github.io/2022/02/03/principle.html"><font color="Blue">Towards Principled Methods for Training Generative Adversarial Networks</font></a>：(arXiv1701)训练生成对抗网络的原则性方法。</li>
  <li><a href="https://0809zheng.github.io/2022/02/25/lsgan.html"><font color="Blue">Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities</font></a>：(arXiv1701)LSGAN：损失敏感GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/02/27/began.html"><font color="Blue">BEGAN: Boundary Equilibrium Generative Adversarial Networks</font></a>：(arXiv1703)BEGAN：边界平衡GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/02/04/wgan.html"><font color="Blue">Wasserstein GAN</font></a>：(arXiv1701)WGAN：使用Wasserstein距离构造GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/03/26/mcgan.html"><font color="Blue">McGan: Mean and Covariance Feature Matching GAN</font></a>：(arXiv1702)McGAN：均值和协方差特征匹配GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/02/28/bgan.html"><font color="Blue">Boundary-Seeking Generative Adversarial Networks</font></a>：(arXiv1702)BGAN：边界搜索GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/02/14/cyclegan.html"><font color="Blue">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</font></a>：(arXiv1703)CycleGAN：使用循环一致损失实现无配对数据的图像转换。</li>
  <li><a href="https://0809zheng.github.io/2022/03/16/discogan.html"><font color="Blue">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</font></a>：(arXiv1703)DiscoGAN：使用GAN学习发现跨领域关系。</li>
  <li><a href="https://0809zheng.github.io/2022/03/21/unit.html"><font color="Blue">Unsupervised Image-to-Image Translation Networks</font></a>：(arXiv1703)UNIT：无监督图像到图像翻译网络。</li>
  <li><a href="https://0809zheng.github.io/2020/10/23/anogan.html"><font color="Blue">Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</font></a>：(arXiv1703)AnoGAN：使用生成对抗网络进行异常检测。</li>
  <li><a href="https://0809zheng.github.io/2022/03/17/dualgan.html"><font color="Blue">DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</font></a>：(arXiv1704)DualGAN：图像转换的无监督对偶学习。</li>
  <li><a href="https://0809zheng.github.io/2022/02/06/wgangp.html"><font color="Blue">Improved Training of Wasserstein GANs</font></a>：(arXiv1704)WGAN-GP：在WGAN中引入梯度惩罚。</li>
  <li><a href="https://0809zheng.github.io/2022/02/24/magan.html"><font color="Blue">MAGAN: Margin Adaptation for Generative Adversarial Networks</font></a>：(arXiv1704)MAGAN：自适应调整EBGAN的能量边界。</li>
  <li><a href="https://0809zheng.github.io/2022/05/15/softmaxgan.html"><font color="Blue">Softmax GAN</font></a>：(arXiv1704)把生成对抗网络建模为Softmax函数。</li>
  <li><a href="https://0809zheng.github.io/2022/05/14/dragan.html"><font color="Blue">On Convergence and Stability of GANs</font></a>：(arXiv1705)DRAGAN：调整梯度惩罚的插值空间。</li>
  <li><a href="https://0809zheng.github.io/2022/03/28/mmdgan.html"><font color="Blue">MMD GAN: Towards Deeper Understanding of Moment Matching Network</font></a>：(arXiv1705)MMD GAN：最大平均差异生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/29/fisher.html"><font color="Blue">Fisher GAN</font></a>：(arXiv1705)Fisher GAN：使用Fisher差异构造生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/03/24/ttur.html"><font color="Blue">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</font></a>：(arXiv1706)GAN的TTUR训练方法和FID评估指标。</li>
  <li><a href="https://0809zheng.github.io/2022/05/21/pggan.html"><font color="Blue">Progressive Growing of GANs for Improved Quality, Stability, and Variation</font></a>：(arXiv1710)PGGAN: 渐进生成高质量、多样性的图像。</li>
  <li><a href="https://0809zheng.github.io/2022/03/18/bicyclegan.html"><font color="Blue">Toward Multimodal Image-to-Image Translation</font></a>：(arXiv1711)BicycleGAN：多模态图像翻译。</li>
  <li><a href="https://0809zheng.github.io/2022/03/19/stargan.html"><font color="Blue">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</font></a>：(arXiv1711)StarGAN：统一的多领域图像翻译框架。</li>
  <li><a href="https://0809zheng.github.io/2022/02/09/wgandiv.html"><font color="Blue">Wasserstein Divergence for GANs</font></a>：(arXiv1712)WGAN-div：通过Wasserstein散度构造GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/03/25/is.html"><font color="Blue">A Note on the Inception Score</font></a>：(arXiv1801)GAN的Inception Score评估指标。</li>
  <li><a href="https://0809zheng.github.io/2022/03/30/kid.html"><font color="Blue">Demystifying MMD GANs</font></a>：(arXiv1801)GAN的KID评估指标。</li>
  <li><a href="https://0809zheng.github.io/2022/03/23/diracgan.html"><font color="Blue">Which Training Methods for GANs do actually Converge?</font></a>：(arXiv1801)使用Dirac GAN分析GAN的收敛性态。</li>
  <li><a href="https://0809zheng.github.io/2022/02/08/sngan.html"><font color="Blue">Spectral Normalization for Generative Adversarial Networks</font></a>：(arXiv1802)SN-GAN：在WGAN中引入谱归一化。</li>
  <li><a href="https://0809zheng.github.io/2022/05/24/cgan.html"><font color="Blue">cGANs with Projection Discriminator</font></a>：(arXiv1802)通过投影判别器构造条件生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/04/26/munit.html"><font color="Blue">Multimodal Unsupervised Image-to-Image Translation</font></a>：(arXiv1804)MUNIT：多模态无监督图像到图像翻译网络。</li>
  <li><a href="https://0809zheng.github.io/2022/05/25/sagan.html"><font color="Blue">Self-Attention Generative Adversarial Networks</font></a>：(arXiv1805)SAGAN：自注意力生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/02/21/rgan.html"><font color="Blue">The relativistic discriminator: a key element missing from standard GAN</font></a>：(arXiv1807)RGAN：GAN中的相对判别器。</li>
  <li><a href="https://0809zheng.github.io/2022/06/13/clustergan.html"><font color="blue">ClusterGAN : Latent Space Clustering in Generative Adversarial Networks</font></a>：(arXiv1809)ClusterGAN：生成对抗网络的隐空间聚类。</li>
  <li><a href="https://0809zheng.github.io/2020/08/12/esrgan.html"><font color="blue">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</font></a>：(arXiv1809)ESRGAN：增强的图像超分辨率生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/05/27/biggan.html"><font color="blue">Large Scale GAN Training for High Fidelity Natural Image Synthesis</font></a>：(arXiv1809)BigGAN：用于高保真度自然图像合成的大规模GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/05/28/selfmod.html"><font color="Blue">On Self Modulation for Generative Adversarial Networks</font></a>：(arXiv1810)生成对抗网络的自调制。</li>
  <li><a href="https://0809zheng.github.io/2020/09/27/vdb.html"><font color="Blue">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</font></a>：(arXiv1810)变分判别瓶颈：通过约束信息流改进深度学习模型。</li>
  <li><a href="https://0809zheng.github.io/2022/02/22/ganqp.html"><font color="Blue">GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint</font></a>：(arXiv1811)GAN-QP：在对偶空间定义没有梯度消失且满足Lipschitz约束的目标。</li>
  <li><a href="https://0809zheng.github.io/2022/05/30/stylegan.html"><font color="Blue">A Style-Based Generator Architecture for Generative Adversarial Networks</font></a>：(arXiv1812)StyleGAN：一种基于风格的生成器结构。</li>
  <li><a href="https://0809zheng.github.io/2022/02/23/meg.html"><font color="Blue">Maximum Entropy Generators for Energy-Based Models</font></a>：(arXiv1901)MEG：基于能量模型的最大熵生成器。</li>
  <li><a href="https://0809zheng.github.io/2022/05/18/gaugan.html"><font color="Blue">Semantic Image Synthesis with Spatially-Adaptive Normalization</font></a>：(arXiv1901)通过空间自适应归一化进行语义图像合成。</li>
  <li><a href="https://0809zheng.github.io/2022/06/16/ogan.html"><font color="Blue">O-GAN: Extremely Concise Approach for Auto-Encoding Generative Adversarial Networks</font></a>：(arXiv1903)O-GAN：把GAN的判别器修改为编码器。</li>
  <li><a href="https://0809zheng.github.io/2022/05/22/singan.html"><font color="Blue">SinGAN: Learning a Generative Model from a Single Natural Image</font></a>：(arXiv1905)SinGAN: 通过单张自然图像训练生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/02/12/ctrans.html"><font color="Blue">How Well Do WGANs Estimate the Wasserstein Metric?</font></a>：(arXiv1910)讨论WGAN与Wasserstein距离的近似程度。</li>
  <li><a href="https://0809zheng.github.io/2022/04/25/cascading.html"><font color="Blue">Simple yet Effective Way for Improving the Performance of GAN</font></a>：(arXiv1911)通过级联抑制方法增强GAN的判别器。</li>
  <li><a href="https://0809zheng.github.io/2022/05/31/styleganv2.html"><font color="Blue">Analyzing and Improving the Image Quality of StyleGAN</font></a>：(arXiv1912)StyleGAN2：分析和改进StyleGAN的图像生成质量。</li>
  <li><a href="https://0809zheng.github.io/2022/03/07/design.html"><font color="Blue">Designing GANs: A Likelihood Ratio Approach</font></a>：(arXiv2002)Designing GANs：在对偶空间设计生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/04/29/ganilla.html"><font color="Blue">GANILLA: Generative Adversarial Networks for Image to Illustration Translation</font></a>：(arXiv2002)GANILLA：把图像转换为儿童绘本风格。</li>
  <li><a href="https://0809zheng.github.io/2022/05/17/nicegan.html"><font color="Blue">Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation</font></a>：(arXiv2003)NICE-GAN: 把判别器重用为编码器的图像翻译模型。</li>
  <li><a href="https://0809zheng.github.io/2022/04/28/tunit.html"><font color="Blue">Rethinking the Truly Unsupervised Image-to-Image Translation</font></a>：(arXiv2006)TUNIT：完全无监督图像到图像翻译。</li>
  <li><a href="https://0809zheng.github.io/2022/06/11/ganaug.html"><font color="Blue">Training Generative Adversarial Networks with Limited Data</font></a>：(arXiv2006)使用有限数据训练生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/05/26/vqgan.html"><font color="Blue">Taming Transformers for High-Resolution Image Synthesis</font></a>：(arXiv2012)通过VQGAN和Transformer实现高分辨率图像合成。</li>
  <li><a href="https://0809zheng.github.io/2021/03/02/transgan.html"><font color="Blue">TransGAN: Two Transformers Can Make One Strong GAN</font></a>：(arXiv2102)TransGAN：用Transformer实现GAN。</li>
  <li><a href="https://0809zheng.github.io/2022/02/13/fail.html"><font color="Blue">Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)</font></a>：(arXiv2103)WGAN的表现与Wasserstein距离的近似程度没有必然联系。</li>
  <li><a href="https://0809zheng.github.io/2022/04/27/lptn.html"><font color="Blue">High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network</font></a>：(arXiv2105)LPTN：高分辨率真实感实时图像翻译。</li>
  <li><a href="https://0809zheng.github.io/2022/06/15/stylegan3.html"><font color="Blue">Alias-Free Generative Adversarial Networks</font></a>：(arXiv2106)StyleGAN3：无混叠生成对抗网络。</li>
  <li><a href="https://0809zheng.github.io/2022/02/10/gngan.html"><font color="Blue">Gradient Normalization for Generative Adversarial Networks</font></a>：(arXiv2109)GN-GAN：在WGAN中引入梯度归一化。</li>
  <li><a href="https://0809zheng.github.io/2022/02/11/grangan.html"><font color="Blue">GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks</font></a>：(arXiv2111)GraN-GAN：在WGAN中引入分段线性的梯度归一化。</li>
  <li><a href="https://0809zheng.github.io/2022/03/31/hubness.html"><font color="Blue">Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling</font></a>：(arXiv2206)高质量GAN采样的枢纽度先验。</li>
</ul>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2022/02/02/improve.html" class="read-next-link"></a>
        <section>
          <span>Improved Techniques for Training GANs</span>
          <p>  训练生成对抗网络的改进技巧.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/6330f97716f2c2beb140962b.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2022/01/08/mathai.html" class="read-next-link"></a>
          <section>
            <span>Advancing mathematics by guiding human intuition with AI</span>
            <p>  用人工智能引导人类直觉推进数学发展.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/61d9545f2ab3f51d91f808ac.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
