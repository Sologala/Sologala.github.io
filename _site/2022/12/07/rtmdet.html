<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RTMDet: An Empirical Study of Designing Real-Time Object Detectors - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="RTMDet: An Empirical Study of Designing Real-Time Object Detectors">
    <meta name="keywords"  content="论文阅读">
    <!-- Open Graph -->
    <meta property="og:title" content="RTMDet: An Empirical Study of Designing Real-Time Object Detectors - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2022/12/07/rtmdet.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB" class="post-tag">论文阅读</a>
          
        
      </div>
      <h1>RTMDet: An Empirical Study of Designing Real-Time Object Detectors</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="22-12-07"><i class="iconfont icon-date"></i>07 Dec 2022</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/652a4294c458853aefe7fc6c.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>RTMDet：设计实时目标检测器的经验性研究.</p>
</blockquote>

<ul>
  <li>paper：<a href="https://arxiv.org/abs/2212.07784">RTMDet: An Empirical Study of Designing Real-Time Object Detectors</a></li>
</ul>

<p>最近一段时间，开源界涌现出了大量的高精度目标检测项目，其中最突出的就是 <strong>YOLO</strong> 系列。在调研了当前 <strong>YOLO</strong> 系列的诸多改进模型后，<strong>MMDetection</strong> 核心开发者针对这些设计以及训练方式进行了经验性的总结，并进行了优化，推出了高精度、低延时的单阶段目标检测器 <strong>RTMDet (Real-time Models for Object Detection)</strong>。</p>

<p><strong>RTMDet</strong> 由 <strong>tiny/s/m/l/x</strong> 一系列不同大小的模型组成，为不同的应用场景提供了不同的选择。 其中 <strong>RTMDet-x</strong> 在 <strong>52.6 mAP</strong> 的精度下达到了 <strong>300+ FPS</strong> 的推理速度。而最轻量的模型 <strong>RTMDet-tiny</strong>，在仅有 <strong>4M</strong> 参数量的情况下也能够达到 4<strong>0.9 mAP</strong>，且推理速度 $&lt; 1$ <strong>ms</strong>。</p>

<p><img src="https://pic.imgdb.cn/item/652a4374c458853aefeb2af9.jpg" alt="" /></p>

<h1 id="1-数据增强">1. 数据增强</h1>

<p><strong>RTMDet</strong> 采用了多种数据增强的方式来增加模型的性能，主要包括单图数据增强:</p>
<ul>
  <li><strong>RandomResize</strong> 随机尺度变换</li>
  <li><strong>RandomCrop</strong> 随机裁剪</li>
  <li><strong>HSVRandomAug</strong> 颜色空间增强</li>
  <li><strong>RandomFlip</strong> 随机水平翻转</li>
</ul>

<p>以及混合类数据增强：</p>
<ul>
  <li><strong>Mosaic</strong> 马赛克</li>
  <li><strong>MixUp</strong> 图像混合</li>
</ul>

<p><img src="https://pic.imgdb.cn/item/652a49bcc458853aef008c8e.jpg" alt="" /></p>

<p>其中 <strong>RandomResize</strong> 超参在大模型 <strong>M,L,X</strong> 和小模型 <strong>S, Tiny</strong> 上是不一样的，大模型由于参数较多，可以使用 <strong>large scale jitter</strong> 策略即参数为 $(0.1,2.0)$，而小模型采用 <strong>stand scale jitter</strong> 策略即 $(0.5, 2.0)$ 策略。</p>

<p>与 <strong>YOLOv5</strong> 不同的是，<strong>YOLOv5</strong> 认为在 <strong>S</strong> 和 <strong>Nano</strong> 模型上使用 <strong>MixUp</strong> 是过剩的，小模型不需要这么强的数据增强。而 <strong>RTMDet</strong> 在 <strong>S</strong> 和 <strong>Tiny</strong> 上也使用了 <strong>MixUp</strong>，这是因为 <strong>RTMDet</strong> 在最后 <strong>20 epoch</strong> 会切换为正常的 <strong>aug</strong>， 并通过训练证明这个操作是有效的。 并且 <strong>RTMDet</strong> 为混合类数据增强引入了 <strong>Cache</strong> 方案，有效地减少了图像处理的时间, 和引入了可调超参 <strong>max_cached_images</strong> ，当使用较小的 <strong>cache</strong> 时，其效果类似 <strong>repeated augmentation</strong>。</p>

<h3 id="1-为图像混合数据增强引入-cache">(1) 为图像混合数据增强引入 Cache</h3>

<p><strong>Mosaic&amp;MixUp</strong> 涉及到多张图片的混合，它们的耗时会是普通数据增强的 $K$ 倍($K$ 为混入图片的数量)。 如在 <strong>YOLOv5</strong> 中，每次做 <strong>Mosaic</strong> 时， <strong>4</strong> 张图片的信息都需要从硬盘中重新加载。 而 <strong>RTMDet</strong> 只需要重新载入当前的一张图片，其余参与混合增强的图片则从缓存队列中获取，通过牺牲一定内存空间的方式大幅提升了效率。 另外通过调整 <strong>cache</strong> 的大小以及 <strong>pop</strong> 的方式，也可以调整增强的强度。</p>

<p><img src="https://pic.imgdb.cn/item/652a4b33c458853aef05a0b5.jpg" alt="" /></p>

<p><strong>cache</strong> 队列中预先储存了 <strong>N</strong> 张已加载的图像与标签数据，每一个训练 <strong>step</strong> 中只需加载一张新的图片及其标签数据并更新到 <strong>cache</strong> 队列中(<strong>cache</strong> 队列中的图像可重复，如图中出现两次 <strong>img3</strong>)，同时如果 <strong>cache</strong> 队列长度超过预设长度，则随机 <strong>pop</strong> 一张图（为了 <strong>Tiny</strong> 模型训练更稳定，在 <strong>Tiny</strong> 模型中不采用随机 <strong>pop</strong> 的方式, 而是移除最先加入的图片），当需要进行混合数据增强时，只需要从 <strong>cache</strong> 中随机选择需要的图像进行拼接等处理，而不需要全部从硬盘中加载，节省了图像加载的时间。</p>

<p><strong>cache</strong> 队列的最大长度 <strong>N</strong> 为可调整参数，根据经验性的原则，当为每一张需要混合的图片提供十个缓存时，可以认为提供了足够的随机性，而 <strong>Mosaic</strong> 增强是四张图混合，因此 <strong>cache</strong> 数量默认 <strong>N=40</strong>， 同理 <strong>MixUp</strong> 的 <strong>cache</strong> 数量默认为<strong>20</strong>， <strong>tiny</strong> 模型需要更稳定的训练条件，因此其 <strong>cache</strong> 数量也为其余规格模型的一半（ <strong>MixUp</strong> 为<strong>10</strong>，<strong>Mosaic</strong> 为<strong>20</strong>）。</p>

<h3 id="2-强弱两阶段训练">(2) 强弱两阶段训练</h3>

<p><strong>Mosaic+MixUp</strong> 失真度比较高，持续用太强的数据增强对模型并不一定有益。为了使数据增强的方式更为通用，<strong>RTMDet</strong> 在前 <strong>280 epoch</strong> 使用不带旋转的 <strong>Mosaic+MixUp</strong>, 且通过混入 <strong>8</strong> 张图片来提升强度以及正样本数。后 <strong>20 epoch</strong> 使用比较小的学习率在比较弱的增强下进行微调，同时在 <strong>EMA</strong> 的作用下将参数缓慢更新至模型，能够得到比较大的提升。</p>

<h1 id="2-模型结构">2. 模型结构</h1>

<p><strong>RTMDet</strong> 模型整体结构和 <strong>YOLOX</strong> 几乎一致，由 <strong>CSPNeXt + CSPNeXtPAFPN +</strong> 共享卷积权重但分别计算 <strong>BN</strong> 的 <strong>SepBNHead</strong> 构成。内部核心模块也是 <strong>CSPLayer</strong>，但对其中的 <strong>Basic Block</strong> 进行了改进，提出了 <strong>CSPNeXt Block</strong>。</p>

<p><img src="https://pic.imgdb.cn/item/652a55fac458853aef288101.jpg" alt="" /></p>

<h3 id="1backbone">（1）backbone</h3>

<p><strong>CSPNeXt</strong> 整体以 <strong>CSPDarknet</strong> 为基础，共 <strong>5</strong> 层结构，包含 <strong>1</strong> 个 <strong>Stem Layer</strong> 和 <strong>4</strong> 个 <strong>Stage Layer</strong>：</p>
<ul>
  <li><strong>Stem Layer</strong> 是 <strong>3</strong> 层 <strong>3x3 kernel</strong> 的 <strong>ConvModule</strong>。</li>
  <li><strong>Stage Layer</strong> 总体结构与已有模型类似，前 <strong>3</strong> 个 <strong>Stage Layer</strong> 由 <strong>1</strong> 个 <strong>ConvModule</strong> 和 <strong>1</strong> 个 <strong>CSPLayer</strong> 组成。第 <strong>4</strong> 个 <strong>Stage Layer</strong> 在 <strong>ConvModule</strong> 和 <strong>CSPLayer</strong> 中间增加了 <strong>SPPF</strong> 模块。</li>
  <li><strong>CSPLayer</strong> 由 <strong>3</strong> 个 <strong>ConvModule + n</strong> 个 <strong>CSPNeXt Block</strong>(带残差连接) + <strong>1</strong> 个 <strong>Channel Attention</strong> 模块组成。<strong>ConvModule</strong> 为 <strong>1</strong> 层 <strong>3x3 Conv2d + BatchNorm + SiLU</strong> 激活函数。<strong>Channel Attention</strong> 模块为 <strong>1</strong> 层 <strong>AdaptiveAvgPool2d + 1</strong> 层 <strong>1x1 Conv2d + Hardsigmoid</strong> 激活函数。</li>
</ul>

<h3 id="2cspnext-block">（2）CSPNeXt Block</h3>

<p><strong>Darknet</strong> （图 <strong>a</strong>）使用 <strong>1x1</strong> 与 <strong>3x3</strong> 卷积的 <strong>Basic Block</strong>。<strong>YOLOv6 、YOLOv7 、PPYOLO-E</strong> （图 <strong>c &amp; d</strong>）使用了重参数化 <strong>Block</strong>。但重参数化的训练代价高，且不易量化，需要其他方式来弥补量化误差。 <strong>RTMDet</strong> 则借鉴了最近比较热门的 <strong>ConvNeXt</strong> 、<strong>RepLKNet</strong> 的做法，为 <strong>Basic Block</strong> 加入了大 <strong>kernel</strong> 的 <strong>depth-wise</strong> 卷积（图 <strong>b</strong>），并将其命名为 <strong>CSPNeXt Block</strong>。</p>

<p><img src="https://pic.imgdb.cn/item/652a57d6c458853aef2db5a3.jpg" alt="" /></p>

<h3 id="3调整检测器不同-stage-间的-block-数">（3）调整检测器不同 stage 间的 block 数</h3>

<p>由于 <strong>CSPNeXt Block</strong> 内使用了 <strong>depth-wise</strong> 卷积，单个 <strong>block</strong> 内的层数增多。如果保持原有的 <strong>stage</strong> 内的 <strong>block</strong> 数，则会导致模型的推理速度大幅降低。</p>

<p><strong>RTMDet</strong> 重新调整了不同 <strong>stage</strong> 间的 <strong>block</strong> 数，并调整了通道的超参，在保证了精度的情况下提升了推理速度。</p>

<h3 id="4backbone-与-neck-之间的参数量和计算量的均衡">（4）Backbone 与 Neck 之间的参数量和计算量的均衡</h3>

<p><strong>RTMDet</strong> 选择不引入额外的连接，而是改变 <strong>Backbone</strong> 与 <strong>Neck</strong> 间参数量的配比。该配比是通过手动调整 <strong>Backbone</strong> 和 <strong>Neck</strong> 的 <strong>expand_ratio</strong> 参数来实现的，其数值在 <strong>Backbone</strong> 和 <strong>Neck</strong> 中都为 <strong>0.5</strong>。<strong>expand_ratio</strong> 实际上是改变 <strong>CSPLayer</strong> 中各层通道数的参数。</p>

<p>实验发现，当 <strong>Neck</strong> 在整个模型中的参数量占比更高时，延时更低，且对精度的影响很小。</p>

<h3 id="5head">（5）Head</h3>

<p>传统的 <strong>YOLO</strong> 系列都使用同一 <strong>Head</strong> 进行分类和回归。<strong>RTMDet</strong> 参考了 <strong>NAS-FPN</strong> 中的做法，使用了 <strong>SepBNHead</strong>，在不同层之间共享卷积权重，但是独立计算 <strong>BN</strong> 的统计量。</p>

<h1 id="3-正负样本匹配策略">3. 正负样本匹配策略</h1>

<p>正负样本匹配策略或者称为标签匹配策略(<strong>Label Assignment</strong>)是目标检测模型训练中最核心的问题之一, 更好的标签匹配策略往往能够使得网络更好学习到物体的特征以提高检测能力。</p>

<p>早期的样本标签匹配策略一般都是基于<strong>空间以及尺度信息的先验</strong>来决定样本的选取。 典型案例如下：</p>
<ul>
  <li><strong>FCOS</strong> 中先限定网格中心点在 <strong>GT</strong> 内筛选后然后再通过不同特征层限制尺寸来决定正负样本</li>
  <li><strong>RetinaNet</strong> 则是通过 <strong>Anchor</strong> 与 <strong>GT</strong> 的最大 <strong>IOU</strong> 匹配来划分正负样本</li>
  <li><strong>YOLOV5</strong> 的正负样本则是通过样本的宽高比先筛选一部分, 然后通过位置信息选取 <strong>GT</strong> 中心落在的 <strong>Grid</strong> 以及临近的两个作为正样本</li>
</ul>

<p>但是上述方法都是属于基于<strong>先验</strong>的静态匹配策略, 就是样本的选取方式是根据人的经验规定的，不会随着网络的优化而进行自动优化选取到更好的样本, 近些年涌现了许多优秀的动态标签匹配策略：</p>
<ul>
  <li><strong>OTA</strong> 提出使用 <strong>Sinkhorn</strong> 迭代求解匹配中的最优传输问题</li>
  <li><strong>YOLOX</strong> 中使用 <strong>OTA</strong> 的近似算法 <strong>SimOTA</strong> , <strong>TOOD</strong> 将分类分数以及 <strong>IOU</strong> 相乘计算 <strong>Cost</strong> 矩阵进行标签匹配等等</li>
</ul>

<p>这些算法将 预测的 <strong>Bboxes</strong> 与 <strong>GT</strong> 的 <strong>IOU</strong>  和 分类分数 或者是对应 分类 <strong>Loss</strong> 和 回归 <strong>Loss</strong> 拿来计算 <strong>Matching Cost</strong> 矩阵再通过 <strong>top-k</strong> 的方式动态决定样本选取以及样本个数。通过这种方式, 在网络优化的过程中会自动选取对分类或者回归更加敏感有效的位置的样本, 它不再只依赖先验的静态的信息, 而是使用当前的预测结果去动态寻找最优的匹配, 只要模型的预测越准确, 匹配算法求得的结果也会更优秀。但是在网络训练的初期, 网络的分类以及回归是随机初始化, 这个时候还是需要 先验 来约束, 以达到 冷启动 的效果。</p>

<p><strong>RTMDet</strong> 作者也是采用了动态的 <strong>SimOTA</strong> 做法，不过其对动态的正负样本分配策略进行了改进。 之前的动态匹配策略往往使用与 <strong>Loss</strong> 完全一致的代价函数作为匹配的依据，但经过实验发现这并不一定是最优的。 使用更多 <strong>Soften</strong> 的 <strong>Cost</strong> 以及先验，能够提升性能。</p>

<h3 id="1bbox编解码">（1）bbox编解码</h3>

<p><strong>RTMDet</strong> 的编码器将 <strong>gt bboxes</strong> 的左上角与右下角坐标 $(x_1, y_1, x_2, y_2)$ 编码为 中心点距离四边的距离 <strong>(top, bottom, left, right)</strong>，并且解码至原图像上。</p>

<h3 id="2匹配策略">（2）匹配策略</h3>

<p><strong>RTMDet</strong> 提出了 <strong>Dynamic Soft Label Assigner</strong> 来实现标签的动态匹配策略, 该方法主要包括使用位置先验信息损失、样本回归损失、样本分类损失，同时对三个损失进行了 <strong>Soft</strong> 处理进行参数调优, 以达到最佳的动态匹配效果。</p>

<p>位置先验信息损失(<strong>Soft_Center_Prior</strong>)：</p>

\[C_{center} = \alpha^{|x_{pred}-x_{gt}|-\beta}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 计算gt与anchor point的中心距离并转换到特征图尺度
</span><span class="n">distance</span> <span class="o">=</span> <span class="p">(</span><span class="n">valid_prior</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">gt_center</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
                    <span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">strides</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>
<span class="c1"># 以10为底计算位置的软化损失,限定在gt的6个单元格以内
</span><span class="n">soft_center_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">distance</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<p>样本回归损失(<strong>IOU_Cost</strong>)：</p>

\[C_{reg} = -\log(IOU)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 计算回归 bboxes 和 gts 的 iou
</span><span class="n">pairwise_ious</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">iou_calculator</span><span class="p">(</span><span class="n">valid_decoded_bbox</span><span class="p">,</span> <span class="n">gt_bboxes</span><span class="p">)</span>
<span class="c1"># 将 iou 使用 log 进行 soft , iou 越小 cost 更小
</span><span class="n">iou_cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">pairwise_ious</span> <span class="o">+</span> <span class="n">EPS</span><span class="p">)</span> <span class="o">*</span> <span class="mi">3</span>
</code></pre></div></div>

<p>样本分类损失(<strong>Soft_Cls_Cost</strong>)：</p>

\[C_{cls} = CE(P,Y_{soft}) *(Y_{soft}-P)^2\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 生成分类标签
</span><span class="n">gt_onehot_label</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">F</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">gt_labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">),</span>
              <span class="n">pred_scores</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nb">float</span><span class="p">().</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span>
                  <span class="n">num_valid</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">valid_pred_scores</span> <span class="o">=</span> <span class="n">valid_pred_scores</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_gt</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># 不单单将分类标签为01,而是换成与 gt 的 iou
</span><span class="n">soft_label</span> <span class="o">=</span> <span class="n">gt_onehot_label</span> <span class="o">*</span> <span class="n">pairwise_ious</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>
<span class="c1"># 使用 quality focal loss 计算分类损失 cost ,与实际的分类损失计算保持一致
</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">soft_label</span> <span class="o">-</span> <span class="n">valid_pred_scores</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="n">soft_cls_cost</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
    <span class="n">valid_pred_scores</span><span class="p">,</span> <span class="n">soft_label</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span><span class="p">.</span><span class="nb">abs</span><span class="p">().</span><span class="nb">pow</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">soft_cls_cost</span> <span class="o">=</span> <span class="n">soft_cls_cost</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>通过计算上述三个损失的和得到最终的 <strong>cost_matrix</strong> 后, 再使用 <strong>SimOTA</strong> 决定每一个 <strong>GT</strong> 匹配的样本的个数并决定最终的样本。具体操作如下所示：</p>
<ul>
  <li>首先通过自适应计算每一个 <strong>gt</strong> 要选取的样本数量： 取每一个 <strong>gt</strong> 与所有 <strong>bboxes</strong> 前 <strong>13</strong> 大的 <strong>iou</strong>, 得到它们的和取整后作为这个 <strong>gt</strong> 的 样本数目 , 最少为 <strong>1</strong> 个, 记为 <strong>dynamic_ks</strong>。</li>
  <li>对于每一个 <strong>gt</strong> , 将其 <strong>cost_matrix</strong> 矩阵前 <strong>dynamic_ks</strong> 小的位置作为该 <strong>gt</strong> 的正样本。</li>
  <li>对于某一个 <strong>bbox</strong>, 如果被匹配到多个 <strong>gt</strong> 就将与这些 <strong>gts</strong> 的 <strong>cost_marix</strong> 中最小的那个作为其 <strong>label</strong>。</li>
</ul>

<p>在网络训练初期，因参数初始化，回归和分类的损失值 <strong>Cost</strong> 往往较大, 这时候 <strong>IOU</strong> 比较小， 选取的样本较少，主要起作用的是 <strong>Soft_center_prior</strong> 也就是位置信息，优先选取位置距离 <strong>GT</strong> 比较近的样本作为正样本，这也符合人们的理解，在网络前期给少量并且有足够质量的样本，以达到冷启动。</p>

<p>当网络进行训练一段时间过后，分类分支和回归分支都进行了一定的优化后，这时 <strong>IOU</strong> 变大， 选取的样本也逐渐增多，这时网络也有能力学习到更多的样本，同时因为 <strong>IOU_Cost</strong> 以及 <strong>Soft_Cls_Cost</strong> 变小，网络也会动态的找到更有利优化分类以及回归的样本点。</p>

<h3 id="3损失函数设计">（3）损失函数设计</h3>

<p>参与 <strong>Loss</strong> 计算的共有两个值：<strong>loss_cls</strong> 和 <strong>loss_bbox</strong>，其中<strong>loss_cls</strong>采用<strong>QualityFocalLoss</strong>，<strong>loss_bbox</strong>采用<strong>GIoULoss</strong>；权重比例是：<strong>loss_cls : loss_bbox = 1 : 2</strong>。</p>

<p><strong>QualityFocalLoss</strong>将离散标签的 <strong>focal loss</strong> 泛化到连续标签上，将 <strong>bboxes</strong> 与 <strong>gt</strong> 的 <strong>IoU</strong> 作为分类分数的标签，使得分类分数为表征回归质量的分数。</p>

\[{QFL}(\sigma) = -|y-\sigma|^\beta((1-y)\log(1-\sigma)+y\log(\sigma))\]

<p><strong>GIoU Loss</strong> 用于计算两个框重叠区域的关系，重叠区域越大，损失越小，反之越大。而且 <strong>GIoU</strong> 是在 $[0,2]$ 之间，因为其值被限制在了一个较小的范围内，所以网络不会出现剧烈的波动，证明了其具有比较好的稳定性。</p>

<h3 id="4推理过程">（4）推理过程</h3>

<p><img src="https://pic.imgdb.cn/item/652c9a3ec458853aef6467e5.jpg" alt="" /></p>

<ol>
  <li>特征图输入：预测的图片输入大小为 <strong>640 x 640</strong>, 通道数为 <strong>3</strong> ,经过 <strong>CSPNeXt</strong>, <strong>CSPNeXtPAFPN</strong> 层的 <strong>8</strong> 倍、<strong>16</strong> 倍、<strong>32</strong> 倍下采样得到 <strong>80 x 80, 40 x 40, 20 x 20</strong> 三个尺寸的特征图。以 <strong>rtmdet-l</strong> 模型为例，此时三层通道数都为 <strong>256</strong>，经过 <strong>bbox_head</strong> 层得到两个分支，分别为 <strong>rtm_cls</strong> 类别预测分支，将通道数从 <strong>256</strong> 变为 <strong>80</strong>，<strong>80</strong> 对应所有类别数量; <strong>rtm_reg</strong> 边框回归分支将通道数从 <strong>256</strong> 变为 <strong>4</strong>，<strong>4</strong> 代表框的坐标。</li>
  <li>初始化网格：根据特征图尺寸初始化三个网格，大小分别为 <strong>6400 (80 x 80)</strong>、<strong>1600 (40 x 40)</strong>、<strong>400 (20 x 20)</strong>，表示当前特征层的网格点数量，最后一个维度是 <strong>2</strong>，为网格点的横纵坐标。</li>
  <li>维度变换：经过 <strong>_predict_by_feat_single</strong> 函数，将从 <strong>head</strong> 提取的单一图像的特征转换为 <strong>bbox</strong> 结果输入，之后分别遍历三个特征层，分别对 <strong>class</strong> 类别预测分支、<strong>bbox</strong> 回归分支进行处理。以第一层为例，对 <strong>bbox</strong> 预测分支 <strong>[ 4，80，80 ]</strong> 维度变换为 <strong>[ 6400，4 ]</strong>，对类别预测分支 <strong>[ 80，80，80 ]</strong> 变化为 <strong>[ 6400，80 ]</strong>，并对其做归一化，确保类别置信度在 <strong>0 - 1</strong> 之间。</li>
  <li>阈值过滤：先使用一个 <strong>nms_pre</strong> 操作，先过滤大部分置信度比较低的预测结果（比如 <strong>score_thr</strong> 阈值设置为 <strong>0.05</strong>，则去除当前预测置信度低于 <strong>0.05</strong> 的结果），然后得到 <strong>bbox</strong> 坐标、所在网格的坐标、置信度、标签的信息。经过三个特征层遍历之后，分别整合这三个层得到的的四个信息放入列表中。</li>
  <li>还原到原图尺度：最后将网络的预测结果映射到整图当中，得到 <strong>bbox</strong> 在整图中的坐标值。</li>
  <li><strong>NMS</strong>：进行 <strong>nms</strong> 操作，最终预测得到的返回值为经过后处理的每张图片的检测结果，包含分类置信度、框的<strong>labels</strong>、框的四个坐标。</li>
</ol>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2022/12/08/alphapose.html" class="read-next-link"></a>
        <section>
          <span>AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time</span>
          <p>  AlphaPose: 全身区域实时多人姿态估计与跟踪.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/668f8ce0d9c307b7e9ee80d0.png" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2022/12/06/ipm.html" class="read-next-link"></a>
          <section>
            <span>积分概率度量(Integral Probability Metric)</span>
            <p>  Integral Probability Metric.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/63904005b1fccdcd3609179e.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
