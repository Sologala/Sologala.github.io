<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>半监督学习(Semi-Supervised Learning) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="半监督学习(Semi-Supervised Learning)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="半监督学习(Semi-Supervised Learning) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2022/09/01/semi.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>半监督学习(Semi-Supervised Learning)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="22-09-01"><i class="iconfont icon-date"></i>01 Sep 2022</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/63bfccdabe43e0d30edcee30.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Semi-Supervised Learning.</p>
</blockquote>

<p><strong>半监督学习(Semi-Supervised Learning)</strong>是指同时从有标签数据和无标签数据中进行学习，适用于标注数据有限、标注成本较高的场合。值得一提的是，目前半监督学习方法主要是为视觉任务设计的，而在自然语言任务中通常采用预训练+微调的学习流程。</p>

<p>在半监督学习中，通过有标签数据构造监督损失\(\mathcal{L}_s\)，通过无标签数据构造无监督损失\(\mathcal{L}_u\)，总损失函数通过权重项$\mu$加权：</p>

\[\mathcal{L} = \mathcal{L}_s + \mu \mathcal{L}_u\]

<p>其中权重项$\mu=\mu(t)$通常设置为随训练轮数$t$变化的斜坡函数(<strong>ramp function</strong>)，以逐渐增大无监督损失的重要性。</p>

<p>半监督学习的一些假设：</p>
<ul>
  <li><strong>平滑性假设 (Smoothness Assumption)</strong>：如果两个数据样本在特征空间的高密度区域中接近，则他们的标签应该相同或非常相似。</li>
  <li><strong>聚类假设 (Cluster Assumption)</strong>：特征空间既有高密度区域又有稀疏区域。高密度分组的数据点自然形成一个聚类集群。同一集群中的样本应具有相同的标签。</li>
  <li><strong>低密度分离假设 (Low-density Separation Assumption)</strong>：样本类别之间的决策边界倾向于位于稀疏的低密度区域；否则决策边界会将一个高密度集群划分为两个类，对应于两个集群，这会使平滑性假设和聚类假设无效。</li>
  <li><strong>流形假设 (Manifold Assumption)</strong>：高维数据往往位于低维流形上。尽管真实世界数据通常在非常高的维度上观察到，它们实际上可以由较低维度的流形来捕获，其中某些属性相似的点被分为相近的分组。流形假设是表示学习的基础，能够学习更有效的表示，以便发现和测量无标签数据点之间的相似性。</li>
</ul>

<p>常用的半监督学习方法包括：</p>
<ul>
  <li><strong>一致性正则化</strong>：假设神经网络的随机性或数据增强不会改变输入样本的真实标签，如$\Pi$<strong>-Model</strong>, <strong>Temporal Ensembling</strong>, <strong>Mean Teacher</strong>, <strong>VAT</strong>, <strong>ICT</strong>, <strong>UDA</strong>。</li>
  <li><strong>伪标签</strong>：根据当前模型的最大预测概率为无标签样本指定假标签，如<strong>Label Propagation</strong>, <strong>Confirmation Bias</strong>, <strong>Noisy Student</strong>, <strong>Meta Pseudo Label</strong>。</li>
  <li><strong>一致性正则化+伪标签</strong>：既构造无标签样本的伪标签，又同时建立监督损失和无监督损失，如<strong>MixMatch</strong>, <strong>ReMixMatch</strong>, <strong>FixMatch</strong>, <strong>DivideMix</strong>。</li>
</ul>

<h1 id="1-一致性正则化-consistency-regularization">1. 一致性正则化 Consistency Regularization</h1>

<p><strong>一致性正则化(Consistency Regularization)</strong>也称为<strong>一致性训练(Consistency Training)</strong>，是假设神经网络的随机性或数据增强不会改变输入样本的真实标签，并可以基于此构造一致性正则化损失\(\mathcal{L}_u\)。</p>

<p><img src="https://pic.imgdb.cn/item/63babe39be43e0d30e4bc733.jpg" alt="" /></p>

<h3 id="-pi-model">⚪ <a href="https://0809zheng.github.io/2022/09/02/pimodel.html"><font color="blue">$\Pi$-Model</font></a></h3>

<p>$\Pi$<strong>-Model</strong>的无监督损失旨在最小化一个数据样本两次经过同一个带随机变换(如数据增强或<strong>dropout</strong>)的网络后预测结果的差异：</p>

\[\mathcal{L}_u^{\Pi} = \sum_{x \in \mathcal{D}} \text{Dist}[f_{\theta}(x),f_{\theta}'(x)]\]

<p><img src="https://pic.imgdb.cn/item/63ba8bf9be43e0d30ee9ac61.jpg" alt="" /></p>

<h3 id="-temporal-ensembling">⚪ <a href="https://0809zheng.github.io/2022/09/03/te.html"><font color="blue">Temporal Ensembling</font></a></h3>

<p><strong>时序集成</strong>对每个数据样本$x$的预测结果$f_{\theta}(x)$存储一个指数滑动平均值\(\tilde{f}_{\theta}(x) \leftarrow \beta \tilde{f}_{\theta}(x) + (1-\beta)f_{\theta}(x)\)；则无监督损失旨在最小化当前预测与滑动平均的差异：</p>

\[\mathcal{L}_u^{TE} = \sum_{x \in \mathcal{D}} \text{Dist}[f_{\theta}(x),\tilde{f}_{\theta}(x)]\]

<p><img src="https://pic.imgdb.cn/item/63ba8c12be43e0d30eea0654.jpg" alt="" /></p>

<h3 id="-mean-teacher">⚪ <a href="https://0809zheng.github.io/2022/09/04/meanteacher.html"><font color="blue">Mean Teacher</font></a></h3>

<p><strong>Mean Teacher</strong>存储模型参数的滑动平均值$\theta’\leftarrow \beta \theta’ + (1-\beta)\theta$作为教师模型，通过当前学生模型和教师模型的预测结果构造无监督损失。</p>

\[\mathcal{L}_u^{MT} = \sum_{x \in \mathcal{D}} \text{Dist}[f_{\theta}(x),f_{\theta'}(x)]\]

<p><img src="https://pic.imgdb.cn/item/63ba8c2ebe43e0d30eea62af.jpg" alt="" /></p>

<h3 id="-virtual-adversarial-training-vat">⚪ <a href="https://0809zheng.github.io/2022/09/05/vat.html"><font color="blue">Virtual Adversarial Training (VAT)</font></a></h3>

<p><strong>虚拟对抗训练</strong>把对抗训练的思想引入半监督学习：构造当前样本的攻击噪声$r$，则无监督损失旨在最小化引入噪声$r$前后模型预测结果的差异：</p>

\[\begin{aligned} r &amp;= \mathop{\arg \max}_{||r|| \leq \epsilon} \text{Dist}[\text{sg}(f_{\theta}(x)),f_{\theta}(x+r)] \\ \mathcal{L}_u^{VAT} &amp;= \sum_{x \in \mathcal{D}} \text{Dist}[\text{sg}(f_{\theta}(x)),f_{\theta}(x+r)]  \end{aligned}\]

<h3 id="-interpolation-consistency-training-ict">⚪ <a href="https://0809zheng.github.io/2022/09/06/ict.html"><font color="blue">Interpolation Consistency Training (ICT)</font></a></h3>

<p><strong>插值一致性训练</strong>通过<strong>mixup</strong>构造插值样本进行一致性预测，则无监督损失旨在最小化插值样本的预测结果和预测结果的插值之间的差异：</p>

\[\mathcal{L}_u^{ICT} = \sum_{(x_i,x_j) \in \mathcal{D}} \text{Dist}[f_{\theta}(\lambda x_i + (1-\lambda) x_j),\lambda f_{\theta'}(x_i) + (1-\lambda) f_{\theta'}(x_j)]\]

<p>其中$\theta’$是$\theta$的滑动平均值。</p>

<p><img src="https://pic.imgdb.cn/item/63bba04bbe43e0d30ec1763a.jpg" alt="" /></p>

<h3 id="-unsupervised-data-augmentation-uda">⚪ <a href="https://0809zheng.github.io/2022/09/07/uda.html"><font color="blue">Unsupervised Data Augmentation (UDA)</font></a></h3>

<p><strong>无监督数据增强</strong>采用先进的数据增强策略生成噪声样本\(\hat{x}\)，并采用以下技巧：丢弃预测置信度低于阈值$\tau$的样本；在<strong>softmax</strong>中引入温度系数$T$；训练一个分类器预测域标签，并保留域内分类置信度高的样本。</p>

\[\mathcal{L}_u^{UDA} = \sum_{x \in \mathcal{D}} \Bbb{I} [\mathop{\max}_c f_{\theta}^c(x) &gt; \tau] \cdot \text{Dist}[\text{sg}(f_{\theta}(x;T)),f_{\theta}(\hat{x})]\]

<p><img src="https://pic.imgdb.cn/item/63bbf895be43e0d30e769e1b.jpg" alt="" /></p>

<h1 id="2-伪标签-pseudo-label">2. 伪标签 Pseudo Label</h1>

<p><strong>伪标签(pseudo label)</strong>方法是指根据当前模型的最大预测概率为无标签样本指定假标签，然后通过监督学习方法同时训练有标签和无标签样本。</p>

<p>伪标签等价于<a href="https://papers.nips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html"><strong>熵正则化(Entropy Regularization)</strong></a>，即最小化无标签样本的类别概率的条件熵以实现类别间的低密度分离。模型预测的类别概率是<strong>类别间重叠(class overlap)</strong>的一种测度，最小化其熵等价于减少不同类别的重叠。下图表明在训练过程中引入伪标签样本后，学习到的特征空间会更加分离。</p>

<p><img src="https://pic.imgdb.cn/item/63bccedebe43e0d30edc1144.jpg" alt="" /></p>

<p>通常用于生成伪标签的模型称为<strong>教师(Teacher)</strong>，通过伪标签样本进行学习的模型称为<strong>学生(Student)</strong>。使用伪标签进行训练的过程是一种迭代过程。</p>

<h3 id="-label-propagation">⚪ <a href="https://0809zheng.github.io/2022/09/08/labelprop.html"><font color="blue">Label Propagation</font></a></h3>

<p><strong>标签传播(Label Propagation)</strong>通过特征嵌入构造样本之间的相似图，然后把有标签样本的标签传播到无标签样本，传播权重正比于图中的相似度得分。</p>

<p><img src="https://pic.imgdb.cn/item/63bcdb9abe43e0d30ef28825.jpg" alt="" /></p>

<h3 id="-confirmation-bias">⚪ <a href="https://0809zheng.github.io/2022/09/09/confirmation.html"><font color="blue">Confirmation Bias</font></a></h3>

<p><strong>确认偏差(Confirmation bias)</strong>是指教师网络可能会提供错误的伪标签，可能会导致学生网络对这些错误标签过拟合。为了缓解确认偏差问题，可以对样本及其软标签应用<strong>mixup</strong>；并在训练时对有标签样本进行过采样。</p>

<h3 id="-noisy-student">⚪ <a href="https://0809zheng.github.io/2020/08/07/noisy-student-training.html"><font color="blue">Noisy Student</font></a></h3>

<p><strong>Noisy Student</strong>首先使用有标记数据集训练一个教师网络，通过教师网络构造无标签数据的伪标签；然后通过全部数据训练一个更大的学生网络，训练过程中引入噪声干扰；最后将训练好的学生网络作为新的教师网络，并重复上述过程。</p>

<p><img src="https://pic.imgdb.cn/item/63ac21ab08b6830163f13903.jpg" alt="" /></p>

<h3 id="-meta-pseudo-label">⚪ <a href="https://0809zheng.github.io/2022/09/10/metapseudo.html"><font color="blue">Meta Pseudo Label</font></a></h3>

<p><strong>元伪标签</strong>根据学生网络在标注数据集上的反馈表现持续地调整教师网络。记教师网络和学生网络的参数分别为$\theta_T,\theta_S$，则元伪标签的目标函数为：</p>

\[\begin{aligned} \mathop{\min}_{\theta_T} \mathcal{L}_s(\theta_S(\theta_T&amp;)) = \mathop{\min}_{\theta_T} \sum_{(x^l,y) \in \mathcal{X}} \text{CE}[y,f_{\theta_S(\theta_T)}(x^l)] \\ \text{where} \quad \theta_S(\theta_T) &amp;= \mathop{\arg \min}_{\theta_S} \mathcal{L}_u(\theta_S,\theta_T) \\ &amp; = \mathop{\arg \min}_{\theta_S} \sum_{x^u \in \mathcal{U}} \text{CE}[f_{\theta_T}(x),f_{\theta_S}(x)] \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63bd20dabe43e0d30e7c3bf4.jpg" alt="" /></p>

<h1 id="3-一致性正则化伪标签">3. 一致性正则化+伪标签</h1>

<p>在半监督学习中可以结合一致性正则化和伪标签方法：既构造无标签样本的伪标签，又同时建立监督损失和无监督损失。</p>

<h3 id="-mixmatch">⚪ <a href="https://0809zheng.github.io/2022/09/11/mixmatch.html"><font color="blue">MixMatch</font></a></h3>

<p><strong>MixMatch</strong>针对每个无标签样本生成$K$个数据增强的样本\(\overline{u}^{(k)},k=1,...,K\)，然后通过预测结果的平均构造伪标签：\(\hat{y} = \frac{1}{K} \sum_{k=1}^K f_{\theta}(\overline{u}^{(k)})\)。在此基础上结合一致性正则化、熵最小化、<strong>MixUp</strong>增强以构造监督损失和无监督损失：</p>

\[\begin{aligned}  \mathcal{L}_s^{MM} &amp;= \frac{1}{|\overline{\mathcal{X}}|} \sum_{(\overline{x},y) \in \overline{\mathcal{X}}} D[y,f_{\theta}(\overline{x})] \\ \mathcal{L}_u^{MM} &amp;= \frac{1}{C|\overline{\mathcal{U}}|} \sum_{(\overline{u},\hat{y}) \in \overline{\mathcal{U}}} ||\hat{y},f_{\theta}(\overline{u})||_2^2 \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63be1cc8be43e0d30e1c9988.jpg" alt="" /></p>

<h3 id="-remixmatch">⚪ <a href="https://0809zheng.github.io/2022/09/12/remixmatch.html"><font color="blue">ReMixMatch</font></a></h3>

<p><strong>ReMixMatch</strong>引入了分布对齐和增强锚点。分布对齐是指把构造的伪标签的分布\(p(\hat{y})\)调整为更接近已标注样本的标签分布$p(y)$。增强锚点是指给定未标注样本$u$，首先通过较弱的数据增强生成一个样本锚点，然后通过$K$次较强增强的预测均值构造伪标签。</p>

<p><img src="https://pic.imgdb.cn/item/63bf70d6be43e0d30e3efb43.jpg" alt="" /></p>

<h3 id="-fixmatch">⚪ <a href="https://0809zheng.github.io/2022/09/15/fixmatch.html"><font color="blue">FixMatch</font></a></h3>

<p><strong>FixMatch</strong>通过较弱的数据增强生成未标注样本的伪标签，并且只保留具有较高置信度的预测结果。较对于监督损失，通过较弱的数据增强预测结果；对于无监督损失，通过较强的数据增强预测结果。</p>

\[\begin{aligned}  \mathcal{L}_s &amp;= \frac{1}{|\mathcal{X}|} \sum_{(x,y) \in \mathcal{X}} D[y,f_{\theta}(\mathcal{A}_{\text{weak}}(x))] \\ \mathcal{L}_u &amp;= \frac{1}{|\mathcal{U}|} \sum_{(u,\hat{y}) \in \mathcal{U}} \Bbb{I}[\max(\hat{y})\geq \tau] \cdot D[\hat{y},f_{\theta}(\mathcal{A}_{\text{strong}}(u))] \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63bf8332be43e0d30e5c120b.jpg" alt="" /></p>

<h3 id="-dividemix">⚪ <a href="https://0809zheng.github.io/2022/09/13/dividemix.html"><font color="blue">DivideMix</font></a></h3>

<p><strong>DivideMix</strong>把半监督学习和噪声标签学习结合起来，通过高斯混合模型把训练数据分成标注数据集和未标注数据集；同时训练了两个独立的网络，每个网络使用根据另一个网络预测结果划分的数据集；对于标注数据集，根据另一个网络划分数据集的概率$w_i$把真实标签$y_i$和多次数据增强后的预测结果均值\(\hat{y}_i\)进行加权；对于未标注数据集，平均两个网络的预测结果；之后应用<strong>MixMatch</strong>方法进行训练。</p>

<p><img src="https://pic.imgdb.cn/item/63bf7560be43e0d30e454baa.jpg" alt="" /></p>

<h2 id="-参考文献">⭐ 参考文献</h2>
<ul>
  <li><a href="https://lilianweng.github.io/posts/2021-12-05-semi-supervised/">Learning with not Enough Data Part 1: Semi-Supervised Learning</a>(Lil’Log)一篇介绍半监督学习的博客。</li>
  <li><a href="https://arxiv.org/abs/2006.05278">An Overview of Deep Semi-Supervised Learning</a>：(arXiv2006)一篇深度半监督学习的综述。</li>
  <li><a href="https://0809zheng.github.io/2022/09/02/pimodel.html"><font color="blue">Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning</font></a>：(arXiv1606)深度半监督学习的随机变换和扰动正则化。</li>
  <li><a href="https://0809zheng.github.io/2022/09/03/te.html"><font color="blue">Temporal Ensembling for Semi-Supervised Learning</font></a>：(arXiv1610)半监督学习的时序集成。</li>
  <li><a href="https://0809zheng.github.io/2022/09/04/meanteacher.html"><font color="blue">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</font></a>：(arXiv1703)加权平均一致性目标改进半监督深度学习。</li>
  <li><a href="https://0809zheng.github.io/2022/09/05/vat.html"><font color="blue">Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</font></a>：(arXiv1704)虚拟对抗训练：一种半监督学习的正则化方法。</li>
  <li><a href="https://0809zheng.github.io/2022/09/06/ict.html"><font color="blue">Interpolation Consistency Training for Semi-Supervised Learning</font></a>：(arXiv1903)半监督学习的插值一致性训练。</li>
  <li><a href="https://0809zheng.github.io/2022/09/07/uda.html"><font color="blue">Unsupervised Data Augmentation for Consistency Training</font></a>：(arXiv1904)一致性训练的无监督数据增强。</li>
  <li><a href="https://0809zheng.github.io/2022/09/08/labelprop.html"><font color="blue">Label Propagation for Deep Semi-supervised Learning</font></a>：(arXiv1904)深度无监督学习的标签传播。</li>
  <li><a href="https://0809zheng.github.io/2022/09/11/mixmatch.html"><font color="blue">MixMatch: A Holistic Approach to Semi-Supervised Learning</font></a>：(arXiv1905)MixMatch：一种半监督学习的整体方法。</li>
  <li><a href="https://0809zheng.github.io/2022/09/09/confirmation.html"><font color="blue">Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning</font></a>：(arXiv1908)深度无监督学习的伪标签和确认偏差。</li>
  <li><a href="https://0809zheng.github.io/2020/08/07/noisy-student-training.html"><font color="blue">Self-training with Noisy Student improves ImageNet classification</font></a>：(arXiv1911)通过噪声学生网络的自训练改进图像分类。</li>
  <li><a href="https://0809zheng.github.io/2022/09/12/remixmatch.html"><font color="blue">ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring</font></a>：(arXiv1911)ReMixMatch：通过分布对齐和增强锚点实现半监督学习。</li>
  <li><a href="https://0809zheng.github.io/2022/09/15/fixmatch.html"><font color="blue">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</font></a>：(arXiv2001)FixMatch：通过一致性和置信度简化半监督学习。</li>
  <li><a href="https://0809zheng.github.io/2022/09/13/dividemix.html"><font color="blue">DivideMix: Learning with Noisy Labels as Semi-supervised Learning</font></a>：(arXiv2002)DivideMix：通过噪声标签实现半监督学习。</li>
  <li><a href="https://0809zheng.github.io/2022/09/10/metapseudo.html"><font color="blue">Meta Pseudo Labels</font></a>：(arXiv2003)元伪标签。</li>
</ul>


    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2022/09/02/pimodel.html" class="read-next-link"></a>
        <section>
          <span>Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning</span>
          <p>  深度半监督学习的随机变换和扰动正则化.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/63ba34b3be43e0d30e64c509.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2022/08/22/waal.html" class="read-next-link"></a>
          <section>
            <span>Deep Active Learning: Unified and Principled Method for Query and Training</span>
            <p>  WAAL：使用Wasserstein距离建模主动学习的查询过程.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/632adb8516f2c2beb1020a93.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
