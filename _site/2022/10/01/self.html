<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>自监督学习(Self-Supervised Learning) - DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="自监督学习(Self-Supervised Learning)">
    <meta name="keywords"  content="深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="自监督学习(Self-Supervised Learning) - DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/2022/10/01/self.html">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
	<!--
Author: Ray-Eldath
refer to:
 - http://docs.mathjax.org/en/latest/options/index.html
-->

	<script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
			displayMath: [ ["$$", "$$"], ["\\[","\\]"] ],
			skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		},
		"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
      });
    </script>


	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>
	
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?671e6ffb306c963dfa227c8335045b4f";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
		
        })();
    </script>

</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="true"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


  <header
    class="g-banner post-header post-pattern-circuitBoard bgcolor-default "
    data-theme="default"
  >
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="post-tag">深度学习</a>
          
        
      </div>
      <h1>自监督学习(Self-Supervised Learning)</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i>郑之杰</span>
        <time class="post-meta-item" datetime="22-10-01"><i class="iconfont icon-date"></i>01 Oct 2022</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('https://pic.imgdb.cn/item/63bfffa1be43e0d30e3eda4f.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    

    <article class="markdown-body">
      <blockquote>
  <p>Self-Supervised Learning.</p>
</blockquote>

<p><strong>自监督学习(Self-Supervised Learning)</strong>是一种无监督表示学习方法，旨在根据无标签数据集中的一部分信息预测剩余的信息，并以有监督的方式来训练该数据集。</p>

<p>自监督学习的优势包括：</p>
<ul>
  <li>能够充分利用大型无标签数据集，利用数据集本身以较低成本构造大量伪标签；</li>
  <li>能够学习携带语义或结构信息的数据特征表示，从而有益于下游任务。</li>
</ul>

<p>自监督学习已被广泛应用在自然语言处理任务中，比如语言模型默认的<a href="https://0809zheng.github.io/2020/04/27/elmo-bert-gpt.html">预训练任务</a>就是根据过去的序列预测下一个单词。本文主要关注计算机视觉领域的自监督学习方法，即如何构造适用于图像数据集的自监督任务，包括：</p>
<ul>
  <li><strong>前置任务(pretext task)</strong>：通过从数据集中自动构造伪标签而设计的对目标任务有帮助的辅助任务，如<strong>Exemplar-CNN</strong>, <strong>Context Prediction</strong>, <strong>Jigsaw Puzzle</strong>, <strong>Image Colorization</strong>, <strong>Learning to Count</strong>, <strong>Image Rotation</strong>, <strong>Jigsaw Clustering</strong>, <strong>Evolving Loss</strong>, <strong>PIC</strong>, <strong>MP3</strong>。</li>
  <li><strong>对比学习(contrastive learning)</strong>：学习一个特征嵌入空间使得正样本对彼此靠近、负样本对相互远离。(对比损失函数) <strong>NCE</strong>, <strong>CPC</strong>, <strong>CPC v2</strong>, <strong>Alignment and Uniformity</strong>, <strong>Debiased Contrastive Loss</strong>, <strong>Hard Negative Samples</strong>, <strong>FlatNCE</strong>; (并行数据增强) <strong>InvaSpread</strong>, <strong>SimCLR</strong>, <strong>SimCLRv2</strong>, <strong>BYOL</strong>, <strong>SimSiam</strong>, <strong>DINO</strong>, <strong>SwAV</strong>, <strong>PixContrast</strong>, <strong>Barlow Twins</strong>; (存储体) <strong>InstDisc</strong>, <strong>MoCo</strong>, <strong>MoCo v2</strong>, <strong>MoCo v3</strong>; (多模态) <strong>CMC</strong>, <strong>CLIP</strong>; (应用) <strong>CURL</strong>, <strong>CUT</strong>, <strong>Background Augmentation</strong>, <strong>FD</strong>。</li>
  <li><strong>掩码图像建模(masked image modeling)</strong>：随机遮挡图像中的部分<strong>patch</strong>，并以自编码器的形式重构这部分<strong>patch</strong>，如<strong>BEiT</strong>, <strong>MAE</strong>, <strong>SimMIM</strong>, <strong>iBOT</strong>, <strong>ConvMAE</strong>, <strong>QB-Heat</strong>, <strong>LocalMIM</strong>, <strong>DeepMIM</strong>。</li>
</ul>

<h3 id="扩展阅读">⭐扩展阅读：</h3>
<ul>
  <li><a href="https://0809zheng.github.io/2020/07/07/learn-from-one-image.html"><font color="blue">A critical analysis of self-supervision, or what we can learn from a single image</font></a>：(arXiv1904)使用单张图像进行自监督学习。</li>
</ul>

<h1 id="1-基于前置任务的自监督学习方法">1. 基于前置任务的自监督学习方法</h1>

<p><strong>前置任务(pretext task)</strong>也叫<strong>代理任务(surrogate task)</strong>，是指通过从数据集中自动构造伪标签而设计的对目标任务有帮助的辅助任务。</p>

<h3 id="-exemplar-cnn">⚪ <a href="https://0809zheng.github.io/2022/10/02/exemplarcnn.html"><font color="blue">Exemplar-CNN</font></a></h3>

<p><strong>Exemplar-CNN</strong>从图像数据集的梯度较大区域（通常覆盖边缘并包含目标的一部分）中采样$32 \times 32$大小的图像块；对每一个图像块应用不同的随机图像增强，同一个图像块的增强样本属于同一个代理类别；自监督学习的前置任务旨在对不同的代理类别进行分类。</p>

<p><img src="https://pic.imgdb.cn/item/63c0d8a9be43e0d30e7cc5b5.jpg" alt="" /></p>

<h3 id="-context-prediction">⚪ <a href="https://0809zheng.github.io/2022/10/04/context.html"><font color="blue">Context Prediction</font></a></h3>

<p>随机在图像中选取一个图像块；然后考虑以该图像块为中心的$3\times 3$网格，随机选择其$8$个邻域图像块中的一个；则自监督学习的前置任务是预测后者属于哪个邻域的八分类任务。</p>

<p><img src="https://pic.imgdb.cn/item/63c104d3be43e0d30ec7e6ce.jpg" alt="" /></p>

<h3 id="-jigsaw-puzzle">⚪ <a href="https://0809zheng.github.io/2022/10/05/jigsaw.html"><font color="blue">Jigsaw Puzzle</font></a></h3>

<p>随机打乱图像中的九个图像块，通过共享权重的模型分别处理每一个图像块，并根据预定义的排列集合输出图像块排列的索引概率，则自监督学习的前置任务是一种多分类任务。</p>

<p><img src="https://pic.imgdb.cn/item/63c10983be43e0d30ecfac9e.jpg" alt="" /></p>

<h3 id="-image-colorization">⚪ <a href="https://0809zheng.github.io/2022/10/07/colorization.html"><font color="blue">Image Colorization</font></a></h3>

<p><strong>着色</strong>是指把输入灰度图像转化为彩色图像，即将灰度图像映射到量化颜色值输出的分布上。彩色图像设置在<strong>Lab*</strong>颜色空间，其中取值$0$-$100$的整数值<em>L</em>匹配人眼对亮度的感知，<em>ab</em>值控制不同的颜色取值，量化为$313$种颜色对。则自监督学习的前置任务构造为在量化颜色值上预测概率分布的交叉熵损失。</p>

<p><img src="https://pic.imgdb.cn/item/63c129fdbe43e0d30e0b29d3.jpg" alt="" /></p>

<h3 id="-learning-to-count">⚪ <a href="https://0809zheng.github.io/2022/10/06/count.html"><font color="blue">Learning to Count</font></a></h3>

<p>把图像的特征看作一种标量属性，如果把一幅图像划分成$2\times 2$的图像块，则四个图像块中特征的数量之和应该与原始图像的特征数量相同。把模型看作特征计数器$\phi(\cdot)$，对于输入图像$x$定义$2\times$下采样操作$D(\cdot)$和$2\times 2$图像块划分操作$T_i(\cdot),i=1,2,3,4$，则自监督学习的前置任务定义为如下目标函数：</p>

\[\mathcal{L} = ||\phi(D \circ x)  - \sum_{i=1}^4 \phi(T_i \circ x)||_2^2 + \max(0,c-||\phi(D \circ y)  - \sum_{i=1}^4 \phi(T_i \circ x)||_2^2)\]

<p><img src="https://pic.imgdb.cn/item/63c11ef0be43e0d30ef8c190.jpg" alt="" /></p>

<h3 id="-image-rotation">⚪ <a href="https://0809zheng.github.io/2022/10/03/rotation.html"><font color="blue">Image Rotation</font></a></h3>

<p>对输入图像随机地旋转四种不同的角度：\([0^{\circ},90^{\circ},180^{\circ},270^{\circ}]\)，则自监督学习的前置任务是预测图像旋转哪种角度的四分类任务。</p>

<p><img src="https://pic.imgdb.cn/item/63c0e754be43e0d30e91f5aa.jpg" alt="" /></p>

<h3 id="-jigsaw-clustering">⚪ <a href="https://0809zheng.github.io/2022/10/31/jigsawclustering.html"><font color="blue">Jigsaw Clustering</font></a></h3>

<p><strong>Jigsaw Clustering</strong>把一批图像拆分成$m\times m$的图像块，打乱后构成一批新的图像，并通过卷积网络提取每个图像块的特征。则自监督学习的前置任务包括：聚类损失：对于每一个图像块特征$z_i$，属于同一个原始图像的特征$z_j$为正样本，其余特征为负样本；定位损失：每个图像块的位置可以构造一个$mm$分类问题。</p>

<p><img src="https://pic.imgdb.cn/item/63e4aad54757feff33292620.jpg" alt="" /></p>

<h3 id="-evolving-loss">⚪ <a href="https://0809zheng.github.io/2022/10/27/evolving.html"><font color="blue">Evolving Loss</font></a></h3>

<p>本文提出了一种用于视频表示学习的多模态多任务框架。该方法处理四种数据模态：<strong>RGB</strong>、光流图像、灰度图像和音频；对每种模态设置七种自监督学习任务；并且构建其他模态网络对处理<strong>RGB</strong>网络的数据蒸馏。</p>

<p><img src="https://pic.imgdb.cn/item/63e205974757feff331cc71a.jpg" alt="" /></p>

<h3 id="-parametric-instance-classification-pic">⚪ <a href="https://0809zheng.github.io/2022/11/29/pic.html"><font color="blue">Parametric Instance Classification (PIC)</font></a></h3>

<p>参数化实例分类<strong>PIC</strong>框架把每张输入图像作为一个类别，通过预测类别来进行特征学习。使用适当的策略如余弦<strong>Softmax</strong>损失、更强的数据增强与两层映射头网络之后，预训练性能有显著提高。</p>

<p><img src="https://pic.imgdb.cn/item/66879f11d9c307b7e9a10082.png" alt="" /></p>

<h3 id="-masked-patch-position-prediction-mp3">⚪ <a href="https://0809zheng.github.io/2022/12/30/mp3.html"><font color="blue">Masked Patch Position Prediction (MP3)</font></a></h3>

<p>视觉<strong>Transformer</strong>接收一组图像<strong>patch</strong>，但不提供它们的位置信息；通过随机选择一个<strong>patch</strong>子集来计算注意力层的键矩阵和值矩阵。前置任务是预测每个输入位置的分类问题。</p>

<p><img src="https://pic.imgdb.cn/item/668b50edd9c307b7e934b46b.png" alt="" /></p>

<h1 id="2-基于对比学习的自监督学习方法">2. 基于对比学习的自监督学习方法</h1>

<p><strong>对比学习(Contrastive Learning)</strong>旨在学习一个特征嵌入空间，使得相似的样本对(正样本对)彼此靠近，不相似的样本对(负样本对)相互远离。在无监督形式的对比学习中，可以通过<strong>数据增强</strong>等方法构造样本对，从而实现有意义的特征表示学习。</p>

<h2 id="1对比损失函数">（1）对比损失函数</h2>

<p>对比学习中的损失函数可以追溯到监督学习中的<a href="https://0809zheng.github.io/2022/11/01/metric.html">深度度量学习</a>，通过给定类别标签构造正负样本对，最小化正样本对$(x,x^+)$的嵌入距离，最大化负样本对$(x,x^-)$的嵌入距离。</p>

<h3 id="-noise-contrastive-estimation-nce">⚪ <a href="http://proceedings.mlr.press/v9/gutmann10a.html">Noise Contrastive Estimation (NCE)</a></h3>

<p><strong>噪声对比估计</strong>是一种统计模型的参数估计方法，其想法是运行<a href="https://0809zheng.github.io/2020/03/13/logistic-regression.html">逻辑回归</a>来区分目标样本$x$和噪声\(\tilde{x}\)。逻辑回归模型$f(\cdot)$通过<strong>Sigmoid</strong>激活建模属于目标而不是噪声的概率，进而建立二元交叉熵损失：</p>

\[\mathcal{L}_{NCE} = - \Bbb{E}_{x \text{~} p_{\text{data}}} [\log f(x)] - \Bbb{E}_{\tilde{x} \text{~} p_{\text{noise}}} [\log(1-f(\tilde{x}))]\]

<h3 id="-contrastive-predictive-coding-cpc">⚪ <a href="https://0809zheng.github.io/2022/10/08/cpc.html"><font color="blue">Contrastive Predictive Coding (CPC)</font></a></h3>

<p><strong>对比预测编码</strong>把二元<strong>NCE</strong>损失扩展为多元<strong>InfoNCE</strong>损失。给定上下文向量$c$，正样本通过$p(x|c)$构造，$N-1$个负样本通过$p(x)$构造；使用类别交叉熵损失区分正样本和噪声样本：</p>

\[\begin{aligned} \mathcal{L}_{InfoNCE} &amp;= - \Bbb{E}_{x \text{~} p_{\text{data}}} [\log\frac{f(x,c)}{\sum_{x' \text{~} p_{\text{data}}} f(x',c)}] \\ &amp;= - \Bbb{E}_{x,x^+,\{x^-_i\}_{i=1}^N} [\log\frac{e^{f(x)^Tf(x^+)}}{e^{f(x)^Tf(x^+)}+\sum_{i=1}^N e^{f(x)^Tf(x^-_i)}}] \end{aligned}\]

<h3 id="-cpc-v2">⚪ <a href="https://0809zheng.github.io/2022/10/09/cpcv2.html"><font color="blue">CPC v2</font></a></h3>

<p>把<strong>InfoNCE</strong>应用到图像数据集中，把输入图像$x$的每个图像块压缩为潜在表示$z_{i,j}$，从中构造上下文特征$c_{i,j}=g_{\phi}(z_{\leq i,\leq j})$，并进一步预测潜在表示\(\hat{z}_{i+k,j} = W_kc_{i,j}\)。</p>

\[\begin{aligned} \mathcal{L}_N = - \sum_{i,j,k} \log\frac{\exp(\hat{z}_{i+k,j}^Tz_{i+k,j})}{\exp(\hat{z}_{i+k,j}^Tz_{i+k,j}) + \sum_l \exp(\hat{z}_{i+k,j}^Tz_{l})} \end{aligned}\]

<h3 id="-alignment-and-uniformity">⚪ <a href="https://0809zheng.github.io/2022/10/12/hypersphere.html"><font color="blue">Alignment and Uniformity</font></a></h3>

<p>对比学习的损失函数具有两种性质：<strong>对齐性(Alignment)</strong>和<strong>一致性(Uniformity)</strong>。对齐性用于衡量正样本对之间的相似程度；一致性用于衡量归一化的特征在超球面上分布的均匀性。</p>

\[\begin{aligned} \mathcal{L}_{align}(f;\alpha) &amp;=  \Bbb{E}_{(x,y)\text{~}p_{pos}} [||f(x)-f(y)||_2^{\alpha}] \\ \mathcal{L}_{uniform}(f;t) &amp;= \log \Bbb{E}_{(x,y)\text{~}p_{data}} [e^{-t||f(x)-f(y)||_2^2}] \end{aligned}\]

<h3 id="-debiased-contrastive-loss">⚪ <a href="https://0809zheng.github.io/2022/10/13/debiased.html"><font color="blue">Debiased Contrastive Loss</font></a></h3>

<p>由于样本的真实标签是未知的，因此负样本可能采样到假阴性样本。在构造对比损失时，对负样本项进行偏差修正：</p>

\[g(x,\{u_i\}_{i=1}^N,\{v_i\}_{i=1}^M) = \max(\frac{1}{\eta^-}(\frac{1}{N}\sum_{i=1}^N \exp(f(x)^Tf(u_i))-\frac{\eta^+}{M}\sum_{i=1}^M \exp(f(x)^Tf(v_i))),\exp(-1/\tau)) \\ \mathcal{L}_{unbiased} = \Bbb{E}_{x,\{u_i\}_{i=1}^N\text{~}p;x^+,\{v_i\}_{i=1}^M\text{~}p_x^+} [-\log \frac{\exp(f(x)^Tf(x^+))}{\exp(f(x)^Tf(x^+))+Ng(x,\{u_i\}_{i=1}^N,\{v_i\}_{i=1}^M)}]\]

<h3 id="-hard-negative-samples">⚪ <a href="https://0809zheng.github.io/2022/10/14/hardnegtive.html"><font color="blue">Hard Negative Samples</font></a></h3>

<p>对对比损失中的负样本对项\(\exp(f(x)^Tf(x^-))\)进行加权，权重正比于负样本与<strong>anchor</strong>样本的相似度，设置为：</p>

\[\frac{\beta \exp(f(x)^Tf(x^-))}{\sum_{x^-} \exp(f(x)^Tf(x^-))}\]

<h3 id="-flatnce">⚪ <a href="https://0809zheng.github.io/2021/08/04/flatnce.html"><font color="blue">FlatNCE</font></a></h3>

<p>对比学习损失在批量较小时效果较差的原因之一是损失和梯度计算的浮点误差。把对比损失修改为：</p>

\[\begin{aligned} \mathcal{L}_{FlatNCE} &amp;= - \Bbb{E}_{x,x^+,\{x^-_i\}_{i=1}^N} [\log\frac{e^{f(x)^Tf(x^+)}}{\sum_{i=1}^N e^{f(x)^Tf(x^-_i)}}] \end{aligned}\]

<h2 id="2并行数据增强-parallel-augmentation">（2）并行数据增强 Parallel Augmentation</h2>

<p>基于<strong>并行数据增强</strong>的对比学习方法为<strong>anchor</strong>样本同时生成两个数据增强样本，并使得它们共享相同的特征表示。</p>

<h3 id="-invariant-and-spreading-instance-feature-invaspread">⚪ <a href="https://0809zheng.github.io/2022/10/18/invaspread.html"><font color="blue">Invariant and Spreading Instance Feature (InvaSpread)</font></a></h3>

<p><strong>InvaSpread</strong>对于一批样本进行数据增强，把样本$x$的增强样本\(\hat{x}\)视为正样本，其余所有样本视为负样本；正样本特征应具有不变性，负样本特征应尽可能地分开。</p>

\[\mathcal{L}_{\text{InvaSpread}} = -\sum_i \log \frac{\exp(f_i^T\hat{f}_i/\tau)}{\sum_{k=1}^N\exp(f_k^T\hat{f}_i/\tau)}-\sum_i \sum_{j\neq i} \log(1- \frac{\exp(f_i^Tf_j/\tau)}{\sum_{k=1}^N\exp(f_k^Tf_j/\tau)})\]

<p><img src="https://pic.imgdb.cn/item/63d882dbface21e9efa1f99b.jpg" alt="" /></p>

<h3 id="-simple-framework-for-contrastive-learning-of-visual-representation-simclr">⚪ <a href="https://0809zheng.github.io/2022/10/15/simclr.html"><font color="blue">Simple Framework for Contrastive Learning of Visual Representation (SimCLR)</font></a></h3>

<p><strong>SimCLR</strong>随机采样$N$个数据样本，对每个样本应用两次同一类的不同数据增强，构造$2N$个增强样本；对于任意样本\(\tilde{x}_i\)，\(\tilde{x}_j\)为正样本，其余$2(N-1)$个样本为负样本。通过编码网络$f(\cdot)$和映射层$g(\cdot)$提取特征表示，并构造对比损失：</p>

\[\mathcal{L}^{(i,j)}_{\text{SimCLR}} = -\log \frac{\exp(\text{sim}(z_i,z_j)/\tau)}{\sum_{k=1,...,2N;k\neq i}\exp(\text{sim}(z_i,z_k)/\tau)}\]

<p><img src="https://pic.imgdb.cn/item/63d5e959face21e9efe0214a.jpg" alt="" /></p>

<h3 id="-simclrv2">⚪ <a href="https://0809zheng.github.io/2022/09/16/selfsemi.html"><font color="blue">SimCLRv2</font></a></h3>

<p><strong>SimCLRv2</strong>在<strong>SimCLR</strong>的基础上采用更大的卷积网络和更深的映射头，并通过微调和数据蒸馏实现半监督学习：</p>

<p><img src="https://pic.imgdb.cn/item/63bfd38fbe43e0d30ee98443.jpg" alt="" /></p>

<h3 id="-bootstrap-your-own-latent-byol">⚪ <a href="https://0809zheng.github.io/2022/10/17/byol.html"><font color="blue">Bootstrap your own latent (BYOL)</font></a></h3>

<p><strong>BYOL</strong>没有构建负样本对，而是使用参数为$\theta$的在线网络和参数为$\xi$的目标网络分别从图像$x$的两个增强版本中提取特征$z,z’$，根据$z$预测$z’$(或交换顺序后根据$z’$预测$z$)。损失函数设置为归一化特征的均方误差损失，更新参数$\theta$，参数$\xi$是参数$\theta$的滑动平均：\(\xi \leftarrow \tau \xi + (1-\tau)\theta\)。</p>

\[\begin{aligned} \mathcal{L}_{\text{BYOL}} \propto -2(\frac{&lt;q_{\theta}(z_{\theta}),z'_{\xi}&gt;}{||q_{\theta}(z_{\theta})||_2 \cdot ||z'_{\xi}||_2}+\frac{&lt;q_{\theta}(z'_{\theta}),z_{\xi}&gt;}{||q_{\theta}(z'_{\theta})||_2 \cdot ||z_{\xi}||_2}) \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63d8c180face21e9ef442d3e.jpg" alt="" /></p>

<h3 id="-simple-siamese-representation-learning-simsiam">⚪ <a href="https://0809zheng.github.io/2022/10/26/simsiam.html"><font color="blue">Simple Siamese Representation Learning (SimSiam)</font></a></h3>

<p><strong>SimSiam</strong>使用孪生网络$f$从图像$x$的两个增强版本$x_1,x_2$中提取特征$z_1,z_2$，并使用预测头$h$根据一个特征预测另一个特征。损失函数设置为负余弦相似度：</p>

\[\begin{aligned} \mathcal{L}_{\text{SimSiam}} = -\frac{1}{2} \frac{h(z_1)}{||h(z_1)||_2} \cdot \frac{sg(z_2)}{||sg(z_2)||_2} -\frac{1}{2} \frac{h(z_2)}{||h(z_2)||_2} \cdot \frac{sg(z_1)}{||sg(z_1)||_2} \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63e1bb6f4757feff33a0d545.jpg" alt="" /></p>

<h3 id="-self-distillation-with-no-labels-dino">⚪ <a href="https://0809zheng.github.io/2022/10/29/dino.html"><font color="blue">Self-distillation with no labels (DINO)</font></a></h3>

<p><strong>DINO</strong>使用学生网络$f_s$和滑动平均更新的教师网络$f_t$从图像$x$的两个增强版本$x_1,x_2$中提取特征$f_s(x_1),f_t(x_2)$。为教师网络的预测特征引入<strong>centering</strong>操作，然后把特征$f_s(x_1),f_t(x_2)$通过<strong>softmax</strong>函数映射为概率分布。则损失函数构建为两个概率分布的交叉熵：</p>

\[\mathcal{L}_{\text{DINO}} = -p_t(x_2) \log p_s(x_1) -p_t(x_1) \log p_s(x_2)\]

<p><img src="https://pic.imgdb.cn/item/63e44bd84757feff336e3d20.jpg" alt="" /></p>

<h3 id="-swapping-assignments-between-multiple-views-swav">⚪ <a href="https://0809zheng.github.io/2022/10/24/swav.html"><font color="blue">Swapping Assignments between multiple Views (SwAV)</font></a></h3>

<p><strong>SwAV</strong>使用样本特征和预定义的$K$个原型向量(聚类中心) \(C=\{c_1,...,c_K\}\)进行对比学习。给定两个数据样本$x_t,x_s$，构造特征向量$z_t,z_s$，并进一步构造编码$q_t,q_s$。则损失函数定义为聚类预测和编码之间的交叉熵：</p>

\[\mathcal{L}_{\text{SwAV}} = -\sum_k q_s^{(k)} \log \frac{\exp(z_t^Tc_k/ \tau)}{\sum_{k'} \exp(z_t^Tc_{k'}/ \tau)}  -\sum_k q_t^{(k)} \log \frac{\exp(z_s^Tc_k/ \tau)}{\sum_{k'} \exp(z_s^Tc_{k'}/ \tau)}\]

<p><img src="https://pic.imgdb.cn/item/63e059ae4757feff338e664a.jpg" alt="" /></p>

<h3 id="-pixel-level-contrastive-learning-pixcontrast">⚪ <a href="https://0809zheng.github.io/2022/10/30/pixpro.html"><font color="blue">Pixel-level Contrastive Learning (PixContrast)</font></a></h3>

<p><strong>PixContrast</strong>是一种像素级的对比学习方法。对于一幅图像中的目标，分别选取两个子图像，则两个图像中对应同一个目标位置的像素可以看作正样本对。</p>

<p><img src="https://pic.imgdb.cn/item/63e49bc04757feff330a221b.jpg" alt="" /></p>

<h3 id="-barlow-twins">⚪ <a href="https://0809zheng.github.io/2022/10/16/barlow.html"><font color="blue">Barlow Twins</font></a></h3>

<p><strong>Barlow Twins</strong>把数据样本$x$的两个增强版本$x^A,x^B$喂入同一个神经网络以提取特征表示$z^A,z^B$，并使得两组输出特征的互相关矩阵\(\mathcal{C}\)接近单位矩阵。</p>

\[\mathcal{L}_{\text{BT}} = \sum_i (1-\mathcal{C}_{ii})^2 + \lambda \sum_i \sum_{j\neq i} \mathcal{C}_{ij}^2 , \quad \mathcal{C}_{ij} = \frac{\sum_bz^A_{b,i}z^B_{b,j}}{\sqrt{\sum_b(z_{b,i}^A)^2}\sqrt{\sum_b(z^B_{b,j})^2}}\]

<p><img src="https://pic.imgdb.cn/item/63d797e3face21e9ef0c7486.jpg" alt="" /></p>

<h2 id="3存储体-memory-bank">（3）存储体 Memory Bank</h2>

<p>基于<strong>存储体(Memory Bank)</strong>的对比学习方法把所有样本的特征向量存储在内存中，以减小计算开销。</p>

<h3 id="-instance-level-discrimination-instdisc">⚪ <a href="https://0809zheng.github.io/2022/10/19/instdisc.html"><font color="blue">Instance-level Discrimination (InstDisc)</font></a></h3>

<p><strong>InstDisc</strong>把每一个数据样本看作一个类别，把样本的特征向量\(V=\{v_i\}\)存储在<strong>Memory Bank</strong>中。每次更新时从<strong>Memory Bank</strong>中采样负样本，采用<strong>NCE</strong>的形式区分不同样本类别：</p>

\[\begin{aligned} \mathcal{L}_{InstDisc} = &amp;- \Bbb{E}_{p_{\text{data}}} [\log h(i,v_i^{(t-1)})-\lambda ||v_i^{(t)}-v_i^{(t-1)}||_2^2]  \\ &amp; - M \cdot \Bbb{E}_{P_N} [\log(1-h(i,v'^{(t-1)}))] \end{aligned}\]

<p><img src="https://pic.imgdb.cn/item/63da237bac6ef8601607d72a.jpg" alt="" /></p>

<h3 id="-momentum-contrast-moco">⚪ <a href="https://0809zheng.github.io/2022/10/21/moco.html"><font color="blue">Momentum Contrast (MoCo)</font></a></h3>

<p><strong>MoCo</strong>通过编码器$f_q(\cdot)$构造查询样本$x_q$的查询表示$q=f_q(x_q)$，通过滑动平均更新的矩编码器$f_k(\cdot)$构造键表示$k_i=f_k(x_k^i)$，并维持一个存储键表示的先入先出队列。</p>

\[\mathcal{L}_{\text{MoCo}} = -\log \frac{\exp(q \cdot k^+/\tau)}{\sum_{i=0}^{N}\exp(q \cdot k_i/\tau)}\]

<p><img src="https://pic.imgdb.cn/item/63db236dac6ef86016f41ca0.jpg" alt="" /></p>

<h3 id="-moco-v2">⚪ <a href="https://0809zheng.github.io/2022/10/22/mocov2.html"><font color="blue">MoCo v2</font></a></h3>

<p><strong>MoCo v2</strong>在<strong>MoCo</strong>的基础上引入了映射头、采用更多数据增强、余弦学习率策略和更长的训练轮数。</p>

<h3 id="-moco-v3">⚪ <a href="https://0809zheng.github.io/2022/10/23/mocov3.html"><font color="blue">MoCo v3</font></a></h3>

<p><strong>MoCo v3</strong>把矩对比方法应用到视觉<strong>Transformer</strong>的自监督训练中，没有采取<strong>MoCo</strong>中的队列设计，而是根据每批样本构造正负样本对，并在编码器后引入预测头。</p>

<p>给定一批样本$x$，分别做两次数据增强得到$x_1,x_2$，通过编码器构造$q_1,q_2$，通过矩编码器构造$k_1,k_2$。则对比损失对称地构造为：</p>

\[\mathcal{L}_{\text{MoCov3}} = -\log \frac{\exp(q_1 \cdot k_2^+/\tau)}{\sum_{i=0}^{N}\exp(q_1 \cdot k_2^i/\tau)}-\log \frac{\exp(q_2 \cdot k_1^+/\tau)}{\sum_{i=0}^{N}\exp(q_2 \cdot k_1^i/\tau)}\]

<h2 id="4多模态-multi-modality">（4）多模态 Multi-Modality</h2>

<h3 id="-contrastive-multiview-coding-cmc">⚪ <a href="https://0809zheng.github.io/2022/10/20/cmc.html"><font color="blue">Contrastive Multiview Coding (CMC)</font></a></h3>

<p><strong>CMC</strong>把来自不同传感器的多模态数据之间视为正样本，对于样本$x$的$M$种不同的模态，可构造任意两种模态之间的对比损失：</p>

\[\mathcal{L}^{(i,j)}_{\text{CMC}} = -\log \frac{\exp(f(v_i)^Tf(v_j)/\tau)}{\sum_{k}\exp(f(v_i)^Tf(v_j^k)/\tau)} -\log \frac{\exp(f(v_j)^Tf(v_i)/\tau)}{\sum_{k}\exp(f(v_j)^Tf(v_i^k)/\tau)}\]

<p><img src="https://pic.imgdb.cn/item/63da57a4ac6ef86016679b14.jpg" alt="" /></p>

<h3 id="-contrastive-language-image-pre-training-clip">⚪ <a href="https://0809zheng.github.io/2021/01/06/dalle.html"><font color="blue">Contrastive Language-Image Pre-training (CLIP)</font></a></h3>

<p><strong>CLIP</strong>方法用于在图像和文本数据集中进行匹配。给定$N$个图像-文本对，首先计算任意一个图像和文本之间的余弦相似度矩阵，尺寸为$N \times N$；通过交叉熵损失使得匹配的$N$个图像-文本对的相似度最大，其余$N(N-1)$个相似度最小。</p>

<p><img src="https://pic.imgdb.cn/item/63e2f48f4757feff3376d5d7.jpg" alt="" /></p>

<h2 id="5应用-applications">（5）应用 Applications</h2>

<h3 id="-contrastive-unsupervised-representations-for-reinforcement-learning-curl">⚪ <a href="https://0809zheng.github.io/2022/10/25/curl.html"><font color="blue">Contrastive Unsupervised Representations for Reinforcement Learning (CURL)</font></a></h3>

<p><strong>CURL</strong>把对比学习应用到强化学习领域。它采用<strong>MoCo</strong>方法学习强化学习任务的视觉表示，通过随机裁剪构造观测$o$的两个数据增强版本$o_q,o_k$。</p>

<p><img src="https://pic.imgdb.cn/item/63e1f8ef4757feff3306c153.jpg" alt="" /></p>

<h3 id="-contrastive-unpaired-translation-cut">⚪ <a href="https://0809zheng.github.io/2022/05/10/cut.html"><font color="blue">Contrastive Unpaired Translation (CUT)</font></a></h3>

<p><strong>CUT</strong>是一种基于对比学习的图像到图像翻译方法。它构造输入图像$x$和生成图像$\hat{y}$特征的<strong>PatchNCE</strong>损失：特征的每个像素位置对应原始图像的一个图像块；则两个相同位置的特征向量为正样本对，其余位置的特征向量为负样本。</p>

<p><img src="https://pic.imgdb.cn/item/63e358b54757feff3319a783.jpg" alt="" /></p>

<h3 id="-background-augmentation">⚪ <a href="https://0809zheng.github.io/2022/10/28/background.html"><font color="blue">Background Augmentation</font></a></h3>

<p><strong>Background Augmentation</strong>是一种增强对比学习的性能表现的数据增强策略。使用显著性图生成方法提取图像的前景区域，并调整图像的背景区域。</p>

<p><img src="https://pic.imgdb.cn/item/63e34a8b4757feff33026ea2.jpg" alt="" /></p>

<h3 id="-feature-distillation-fd">⚪ <a href="https://0809zheng.github.io/2022/11/30/featuredistillation.html"><font color="blue">Feature Distillation (FD)</font></a></h3>

<p>对于任意基于对比的自监督预训练模型，<strong>FD</strong>使用特征作为蒸馏的目标，已经学习到的特征会再被蒸馏成为全新的特征。通过引入白化蒸馏目标 ，共享相对位置编码以及非对称的 <strong>Drop Path</strong> 率，基于对比的自监督预训练方法的微调性能达到与掩码图像建模方法相当的表现。</p>

<p><img src="https://pic.imgdb.cn/item/6687bf64d9c307b7e9dd7a47.png" alt="" /></p>

<h1 id="3-基于掩码图像建模的自监督学习方法">3. 基于掩码图像建模的自监督学习方法</h1>

<p>随着计算机视觉的主流架构从卷积神经网络过度到<a href="https://0809zheng.github.io/2023/01/01/vit.html">视觉<strong>Transformer</strong></a>，图像可以被表示为一系列<strong>patch token</strong>，因此自然地引入<strong>token-level</strong>的自监督方法，即掩码图像建模（<strong>masked image modeling, MIM</strong>）。掩码图像建模是指随机遮挡图像中的部分<strong>patch</strong>，并以自编码器的形式重构这部分<strong>patch</strong>。</p>

<h3 id="扩展阅读-1">⭐扩展阅读：</h3>
<ul>
  <li><a href="https://0809zheng.github.io/2022/11/27/darksecret.html"><font color="blue">Revealing the Dark Secrets of Masked Image Modeling</font></a>：揭露掩码图像建模方法的有效性。</li>
  <li><a href="https://0809zheng.github.io/2022/11/26/datascaling.html"><font color="blue">On Data Scaling in Masked Image Modeling</font></a>：探究掩码图像建模中的数据可扩展性。</li>
</ul>

<h3 id="-beit">⚪ <a href="https://0809zheng.github.io/2022/11/22/beit.html"><font color="blue">BEiT</font></a></h3>

<p><strong>BEiT</strong>使用<strong>dVAE</strong>将图像<strong>Patch</strong>编码成视觉<strong>Token</strong>，使用<strong>BERT</strong>预测图像掩码部分对应的视觉<strong>Token</strong>。</p>

<p><img src="https://pic.imgdb.cn/item/65583504c458853aef357f14.jpg" alt="" /></p>

<h3 id="-masked-autoencoder-mae">⚪ <a href="https://0809zheng.github.io/2021/12/09/mae.html"><font color="blue">Masked Autoencoder (MAE)</font></a></h3>

<p><strong>MAE</strong>采用非对称的编码器-解码器结构。编码器只对未遮挡的图像块进行操作；解码器从编码特征和遮挡<strong>token</strong>中重建整个图像。</p>

<p><img src="https://pic.imgdb.cn/item/61b171a22ab3f51d919d9984.jpg" alt="" /></p>

<h3 id="-simmim">⚪ <a href="https://0809zheng.github.io/2022/11/24/simmim.html"><font color="blue">SimMIM</font></a></h3>

<p><strong>SimMIM</strong>随机<strong>mask</strong>图像的一部分<strong>patches</strong>，并直接回归预测这部分<strong>patches</strong>的原始像素 <strong>RGB</strong> 值。</p>

<p><img src="https://pic.imgdb.cn/item/655856e8c458853aefad7c62.jpg" alt="" /></p>

<h3 id="-ibot">⚪ <a href="https://0809zheng.github.io/2022/11/23/ibot.html"><font color="blue">iBOT</font></a></h3>

<p><strong>iBOT</strong>通过参数滑动平均构造在线 <strong>tokenizer</strong>，通过构造<strong>mask</strong>与<strong>unmask</strong>版本输出<strong>token</strong>的自蒸馏损失捕捉高层语义的特性。</p>

<p><img src="https://pic.imgdb.cn/item/65584a6ac458853aef7d1279.jpg" alt="" /></p>

<h3 id="-convmae">⚪ <a href="https://0809zheng.github.io/2022/11/25/convmae.html"><font color="blue">ConvMAE</font></a></h3>

<p><strong>ConvMAE</strong>把模型架构设置为多尺度的金字塔式架构，对于编码器使用卷积<strong>+Transformer</strong>结合的模型。</p>

<p><img src="https://pic.imgdb.cn/item/65585a60c458853aefbaaa0b.jpg" alt="" /></p>

<h3 id="-qb-heat">⚪ <a href="https://0809zheng.github.io/2022/11/28/qbheat.html"><font color="blue">QB-Heat</font></a></h3>

<p><strong>QB-Heat</strong>每次只输入一小部分图像，经过编码器后得到对应的特征，通过下列方程组来预测完整图像的特征，然后将特征传入一个较小的解码器来重建完整图像。</p>

\[z(x+\Delta x, y) \approx z(x,y) + \Delta x A z(x,y) = (I+\Delta x A) z(x,y)\\
z(x, y+\Delta y) \approx z(x,y) + \Delta y B z(x,y) = (I+\Delta y B) z(x,y)\]

<p><img src="https://pic.imgdb.cn/item/668661d7d9c307b7e978a663.png" alt="" /></p>

<h3 id="-localmim">⚪ <a href="https://0809zheng.github.io/2023/03/14/localmim.html"><font color="blue">LocalMIM</font></a></h3>

<p><strong>LocalMIM</strong>将重构任务引入多个选择的局部层，并提出多尺度重构：较低层重构细尺度信息，较高层重构粗尺度信息。</p>

<p><img src="https://pic.imgdb.cn/item/655abc44c458853aef4191d9.jpg" alt="" /></p>

<h3 id="-deepmim">⚪ <a href="https://0809zheng.github.io/2023/03/15/deepmim.html"><font color="blue">DeepMIM</font></a></h3>

<p><strong>DeepMIM</strong>在 <strong>Masked Image Modeling</strong> 训练过程中加上 <strong>Deep Supervision</strong>，可以促进浅层学习更有意义的表示。</p>

<p><img src="https://pic.imgdb.cn/item/655aaba0c458853aef1d8ea4.jpg" alt="" /></p>

<h1 id="-参考文献">⭐ 参考文献</h1>
<ul>
  <li><a href="https://lilianweng.github.io/posts/2019-11-10-self-supervised/">Self-Supervised Representation Learning</a>(Lil’Log)一篇介绍自监督学习的博客。</li>
  <li><a href="https://github.com/jason718/awesome-self-supervised-learning">Awesome Self-Supervised Learning</a>：(github) A curated list of awesome self-supervised methods.</li>
  <li><a href="https://arxiv.org/abs/2006.05278">An Overview of Deep Semi-Supervised Learning</a>：(arXiv2006)一篇深度半监督学习的综述。</li>
  <li><a href="https://0809zheng.github.io/2022/10/02/exemplarcnn.html"><font color="blue">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</font></a>：(arXiv1406)通过Exemplar-CNN实现判别无监督特征学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/04/context.html"><font color="blue">Unsupervised Visual Representation Learning by Context Prediction</font></a>：(arXiv1505)通过上下文预测实现无监督视觉表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/05/jigsaw.html"><font color="blue">Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</font></a>：(arXiv1603)通过解决拼图问题实现无监督视觉表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/07/colorization.html"><font color="blue">Colorful Image Colorization</font></a>：(arXiv1603)通过彩色图像着色实现无监督特征学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/06/count.html"><font color="blue">Representation Learning by Learning to Count</font></a>：(arXiv1708)通过学习计数实现无监督表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/03/rotation.html"><font color="blue">Unsupervised Representation Learning by Predicting Image Rotations</font></a>：(arXiv1803)通过预测图像旋转角度实现无监督表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/19/instdisc.html"><font color="blue">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination</font></a>：(arXiv1805)通过非参数化实例级判别实现无监督特征学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/08/cpc.html"><font color="blue">Representation Learning with Contrastive Predictive Coding</font></a>：(arXiv1807)通过对比预测编码进行表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/18/invaspread.html"><font color="blue">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</font></a>：(arXiv1904)通过不变和扩散的实例特征实现无监督嵌入学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/09/cpcv2.html"><font color="blue">Data-Efficient Image Recognition with Contrastive Predictive Coding</font></a>：(arXiv1905)通过对比预测编码实现数据高效的图像识别。</li>
  <li><a href="https://0809zheng.github.io/2022/10/20/cmc.html"><font color="blue">Contrastive Multiview Coding</font></a>：(arXiv1906)对比多视角编码。</li>
  <li><a href="https://0809zheng.github.io/2022/10/21/moco.html"><font color="blue">Momentum Contrast for Unsupervised Visual Representation Learning</font></a>：(arXiv1911)MoCo：无监督视觉表示学习的矩对比。</li>
  <li><a href="https://0809zheng.github.io/2022/10/27/evolving.html"><font color="blue">Evolving Losses for Unsupervised Video Representation Learning</font></a>：(arXiv2002)无监督视频表示学习的进化损失。</li>
  <li><a href="https://0809zheng.github.io/2022/10/15/simclr.html"><font color="blue">A Simple Framework for Contrastive Learning of Visual Representations</font></a>：(arXiv2002)SimCLR：一种视觉对比表示学习的简单框架。</li>
  <li><a href="https://0809zheng.github.io/2022/10/22/mocov2.html"><font color="blue">Improved Baselines with Momentum Contrastive Learning</font></a>：(arXiv2003)MoCo v2：改进矩对比学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/25/curl.html"><font color="blue">CURL: Contrastive Unsupervised Representations for Reinforcement Learning</font></a>：(arXiv2004)CURL：强化学习的对比无监督表示。</li>
  <li><a href="https://0809zheng.github.io/2022/10/12/hypersphere.html"><font color="blue">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</font></a>：(arXiv2005)通过超球面上的对齐和一致性理解对比表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/11/29/pic.html"><font color="blue">Parametric Instance Classification for Unsupervised Visual Feature Learning</font></a>：(arXiv2006)无监督视觉特征学习的参数化实例分类。</li>
  <li><a href="https://0809zheng.github.io/2022/09/16/selfsemi.html"><font color="blue">Big Self-Supervised Models are Strong Semi-Supervised Learners</font></a>：(arXiv2006)SimCLRv2：自监督大模型是强半监督学习器。</li>
  <li><a href="https://0809zheng.github.io/2022/10/17/byol.html"><font color="blue">Bootstrap your own latent: A new approach to self-supervised Learning</font></a>：(arXiv2006)BYOL：通过在隐空间应用自举法实现自监督学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/24/swav.html"><font color="blue">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</font></a>：(arXiv2006)SwAV：通过对比聚类指派实现无监督视觉特征学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/26/simsiam.html"><font color="blue">Exploring Simple Siamese Representation Learning</font></a>：(arXiv2006)SimSiam：探索简单的孪生表示学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/13/debiased.html"><font color="blue">Debiased Contrastive Learning</font></a>：(arXiv2007)偏差修正的对比学习。</li>
  <li><a href="https://0809zheng.github.io/2022/05/10/cut.html"><font color="blue">Contrastive Learning for Unpaired Image-to-Image Translation</font></a>：(arXiv2007)无配对数据图像到图像翻译中的对比学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/14/hardnegtive.html"><font color="blue">Contrastive Learning with Hard Negative Samples</font></a>：(arXiv2010)使用难例负样本进行对比学习。</li>
  <li><a href="https://0809zheng.github.io/2022/10/30/pixpro.html"><font color="blue">Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning</font></a>：(arXiv2011)探索无监督视觉表示学习中的像素级一致性。</li>
  <li><a href="https://0809zheng.github.io/2022/10/16/barlow.html"><font color="blue">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</font></a>：(arXiv2103)Barlow Twins：通过冗余度消除实现自监督学习。</li>
  <li><a href="https://0809zheng.github.io/2021/01/06/dalle.html"><font color="blue">Learning Transferable Visual Models From Natural Language Supervision</font></a>：(arXiv2103)CLIP: 对比语言图像预训练。</li>
  <li><a href="https://0809zheng.github.io/2022/10/28/background.html"><font color="blue">Characterizing and Improving the Robustness of Self-Supervised Learning through Background Augmentations</font></a>：(arXiv2103)通过背景增强改进自监督学习的鲁棒性。</li>
  <li><a href="https://0809zheng.github.io/2022/10/23/mocov3.html"><font color="blue">An Empirical Study of Training Self-Supervised Vision Transformers</font></a>：(arXiv2104)MoCo v3：训练自监督视觉Transformer的经验性研究。</li>
  <li><a href="https://0809zheng.github.io/2022/10/29/dino.html"><font color="blue">Emerging Properties in Self-Supervised Vision Transformers</font></a>：(arXiv2104)DINO：自监督视觉Transformer的新特性。</li>
  <li><a href="https://0809zheng.github.io/2022/10/31/jigsawclustering.html"><font color="blue">Jigsaw Clustering for Unsupervised Visual Representation Learning</font></a>：(arXiv2104)无监督视觉表示学习的拼图聚类方法。</li>
  <li><a href="https://0809zheng.github.io/2022/11/22/beit.html"><font color="blue">BEiT: BERT Pre-Training of Image Transformers</font></a>：(arXiv2107)BEiT：图像Transformer中的BERT预训练。</li>
  <li><a href="https://0809zheng.github.io/2021/08/04/flatnce.html"><font color="blue">Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE</font></a>：(arXiv2107)FlatNCE: 避免浮点数误差的小批量对比学习损失函数。</li>
  <li><a href="https://0809zheng.github.io/2021/12/09/mae.html"><font color="blue">Masked Autoencoders Are Scalable Vision Learners</font></a>：(arXiv2111)MAE: 掩码自编码器是可扩展的视觉学习者。</li>
  <li><a href="https://0809zheng.github.io/2022/11/24/simmim.html"><font color="blue">SimMIM: A Simple Framework for Masked Image Modeling</font></a>：(arXiv2111)SimMIM：一种掩码图像建模的简单框架。</li>
  <li><a href="https://0809zheng.github.io/2022/11/23/ibot.html"><font color="blue">iBOT: Image BERT Pre-Training with Online Tokenizer</font></a>：(arXiv2111)iBOT：使用在线标志进行图像BERT预训练。</li>
  <li><a href="https://0809zheng.github.io/2022/11/25/convmae.html"><font color="blue">ConvMAE: Masked Convolution Meets Masked Autoencoders</font></a>：(arXiv2205)ConvMAE：结合掩码卷积与掩码自编码器。</li>
  <li><a href="https://0809zheng.github.io/2022/11/27/darksecret.html"><font color="blue">Revealing the Dark Secrets of Masked Image Modeling</font></a>：(arXiv2205)揭露掩码图像建模方法的有效性。</li>
  <li><a href="https://0809zheng.github.io/2022/11/30/featuredistillation.html"><font color="blue">Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation</font></a>：(arXiv2205)特征蒸馏使对比学习在微调时击败了掩码图像建模。</li>
  <li><a href="https://0809zheng.github.io/2022/11/26/datascaling.html"><font color="blue">On Data Scaling in Masked Image Modeling</font></a>：(arXiv2206)探究掩码图像建模中的数据可扩展性。</li>
  <li><a href="https://0809zheng.github.io/2022/12/30/mp3.html"><font color="blue">Position Prediction as an Effective Pretraining Strategy</font></a>：(arXiv2207)位置预测作为高效的预训练策略。</li>
  <li><a href="https://0809zheng.github.io/2022/11/28/qbheat.html"><font color="blue">Self-Supervised Learning based on Heat Equation</font></a>：(arXiv2211)基于热传导方程的自监督学习。</li>
  <li><a href="https://0809zheng.github.io/2023/03/14/localmim.html"><font color="blue">Masked Image Modeling with Local Multi-Scale Reconstruction</font></a>：(arXiv2305)通过局部多尺度重构进行掩码图像建模。</li>
  <li><a href="https://0809zheng.github.io/2023/03/15/deepmim.html"><font color="blue">DeepMIM: Deep Supervision for Masked Image Modeling</font></a>：(arXiv2305)DeepMIM：掩码图像建模中的深度监督。</li>
</ul>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
      </div>
      <div class="author-name" rel="author">DawsonWen</div>
      <div class="bio">
        <p></p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="//github.com/Sologala" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>
    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/2022/10/02/exemplarcnn.html" class="read-next-link"></a>
        <section>
          <span>Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</span>
          <p>  通过Exemplar-CNN实现判别无监督特征学习.</p>
        </section>
        
        <div class="filter"></div>
        <img src="https://pic.imgdb.cn/item/63c0d642be43e0d30e782142.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/2022/09/30/yolov6.html" class="read-next-link"></a>
          <section>
            <span>YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications</span>
            <p>  YOLOv6：用于工业应用的单阶段目标检测框架.</p>
          </section>
          
          <div class="filter"></div>
          <img src="https://pic.imgdb.cn/item/652ce4d2c458853aef20c066.jpg" alt="">
          
      </div>
      
    </section>
    
    <section class="post-footer-item comment">
      <div id="disqus_thread"></div>
      <div id="gitalk_container"></div>
    </section>
  </section>

  <!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

  <script src="/assets/js/social-share.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'douban'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  
	
  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
