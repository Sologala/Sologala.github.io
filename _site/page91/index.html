<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DawsonWen的个人网站</title>
    <meta name="author"  content="DawsonWen">
    <meta name="description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta name="keywords"  content="数学, 机器学习, 深度学习">
    <!-- Open Graph -->
    <meta property="og:title" content="DawsonWen的个人网站">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/page91/">
    <meta property="og:description" content="为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平">
    <meta property="og:site_name" content="DawsonWen的个人网站">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_271755_rdxzzo2kqwk.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
        }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body>
    <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
    <input id="nm-switch" type="hidden" value="true">
    <div class="visible">
      <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


<div
    class="g-banner home-banner banner-theme-default"
    data-theme="default"
    style="background: url(https://raw.githubusercontent.com/Sologala/imgdb/master/post/image-20241217235839472.png) no-repeat center center; background-size: cover;"
>
    <h2>Dawson的个人博客</h2>
    <h3>欢迎光临</h3>
</div>

<main class="g-container home-content">
    <div class="article-list">
        
            <article class="article-item">
                
                <div class="post-cover">
                    <a class="post-link" href="/2021/08/11/reformer.html" title="Reformer: The Efficient Transformer"></a>
                    <img src="https://pic.imgdb.cn/item/61136f275132923bf8265b88.jpg" href="/2021/08/11/reformer.html" alt="">
                </div>
                
                <section class="post-preview">
                    <a class="post-link" href="/2021/08/11/reformer.html" title="Reformer: The Efficient Transformer"></a>
                    <h2 class="post-title">Reformer: The Efficient Transformer</h2>
                    
                    
                    <p class="post-excerpt">  Reformer: 使用局部敏感哈希和可逆FFN实现高效Transformer.</p>
                    
                </section>
                <footer class="post-meta">
                    <div class="post-tags">
                        
                            
                            <a href=/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB class="post-tag">论文阅读</a>
                            
                        
                    </div>
                    <time class="post-date" datetime="21-08-11">11 Aug 2021</time>
                </footer>
            </article>
        
            <article class="article-item">
                
                <div class="post-cover">
                    <a class="post-link" href="/2021/08/10/linear.html" title="Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"></a>
                    <img src="https://pic.imgdb.cn/item/611281935132923bf8da0262.jpg" href="/2021/08/10/linear.html" alt="">
                </div>
                
                <section class="post-preview">
                    <a class="post-link" href="/2021/08/10/linear.html" title="Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"></a>
                    <h2 class="post-title">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</h2>
                    
                    
                    <p class="post-excerpt">  Linear Transformer: 使用线性注意力实现快速自回归的Transformer.</p>
                    
                </section>
                <footer class="post-meta">
                    <div class="post-tags">
                        
                            
                            <a href=/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB class="post-tag">论文阅读</a>
                            
                        
                    </div>
                    <time class="post-date" datetime="21-08-10">10 Aug 2021</time>
                </footer>
            </article>
        
            <article class="article-item">
                
                <div class="post-cover">
                    <a class="post-link" href="/2021/08/09/external.html" title="Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks"></a>
                    <img src="https://pic.imgdb.cn/item/611112a95132923bf84df9a1.jpg" href="/2021/08/09/external.html" alt="">
                </div>
                
                <section class="post-preview">
                    <a class="post-link" href="/2021/08/09/external.html" title="Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks"></a>
                    <h2 class="post-title">Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks</h2>
                    
                    
                    <p class="post-excerpt">  External Attention: 使用两个外部记忆单元的注意力机制.</p>
                    
                </section>
                <footer class="post-meta">
                    <div class="post-tags">
                        
                            
                            <a href=/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB class="post-tag">论文阅读</a>
                            
                        
                    </div>
                    <time class="post-date" datetime="21-08-09">09 Aug 2021</time>
                </footer>
            </article>
        
            <article class="article-item">
                
                <div class="post-cover">
                    <a class="post-link" href="/2021/08/08/mitchell.html" title="二进制乘法的Mitchell近似"></a>
                    <img src="https://pic.imgdb.cn/item/610df2185132923bf8bdbb0f.jpg" href="/2021/08/08/mitchell.html" alt="">
                </div>
                
                <section class="post-preview">
                    <a class="post-link" href="/2021/08/08/mitchell.html" title="二进制乘法的Mitchell近似"></a>
                    <h2 class="post-title">二进制乘法的Mitchell近似</h2>
                    
                    
                    <p class="post-excerpt">  使用Mitchell近似构造加法神经网络.</p>
                    
                </section>
                <footer class="post-meta">
                    <div class="post-tags">
                        
                            
                            <a href=/tags.html#%E6%95%B0%E5%AD%A6 class="post-tag">数学</a>
                            
                        
                    </div>
                    <time class="post-date" datetime="21-08-08">08 Aug 2021</time>
                </footer>
            </article>
        
            <article class="article-item">
                
                <div class="post-cover">
                    <a class="post-link" href="/2021/08/06/resmlp.html" title="ResMLP: Feedforward networks for image classification with data-efficient training"></a>
                    <img src="https://pic.imgdb.cn/item/610bb3935132923bf85d1e32.jpg" href="/2021/08/06/resmlp.html" alt="">
                </div>
                
                <section class="post-preview">
                    <a class="post-link" href="/2021/08/06/resmlp.html" title="ResMLP: Feedforward networks for image classification with data-efficient training"></a>
                    <h2 class="post-title">ResMLP: Feedforward networks for image classification with data-efficient training</h2>
                    
                    
                    <p class="post-excerpt">  ResMLP：数据高效训练的全连接图像分类网络.</p>
                    
                </section>
                <footer class="post-meta">
                    <div class="post-tags">
                        
                            
                            <a href=/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB class="post-tag">论文阅读</a>
                            
                        
                    </div>
                    <time class="post-date" datetime="21-08-06">06 Aug 2021</time>
                </footer>
            </article>
        
            <article class="article-item">
                
                <div class="post-cover">
                    <a class="post-link" href="/2021/08/06/ffnvit.html" title="Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"></a>
                    <img src="https://pic.imgdb.cn/item/610bae695132923bf854f012.png" href="/2021/08/06/ffnvit.html" alt="">
                </div>
                
                <section class="post-preview">
                    <a class="post-link" href="/2021/08/06/ffnvit.html" title="Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"></a>
                    <h2 class="post-title">Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet</h2>
                    
                    
                    <p class="post-excerpt">  使用全连接层替换ViT中的自注意力层.</p>
                    
                </section>
                <footer class="post-meta">
                    <div class="post-tags">
                        
                            
                            <a href=/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB class="post-tag">论文阅读</a>
                            
                        
                    </div>
                    <time class="post-date" datetime="21-08-06">06 Aug 2021</time>
                </footer>
            </article>
        

        
            <nav class="pagination">
    <input type="hidden" id="total_pages" value="167">
    <input type="hidden" id="current_pages" value="91">
    <input type="hidden" id="base_url" value="/">
    <div class="page-links">
        
            
            <a href="/page90/" class="page-link" title="Previous Page">&laquo;</a>
            
        
        <div id="page-link-container"></div>
        
        <a href="/page92/" class="page-link">&raquo;</a>
        
    </div>
</nav>

        

    </div>

    <aside class="g-sidebar-wrapper">
        <div class="g-sidebar">
            <section class="author-card">
                <div class="avatar">
                    <img src="https://avatars.githubusercontent.com/u/46283762?v=4&size=64" alt="">
                </div>
                <div class="author-name" rel="author">DawsonWen</div>
                <div class="bio">
                    <p></p>
                </div>
                
                <ul id="sns-links" class="sns-links">
                    
                    <li>
                        <a href="//github.com/Sologala" target="_blank">
                            <i class="iconfont icon-github"></i>
                        </a>
                    </li>
                    
                </ul>
                
            </section>

            
            <section class="tags-card">
                
                    
                    <a href="/tags.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="tag">机器学习</a>
                
                    
                    <a href="/tags.html#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="tag">深度学习</a>
                
                    
                    <a href="/tags.html#%E6%95%B0%E5%AD%A6" class="tag">数学</a>
                
                    
                    <a href="/tags.html#%E8%8B%B1%E8%AF%AD" class="tag">英语</a>
                
                    
                    <a href="/tags.html#Python" class="tag">Python</a>
                
                    
                    <a href="/tags.html#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB" class="tag">论文阅读</a>
                
            </section>
            
        </div>

        
        <div class="search-card">
            <input id="search_input" type="text" placeholder="Search..." autocomplete="off">
            <i class="iconfont icon-search"></i>
            <div class="search_result"></div>
        </div>
        

    </aside>

</main>

<!-- <footer class="g-footer">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=800&t=m&d=WWuzUTmOt8V9vdtIQd5uqrEcKsRg4IiPuy9gg21CQO8'></script>
  <section>DawsonWen的个人网站 ©
  
  
    2020
    -
  
  2024
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a></section>
</footer>
 -->

    </div>
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
    <script src="/assets/js/prism.js"></script>
    <script src="/assets/js/index.min.js"></script>
</body>
</html>
